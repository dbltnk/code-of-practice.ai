<!DOCTYPE html>
<html lang="en" data-theme="light">

<head>
    <meta charset="UTF-8">
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0, viewport-fit=cover">
    <title>EU AI Act: General-Purpose AI Code of Practice · Draft 3</title>
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="favicon-96.png">
    <link rel="icon" type="image/png" sizes="128x128" href="favicon-128.png">
    <link rel="icon" type="image/png" sizes="196x196" href="favicon-196.png">
    <link rel="icon" type="image/x-icon" sizes="16x16" href="favicon-16.ico">
    <link rel="icon" type="image/x-icon" sizes="32x32" href="favicon-32.ico">
    <link rel="preload" href="Redaction-Regular.woff2" as="font" type="font/woff2" crossorigin>
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&display=swap">
    <link rel="stylesheet" href="styles.css" />
    <script src="https://unpkg.com/@phosphor-icons/web"></script>
</head>

<body>
    <a href="#main-content" class="skip-link">Skip to main content</a>
    <article role="main">
        <button class="nav-toggle" aria-expanded="false" aria-controls="side-nav" aria-label="Toggle navigation menu">
            <i class="ph ph-list" aria-hidden="true"></i>
        </button>

        <nav id="side-nav" class="side-nav" role="navigation" aria-label="Table of contents">
            <div class="nav-header">
                <span class="box-icon" aria-hidden="true">
                    <i class="ph ph-book-open-text"></i>
                </span>
                <span class="nav-title">Table of Contents</span>
            </div>
            <div id="nav-content" class="nav-content"></div>
            <div class="nav-footer">
                <div class="keyboard-shortcuts">
                    <button id="theme-toggle" class="shortcut-btn" data-key="1"
                        aria-label="Toggle between light and dark theme"><kbd>1</kbd> Change theme</button>
                    <button class="shortcut-btn" data-key="2" aria-label="Toggle all collapsible boxes"><kbd>2</kbd>
                        Toggle boxes</button>
                    <button class="shortcut-btn" data-key="3" aria-label="Scroll to top of page"><kbd>3</kbd> To
                        top</button>
                    <button class="shortcut-btn" data-key="ArrowLeft"
                        aria-label="Use left and right arrow keys to navigate between sections"><kbd>←</kbd>/<kbd>→</kbd>
                        Navigate</button>
                </div>
            </div>
        </nav>

        <section class="main-content" aria-label="Main content">
            <header class="main-header">
                <img src="c-light.png" alt="" class="decorative-letter c" aria-hidden="true">
                <img src="o-light.png" alt="" class="decorative-letter o" aria-hidden="true">
                <img src="p-light.png" alt="" class="decorative-letter p" aria-hidden="true">
                <img src="nose-red.svg" alt="Logo">
                <span class="header-text">EU AI ACT: General Purpose AI</span>
                <h1>Code<i>of</i><span>Practice</span></h1>
                <span class="header-text">DRAFT 3 · 10/03/2025</span>
                <nav class="header-nav-expanded" role="tablist" aria-label="Main sections">
                    <a href="?section=summary" role="tab" aria-controls="section-summary">SUMMARY</a>
                    <a href="?section=transparency" role="tab" aria-controls="section-transparency">TRANSPARENCY</a>
                    <a href="?section=copyright" role="tab" aria-controls="section-copyright">COPYRIGHT</a>
                    <a href="?section=safety-security" role="tab" aria-controls="section-safety-security">SAFETY &
                        SECURITY</a>
                </nav>
                <div class="header-collapsed">
                    <div class="collapsed-header-text-container">
                        <img src="nose-red.svg" alt="Logo" class="collapsed-nose">
                        <span class="collapsed-title">Code <i>of</i> Practice</span>
                    </div>
                    <nav class="header-nav-collapsed" role="tablist" aria-label="Main sections">
                        <a href="?section=summary" role="tab" aria-controls="section-summary">SUMMARY</a>
                        <a href="?section=transparency" role="tab" aria-controls="section-transparency">TRANSPARENCY</a>
                        <a href="?section=copyright" role="tab" aria-controls="section-copyright">COPYRIGHT</a>
                        <a href="?section=safety-security" role="tab" aria-controls="section-safety-security">SAFETY &
                            SECURITY</a>
                    </nav>
                </div>
            </header>

            <section data-section="summary" id="section-summary" class="content-section" role="tabpanel"
                aria-labelledby="section-summary-tab">
                <div class="disclaimer-box keep-open-initially" role="complementary" aria-label="Important disclaimer">
                    <h4><i class="ph ph-warning"></i>Disclaimer & Contributions<i
                            class="ph ph-caret-down box-caret"></i>
                    </h4>
                    <p>This website is an unofficial, best-effort service to make the Code of Practice more accessible
                        to all observers and working group participants. It contains the
                        full text of the third draft as well as two FAQs and an Explainer on parts of the Code. The
                        respective Chairs and Vice Chairs have written these to address questions posed by many
                        stakeholders.</p>

                    <p>While I strive for accuracy, this is
                        <strong>not an official legal document</strong>. Always refer to the <a
                            href="https://digital-strategy.ec.europa.eu/en/library/third-draft-general-purpose-ai-code-practice-published-written-independent-experts">official
                            PDF document</a> provided by the AI Office for the authoritative
                        version. Any discrepancies
                        between this site and the official document should be considered errors on my part.
                    </p>

                    <p>To help me improve this site please <a
                            href="https://github.com/dbltnk/code-of-practice.ai/issues">report
                            issues</a> or
                        <a href="https://github.com/dbltnk/code-of-practice.ai/pulls">submit pull requests</a> on
                        Github and
                        feel
                        free to reach out about
                        anything else <a href="mailto:alexander.zacherl@googlemail.com">via email</a>.
                    </p>

                    <p>Thanks for your support.</p>
                    <p>Alexander Zacherl</p>
                </div>

                <h2>Opening statement by the Chairs and Vice-Chairs</h2>

                <p>As the Chairs and Vice-Chairs of the four Working Groups, we hereby present the third draft of the
                    General-Purpose AI Code of Practice under the AI Act (the "Code"). Participants in the Working
                    Groups and observers of the Code of Practice Plenary are welcome to submit written feedback on this
                    draft by Sunday, 30 March 2025, via a dedicated survey shared with them.</p>

                <p>We encourage all readers — whether they have engaged with previous drafts or not — to visit this
                    website. It contains the text of this third draft as well as two FAQs and an Explainer on parts of
                    the code and is aimed at making the Code more accessible to all observers and working group
                    participants.</p>

                <p>The third draft significantly advances the content compared to the second draft. In the upcoming
                    final drafting round, it will be further improved based on stakeholder feedback. For this third
                    draft, we have focused primarily on streamlining the structure of the Code, providing
                    clarifications, adding essential details, and simplifying the Code.</p>

                <p>This third draft of the Code addresses key considerations for providers of general-purpose AI models
                    and providers of general-purpose AI models with systemic risk when complying with Chapter V of the
                    AI Act, through four Working Groups working in close collaboration:</p>

                <ul>
                    <li>Working Group 1: Transparency and copyright-related rules</li>
                    <li>Working Group 2: Risk assessment for systemic risk</li>
                    <li>Working Group 3: Technical risk mitigation for systemic risk</li>
                    <li>Working Group 4: Governance risk mitigation for systemic risk</li>
                </ul>

                <p>Working Group 1 Transparency applies to all general-purpose AI models, except for those that are
                    released under a free and open-source licence satisfying the conditions specified in Article 53(2)
                    AI Act and not classified as general-purpose AI models with systemic risk. Working Group 1 Copyright
                    applies to all general-purpose AI models. Working Groups 2, 3, and 4 (<a
                        href="?section=safety-security">Safety and Security</a> Section) only apply to providers of
                    general-purpose AI models classified as general-purpose AI models with systemic risk based on
                    Article 51 AI Act.
                </p>

                <p>Following a thorough review of the feedback received from stakeholders on the second draft, we have
                    refined Commitments and Measures while maintaining the Code's Objectives. We present this third
                    draft as the basis for the final drafting round, in which we will again draw on your feedback
                    provided via the EU survey, in provider workshops, and in Working Group meetings. Like in previous
                    drafting rounds, we have found your feedback extremely helpful, resulting in substantial changes. We
                    therefore encourage stakeholders to continue providing comprehensive feedback on all aspects of the
                    Code, including both new and unchanged elements. Your feedback will help shape the final version of
                    the Code, which will play a crucial role in guiding the future of general-purpose AI model
                    development and deployment.</p>

                <p>We have once again included a high-level drafting plan which outlines our guiding principles for the
                    Code, and the assumptions it is based on.</p>

                <p>The AI Act came into force on 1 August 2024, stating that the final version of the Code should be
                    ready by 2 May 2025. The third draft builds upon previous work while aiming to provide a
                    "future-proof" Code, appropriate for the next generation of models which will be developed and
                    released in 2025 and thereafter.</p>

                <p>In formulating this third draft, we have been principally guided by the provisions in the AI Act as
                    to matters within the scope of the Code. Accordingly, unless the context and definition contained
                    within the Code indicates otherwise, the terms used in the Code refer to identical terms from the AI
                    Act.</p>

                <p>Like the first and second drafts, this document is the result of a collaborative effort involving
                    hundreds of participants from across industry, academia, and civil society. It has been informed by
                    three rounds of feedback, including on the previous two drafts, which has been insightful and
                    instructive in our drafting process. We continue to be informed by the evolving literature on AI
                    governance, international approaches (as specified in Article 56(1) AI Act), industry
                    best practice, and the expertise and experience of providers and Working Group members.</p>

                <p>Key features of the development process of the Code include:</p>

                <ul>
                    <li>Drafted by Chairs and Vice-Chairs who were selected by the AI Office for their expertise,
                        experience, independence (including absence of financial interests), and to ensure gender and
                        geographic diversity.</li>
                    <li>A multi-stakeholder consultation which closed in September and received 427 submissions</li>
                    <li>A multi-stakeholder survey on the first draft of the Code which received 354 submissions, and on
                        the second draft which received 336 submissions</li>
                    <li>Provider workshops led by Chairs and Vice-Chairs</li>
                    <li>Four specialised working groups led by Chairs and Vice-Chairs</li>
                    <li>Meetings with representatives from EU Member States in the AI Board and from the European
                        Parliament</li>
                </ul>

                <p>Additional time for consultation and deliberation – both externally and internally – will be needed
                    to further improve the Code. As a group of independent Chairs and Vice-Chairs, we strive to make
                    this process as transparent and accessible to stakeholders as possible, aiming to share our work and
                    our thinking as early as possible, while taking sufficient time to coordinate and discuss key
                    questions within Working Groups. We count on your continued engaged collaboration and constructive
                    criticism.</p>

                <p>We welcome written feedback by the Code of Practice Plenary participants and observers by Sunday, 30
                    March 2025, via a dedicated survey shared with them.</p>

                <p>Thank you for your support!</p>

                <div class="signatures-container">
                    <p class="signature-line"><strong>Working Group 1, Transparency</strong><br> Nuria Oliver
                        (Co-Chair) &
                        Rishi Bommasani (Vice-Chair)</p>
                    <p class="signature-line"><strong>Working Group 1, Copyright</strong><br> Alexander Peukert
                        (Co-Chair)
                        & Céline Castets-Renard (Vice-Chair)</p>
                    <p class="signature-line"><strong>Working Group 2</strong><br> Matthias Samwald (Chair), Marta
                        Ziosi &
                        Alexander Zacherl (Vice-Chairs)</p>
                    <p class="signature-line"><strong>Working Group 3</strong><br> Yoshua Bengio (Chair), Daniel
                        Privitera
                        & Nitarshan Rajkumar (Vice-Chairs)</p>
                    <p class="signature-line"><strong>Working Group 4</strong><br> Marietje Schaake (Chair), Anka Reuel
                        &
                        Markus Anderljung (Vice-Chairs)</p>
                </div>

                <h2>Drafting plan, principles, and assumptions</h2>

                <p>This third draft provides a more streamlined structure with more nuanced Commitments and Measures. In
                    the upcoming final drafting round, it will be further improved based on stakeholder feedback. At
                    this stage, it still does not contain the level of clarity and coherence that we expect in the final
                    adopted version of the Code.</p>

                <p>The Code first outlines the Commitments. Concretely, these are 2 Commitments for providers of
                    general-purpose AI models and further 16 Commitments only for providers of general-purpose AI models
                    classified as general-purpose AI models with systemic risk. Next, in separate documents, the
                    Commitments are detailed out with respective Measures. The draft does not include KPIs and instead
                    sharpened the
                    reporting Commitments. Stakeholders should not expect the final adopted version of the Code to
                    contain KPIs.</p>

                <p>Related to transparency, Chairs have included a user-friendly Model Documentation Form which allows
                    Signatories to easily document the necessary information in a single place. With regards to the
                    review and adaptation of the Code, this draft includes an Appendix 2 with
                    recommendations to the AI Office.</p>

                <p>Below are some high-level principles we follow when drafting the Code:</p>

                <ol>
                    <li><strong>Alignment with EU Principles and Values</strong> – Commitments and Measures will be in
                        line with general principles and values of the Union, as enshrined in EU law, including the
                        Charter of Fundamental Rights of the European Union, the Treaty on European Union and Treaty on
                        the Functioning of the European Union.</li>

                    <li><strong>Alignment with the AI Act and International Approaches</strong> – Commitments and
                        Measures will contribute to a proper application of the AI Act. This includes taking into
                        account international approaches (including standards or metrics developed by AI Safety
                        Institutes, or standard-setting organisations), in accordance with Article 56(1) AI Act.</li>

                    <li><strong>Proportionality to Risks</strong> – Commitments and Measures should be proportionate to
                        risks, meaning they should be (i) suitable to achieve the desired end, (ii) necessary to achieve
                        the desired end, and (iii) should not impose a burden that is excessive in relation to the end
                        sought to be achieved. Some concrete applications of proportionality include:
                        <ol type="a">
                            <li>Commitments and Measures should be more stringent for higher risk tiers or uncertain
                                risks of severe harm.</li>
                            <li>Measures should be specific. While Commitments may be articulated at a higher level of
                                generality, general-purpose AI model providers should have a clear understanding of how
                                to meet Measures. Measures should be designed to be effective and robust against
                                misspecification or any attempts of circumvention. The Code strives to accomplish this
                                by, for example, avoiding unnecessary use of proxy terms or metrics. The AI Office will
                                monitor and review Measures that may be susceptible to circumvention and other forms of
                                misspecification.</li>
                            <li>Commitments and Measures should differentiate, where applicable, between different types
                                of risks, distribution strategies and deployment contexts of the concerned
                                general-purpose AI model, and other factors that may influence the tiers of risk, and
                                how risks need to be assessed and mitigated. For example, Commitments and Measures
                                assessing and mitigating systemic risks might need to differentiate between intentional
                                and unintentional risks, including instances of misalignment. Additionally, Commitments
                                may need to be adapted to take into account the different tools providers have available
                                to assess and mitigate systemic risk where model weights are freely released.</li>
                        </ol>
                    </li>

                    <li><strong>Future-Proof</strong> – AI technology is changing rapidly. Measures should maintain the
                        AI Office's ability to improve its assessment of compliance based on new information. Therefore,
                        the Code shall strive to facilitate its rapid updating, as appropriate. It is important to find
                        a balance between specific commitments on one hand, and the flexibility to update rapidly in
                        light of technological and industry developments on the other. The Code can accomplish this by,
                        for example, referencing dynamic sources of information that providers can be expected to
                        monitor and consider in their risk assessment and mitigation. Examples of such sources could
                        include incident databases, consensus standards, up-to-date risk registers, state-of-the-art
                        risk management frameworks, and AI Office guidance. As technology evolves, it may also be
                        necessary to articulate an additional set of Measures for specific general-purpose AI models,
                        for example, certain models used in agentic AI systems.</li>

                    <li><strong>Proportionality to the size of the general-purpose AI model provider</strong> – Measures
                        related to the obligations applicable to providers of general-purpose AI models should take due
                        account of the size of the general-purpose AI model provider and allow simplified ways of
                        compliance for small and medium enterprises (SMEs) and start-ups with fewer financial resources
                        than those at the frontier of AI development, where appropriate.</li>

                    <li><strong>Support and growth of the ecosystem for safe, human centric and trustworthy AI</strong>
                        – We recognise that the development, adoption, and governance of general-purpose AI models are
                        global issues. Many Commitments in this draft are intended to enable and support cooperation
                        between different stakeholders, for example by sharing general-purpose AI safety infrastructure
                        and best practices amongst model providers, or by encouraging the participation of civil
                        society, academia, third parties, and government organisations in evidence collection. We
                        promote further transparency between stakeholders and increased efforts to share knowledge and
                        cooperate in building a collective and robust evidence base for safe, human centric and
                        trustworthy AI in line with Article 56(1) and (3), Recital 1, and Recital 116 AI Act. We also
                        acknowledge the positive impact that open-source models have had on the development of safe,
                        human centric and trustworthy AI.</li>

                    <li><strong>Innovation in AI governance and risk management</strong> – We recognise that determining
                        the most effective methods for understanding and ensuring the safety of general-purpose AI
                        models remains an evolving challenge. The Code should encourage providers to compete in and
                        advance the state-of-the-art in AI safety governance and related evidence collection methods and
                        practices. When providers can demonstrate equal or superior safety outcomes through alternative
                        approaches that are less burdensome, these innovations should be recognised as improving the
                        state of the art of AI governance and evidence and we should support their wider adoption.</li>
                </ol>

                <p>The current draft is written with the <strong>assumption that there will only be a small number of
                        both
                        general-purpose AI models with systemic risk and providers thereof.</strong> That assumption
                    seems to be
                    confirmed by the information provided from the AI Office accompanying the publication of this draft.
                    The AI Office plans to publish guidance in due time to clarify the scope of the respective AI Act
                    rules in proximity to the publication of the final Code of Practice, including topics addressed in
                    the dedicated <a
                        href="https:/digital-strategy.ec.europa.eu/en/faqs/general-purpose-ai-models-ai-act-questions-answers"
                        target="_blank">Q&A</a> such as downstream modifiers to which obligations should only apply in
                    clearly
                    specified cases. In particular, we want to highlight that even if modifications of general-purpose
                    AI models increase the number of providers in scope, the modifiers’ obligations under Articles 53
                    and 55 AI Act should be limited to the extent of their respective modifications, as appropriate. We
                    expect more clarifications from the AI Office on these points on an ongoing basis, as stated in the
                    Q&A.</p>

                <h2>Preamble</h2>

                <ol type="a">
                    <li>The Signatories of this Code of Practice (hereafter, "Code") recognise the importance of
                        improving the functioning of the internal market and creating a level playing field for the
                        development, placing on the market, and use of human-centric and trustworthy artificial
                        intelligence (hereafter, "AI"), while ensuring a high level of protection of health, safety, and
                        the fundamental rights enshrined in the Charter, including democracy, the rule of law, and
                        environmental protection, against harmful effects of AI in the Union and supporting innovation
                        as emphasised in Article 1(1) AI Act. The Code shall be interpreted in this context.</li>
                    <li>The Signatories recognise that this Code is to be interpreted in conjunction and in accordance
                        with any European AI Office (hereafter, "AI Office") guidance on the AI Act and with applicable
                        Union laws.</li>
                    <li>Whenever the Code refers to providers of general-purpose AI models it shall encompass providers
                        of general-purpose AI models with systemic risk (hereafter "GPAISRs" or "GPAISR"), too. Whenever
                        the Code refers to providers of GPAISRs it shall not encompass providers of other
                        general-purpose AI models. This shall only include general-purpose AI models that are within the
                        scope of the AI Act.</li>
                    <li>The Signatories recognise that the Code serves as a guiding document for demonstrating
                        compliance with the AI Act, while recognising that adherence to the Code does not constitute
                        conclusive evidence of compliance with the AI Act.</li>
                    <li>The Signatories recognise the importance of regularly reporting to the AI Office on their
                        implementation of the Code and its outcomes (Article 56(5) AI Act), including to facilitate the
                        regular monitoring and evaluation of the Code's adequacy by the AI Office and the Board (Article
                        56(6) AI Act).</li>
                    <li>The Signatories recognise that the Code shall be subject to regular review by the AI Office and
                        the Board (Article 56(6) AI Act) and that the AI Office may encourage and facilitate updates of
                        the Code to reflect advances in AI technology, emerging standards, societal changes, and
                        emerging systemic risks (Article 56(8) AI Act), without prejudice to the need for Signatories to
                        sign such updates.</li>
                    <li>The Signatories recognise that the Code may serve as a bridge until the adoption of a harmonised
                        standard. Updates may be needed to facilitate a gradual transition towards future standards.
                    </li>
                    <li>The Signatories recognise that the absence of specific Commitments or Measures within this Code
                        does not absolve providers of GPAISRs from their responsibility to assess and mitigate systemic
                        risks.</li>
                    <li>The Signatories recognise the importance of working in partnership with the AI Office to foster
                        collaboration between providers of general-purpose AI models, researchers, and regulatory bodies
                        to address emerging challenges and opportunities in the AI landscape.</li>
                </ol>

                <h2>The Objectives of the Code are as follows:</h2>
                <ol type="I">
                    <li>Assisting providers of general-purpose AI models to effectively comply with their obligations
                        under the AI Act - if assessed as adequate by the AI Office and the Board (Article 56(6) AI
                        Act). The Code should also enable the AI Office to assess compliance of providers who choose to
                        rely on the Code to demonstrate compliance with their obligations under the AI Act. This can
                        involve, e.g., allowing sufficient visibility into trends in the development, making available,
                        and use of general-purpose AI models, particularly of the most advanced models.</li>
                    <li>Assisting providers of general-purpose AI models to effectively keep up-to-date technical
                        documentation of their models and to effectively ensure a good understanding of general-purpose
                        AI models along the entire AI value chain, both to enable the integration of such models into
                        downstream products and to fulfil subsequent obligations under the AI Act or other regulations
                        (see Articles 53(1)(a) and (b) and Recital 101 AI Act).</li>
                    <li>Assisting providers of general-purpose AI models to effectively comply with Union law on
                        copyright and related rights and increase transparency on the data that is used in the
                        pre-training and training of general-purpose AI models (see Articles 53(1)(c) and (d) and
                        Recitals 106 and 107 AI Act).</li>
                    <li>Assisting providers of GPAISRs to effectively and continuously assess and mitigate systemic
                        risks, including their sources, that may stem from the development, the placing on the market,
                        or the use of GPAISRs (see Article 55(1) and Recital 114 AI Act).</li>
                </ol>

                <h2>I. Commitments by Providers of General-Purpose AI Models</h2>

                <h3>Transparency Section</h3>

                <h4>Commitment I.1. Documentation</h4>
                <p>In order to fulfil the obligations in Article 53(1), points (a) and (b) AI Act, Signatories commit to
                    drawing up and keeping up-to-date model documentation in accordance with Measure I.1.1, providing
                    relevant information to providers of AI systems who intend to integrate the general-purpose AI model
                    into their AI systems (downstream providers hereafter), and to the AI Office upon request (possibly
                    on behalf of national competent authorities when this is strictly necessary for the exercise of
                    their supervisory tasks under the AI Act, in particular to assess the compliance of high-risk AI
                    systems built on general-purpose AI models where the provider of the system is different from the
                    provider of the model [Footnote: See Article 75(1) and (3) AI Act and Article 88(2) AI Act.]), in
                    accordance with Measure I.1.2, and ensuring quality, security, and
                    integrity of the documented information in accordance with Measure I.1.3. These Measures do not
                    apply to providers of open-source AI models satisfying the conditions specified in Article 53(2) AI
                    Act, unless the models are general-purpose AI models with systemic risk.</p>
                <p><a href="?section=transparency" class="commitment-link">Read
                        commitment and measures</a></p>

                <h3>Copyright Section</h3>

                <h4>Commitment I.2. Copyright policy</h4>
                <p>In order to fulfil the obligation to put in place a policy to comply with Union law on
                    copyright and related rights, and in particular to identify and comply with, including
                    through state-of-the-art technologies, a reservation of rights expressed pursuant to <a
                        class="ai-act-link"
                        href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32019L0790#art_4">Article
                        4(3) Directive (EU) 2019/790</a> pursuant to Article 53(1), point (c) AI Act,
                    Signatories commit to drawing up, keeping up-to-date, and implementing a copyright policy in
                    accordance with Measure I.2.1, as well as adopting Measures I.2.2—I.2.6 for their
                    general-purpose AI models placed on the EU market.</p>
                <p><a href="?section=copyright" class="commitment-link">Read commitment and measures</a></p>

                <h2>II. Commitments by Providers of General-Purpose AI Models with Systemic Risk</h2>

                <h3>Safety and Security Section</h3>

                <h4>Commitment II.1. Safety and Security Framework</h4>
                <p>Signatories commit to adopting and implementing a Safety and Security Framework (hereafter,
                    "the Framework") that will: (1) apply to the Signatories' GPAISRs; and (2) detail the
                    systemic risk assessment, systemic risk mitigation, and governance risk mitigation measures
                    and procedures that Signatories intend to adopt to keep systemic risks stemming from their
                    GPAISRs within acceptable levels.</p>
                <p><a href="?section=safety-security" class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.2. Systemic risk assessment and mitigation along the entire model lifecycle,
                    including during model development</h4>
                <p>Signatories commit to conducting systemic risk assessment systematically at appropriate
                    points along the entire model lifecycle, in particular before making the model available on
                    the market. Specifically, Signatories commit to starting to assess and mitigate systemic
                    risks during the development of a GPAISR, as specified in the Measures for this Commitment.
                </p>
                <p><a href="?section=safety-security#commitment-ii-2-systemic-risk-assessment-and-mitigation-along-the-entire-model-lifecycle-including-during-model-development-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.3. Systemic risk identification</h4>
                <p>Signatories commit to selecting and further characterising systemic risks stemming from their
                    GPAISRs that are significant enough to warrant further assessment and mitigation, as
                    specified in the Measures for this Commitment.</p>
                <p><a href="?section=safety-security#commitment-ii-3-systemic-risk-identification-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.4. Systemic risk analysis</h4>
                <p>As part of systemic risk assessment, Signatories commit to carrying out a rigorous analysis
                    of the systemic risks identified pursuant to Commitment II.3 in order to understand the
                    severity and probability of the systemic risks. Signatories commit to carrying out systemic
                    risk analysis with varying degrees of depth and intensity, as appropriate to the systemic
                    risk stemming from the relevant GPAISR and as specified in the Measures for this Commitment.
                    Whenever systemic risk mitigations are implemented, Signatories commit to considering their
                    effectiveness and robustness as part of systemic risk analysis.</p>
                <p>As further specified in the Measures for this Commitment, Signatories commit to making use of
                    a range of information and methods in their systemic risk analysis including
                    model-independent information and state-of-the-art model evaluations, taking into account
                    model affordances, safe originator models, and the context in which the model may be made
                    available on the market and/or used and its effects.</p>
                <p><a href="?section=safety-security#commitment-ii-4-systemic-risk-analysis-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.5. Systemic risk acceptance determination</h4>
                <p>Signatories commit to determining the acceptability of the systemic risks stemming from their
                    GPAISRs by comparing the results of their systemic risk analysis (pursuant to Commitment
                    II.4) to their pre-defined systemic risk acceptance criteria (pursuant to Measure II.1.2),
                    in order to ensure proportionality between the systemic risks of the GPAISR and their
                    mitigations. Signatories commit to using this comparison to inform the decision of whether
                    or not to proceed with the development, the making available on the market, and/or the use
                    of their GPAISR, as specified in the Measures for this Commitment.</p>
                <p><a href="?section=safety-security#commitment-ii-5-systemic-risk-acceptance-determination-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.6. Safety mitigations</h4>
                <p>Signatories commit, as specified in the Measures for this Commitment, to: (1) implementing
                    technical safety mitigations along the entire model lifecycle that are proportionate to the
                    systemic risks arising from the development, the making available on the market, and/or the
                    use of GPAISRs, in order to reduce the systemic risks of such models to acceptable levels,
                    and further reduce systemic risk as appropriate, in accordance with this Code; and (2)
                    ensuring that safety mitigations are proportionate and state-of-the-art.</p>
                <p><a href="?section=safety-security#commitment-ii-6-safety-mitigations-2" class="commitment-link">Read
                        commitment and measures</a></p>

                <h4>Commitment II.7. Security mitigations</h4>
                <p>Signatories commit to mitigating systemic risks that could arise from unauthorised access to
                    unreleased model weights of their GPAISRs and/or unreleased associated assets. Associated
                    assets encompass any information critical to the training of the model, such as algorithmic
                    insights, training data, or training code.</p>
                <p>Consequently, Signatories commit to implementing state-of-the-art security mitigations
                    designed to thwart such unauthorised access by well-resourced and motivated non-state-level
                    adversaries, including insider threats from humans or AI systems, so as to meet at least the
                    <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL3</a>
                    security goal or equivalent, and achieve higher security goals (e.g. <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL4</a> or
                    SL5), as specified in the Measures for this Commitment.
                </p>
                <p><a href="?section=safety-security#commitment-ii-7-security-mitigations-2"
                        class="commitment-link">Read
                        commitment and measures</a></p>

                <h4>Commitment II.8. Safety and Security Model Reports</h4>
                <p>Signatories commit to reporting to the AI Office about their implementation of the Code, and
                    especially the application of their Framework to the development, making available on the
                    market, and/or use of their GPAISRs, by creating a Safety and Security Model Report
                    (hereafter, a "Model Report") for each GPAISR which they make available on the market, which
                    will document, as specified in the Measures for this Commitment: (1) the results of systemic
                    risk assessment and mitigation for the model in question; and (2) justifications of
                    decisions to make the model in question available on the market.</p>
                <p><a href="?section=safety-security#commitment-ii-8-safety-and-security-model-reports-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.9. Adequacy assessments</h4>
                <p>Signatories commit to assessing the adequacy of their Framework, the adoption and
                    implementation of which they have committed to under Commitment II.1, and to updating it
                    based on the findings as specified in the Measures for this Commitment.</p>
                <p><a href="?section=safety-security#commitment-ii-9-adequacy-assessments-2"
                        class="commitment-link">Read
                        commitment and measures</a></p>

                <h4>Commitment II.10. Systemic risk responsibility allocation</h4>
                <p>For activities concerning systemic risk assessment and mitigation for their GPAISRs,
                    Signatories commit, as specified in the Measures for this Commitment, to: (1) clearly
                    defining and allocating responsibilities for managing systemic risk from their GPAISRs
                    across all levels of the organisation; (2) allocating appropriate resources to actors who
                    have been assigned responsibilities for managing systemic risk; and (3) promoting a healthy
                    risk culture.</p>
                <p>Signatories commit to allocating appropriate levels of responsibility and resources
                    proportionately to, at least, the Signatory's organisational complexity and governance
                    structure, and the systemic risks stemming from their GPAISRs.</p>
                <p><a href="?section=safety-security#commitment-ii-10-systemic-risk-responsibility-allocation-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.11. Independent external assessors</h4>
                <p>Before placing a GPAISR on the market, Signatories commit to obtaining independent external
                    systemic risk assessments, including model evaluations, unless the model can be deemed
                    sufficiently safe, as specified in Measure II.11.1. After placing the GPAISR on the market,
                    Signatories commit to facilitating exploratory independent external assessments, including
                    model evaluations, as specified in Measure II.11.2.</p>
                <p><a href="?section=safety-security#commitment-ii-11-independent-external-assessors-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.12. Serious incident reporting</h4>
                <p>Signatories commit, to the extent and under the conditions specified in Measures II.12.1 to
                    II.12.4, to setting up processes for keeping track of, documenting, and reporting to the AI
                    Office and, as appropriate, to national competent authorities without undue delay relevant
                    information about serious incidents throughout the entire model lifecycle and possible
                    corrective measures to address them, with adequate resourcing of such processes relative to
                    the severity of the serious incident and the degree of involvement of their model.</p>
                <p><a href="?section=safety-security#commitment-ii-12-serious-incident-reporting-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.13. Non-retaliation protections</h4>
                <p>Signatories commit to not retaliating against any worker providing information about systemic
                    risks stemming from the Signatories' GPAISRs to the AI Office or, as appropriate, to
                    national competent authorities, and to at least annually informing workers of an AI Office
                    mailbox designated for receiving such information, if such a mailbox exists.</p>
                <p><a href="?section=safety-security#commitment-ii-13-non-retaliation-protections-2"
                        class="commitment-link">Read commitment and measures</a></p>

                <h4>Commitment II.14. Notifications</h4>
                <p>Signatories commit, as specified in the Measures for this Commitment, to: (1) notifying the
                    AI Office of relevant information regarding their general-purpose AI models meeting the
                    condition for classification as GPAISRs; and (2) regularly notifying the AI Office of the
                    implementation of the Commitments and Measures of this Code. For the purpose of assessing
                    the implementation of this Code through the AI Office, Signatories commit to offering
                    clarifications, including via further documentation or interviews, where requested by the AI
                    Office.</p>
                <p><a href="?section=safety-security#commitment-ii-14-notifications-2" class="commitment-link">Read
                        commitment and measures</a></p>

                <h4>Commitment II.15. Documentation</h4>
                <p>Signatories commit to documenting relevant information under the AI Act and the Code, as
                    specified in Measure II.15.1.</p>
                <p><a href="?section=safety-security#commitment-ii-15-documentation-2" class="commitment-link">Read
                        commitment and measures</a></p>

                <h4>Commitment II.16. Public transparency</h4>
                <p>Signatories commit to publishing information relevant to the public understanding of systemic
                    risks stemming from their GPAISRs, where necessary to effectively enable assessment and
                    mitigation of systemic risks, to the extent and under the conditions specified in Measure
                    II.16.1.</p>
                <p><a href="?section=safety-security#commitment-ii-16-public-transparency-2"
                        class="commitment-link">Read
                        commitment and measures</a></p>

                <p><em>The foregoing Commitments are supplemented by Measures found in the relevant Transparency,
                        Copyright or Safety and Security section in the separate accompanying documents. [Note: On this
                        site shown as seperate sub-pages.]</em></p>
            </section>

            <section data-section="transparency" id="section-transparency" class="content-section" role="tabpanel"
                aria-labelledby="section-transparency-tab">

                <h2>Introductory note by the Chair and Vice-Chair of the Transparency Section</h2>

                <div class="faq-box collapsed" aria-expanded="false">
                    <h4><i class="ph ph-question"></i>Frequently Asked Questions: Transparency<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>This FAQ section provides additional explanatory information written and shared by the (vice)
                        chairs for WG1-Transparency. In case of any discrepancies between this section and the legal
                        text of the code, the legal text always takes precedence.</p>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: 1. When do providers need to report information to the AI Office (AIO) and/or
                                National Competent Authorities (NCA)? </strong></p>
                        <p>A: Providers do not need to proactively report information to the AI Office and/or the
                            National Competent Authorities. Instead, they must draw and keep up-to-date the
                            documentation specified in the transparency section of the Code of Practice so that it is
                            available for the AIO or NCA upon request. When these requests are made, they must be
                            legally justified and will generally only be for the relevant elements of documentation, not
                            all the documentation as a whole. Note that open-source GPAI models that do not pose
                            systemic risk are exempt from this obligation.
                        </p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: 2. If a GPAI model provider has produced a GPAI model by modifying another model,
                                what are their obligations?</strong></p>
                        <p>A: The AIO, as opposed to the CoP and its chairs, will clarify the conditions under which the
                            modification of an existing GPAI model constitutes a new GPAI model with AI Act obligations.
                            If the derivative model has AI Act GPAI obligations, then the provider need only document
                            the modification and link to the original model's Model Documentation if they have access to
                            it.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: 3. Will the CoP section on Transparency describe the specific cybersecurity
                                practices that the AIO and/or NCAs will implement to secure information provided to
                                these entities?</strong></p>
                        <p>A: No. These government bodies have cybersecurity obligations regarding the documentation to
                            be shared by GPAI model providers, but it is the responsibility of the AIO and/or NCAs --and
                            not of the CoP or its chairs-- to clarify what cybersecurity protections they will
                            implement.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: 4. How does a provider of an open-source model with no use restrictions satisfy
                                the obligations for an Acceptable Use Policy and a description of intended
                                uses?</strong></p>
                        <p>A: We provide guidance in the transparency template, including the option of providing no
                            Acceptable Use Policy if none exists and indicating N/A if describing intended uses is
                            incompatible with the model license.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: 5. How does a GPAI model provider satisfy the obligations for measuring
                                computational and energy costs in the absence of standards?</strong></p>
                        <p>A: Given the lack of established standards, the Code of Practice does not refer to standards
                            to compute these measures at this time. However, the absence of standards does not absolve
                            companies of their AI Act obligations on these matters. Therefore, the obligations instead
                            require a description of the measurement methodology, including the potential use of
                            estimates, without prescribing any specific methodology at this time. Consistent with the
                            text of the AI Act, a future delegated act on these topics is acknowledged in the Code of
                            Practice.</p>
                    </div>
                </div>

                <p>The Transparency section of the Code of Practice describes three Measures which Signatories commit to
                    taking to comply with their transparency obligations under Article 53(1)(a) and (b) and the
                    corresponding Annexes XI and XII AI Act.</p>

                <p>In this third draft, to streamline fulfilment of the commitments contained in Measure I.1.1 and
                    facilitate Signatories' compliance, we have included a user-friendly Model Documentation Form which
                    allows Signatories to easily document the necessary information in a single place.</p>

                <p>The Form clearly indicates for each item whether it is intended for downstream providers, the AI
                    Office or national competent authorities. Whilst information intended for downstream providers
                    should be made available to them proactively, information intended for the AI Office or national
                    competent authorities is only to be made available following a request from the AI Office, either
                    <em>ex
                        officio</em> or based on a request to the AI Office from national competent authorities. Such
                    requests
                    will state the legal basis and purpose of the request and will concern only items from the Form
                    strictly necessary for the AI Office to fulfil its tasks under the AI Act at the time of the
                    request, or strictly necessary for national competent authorities to exercise their supervisory
                    tasks under the AI Act at the time of the request, in particular to assess compliance of high-risk
                    AI systems built on general-purpose AI models where the provider of the system is different from the
                    provider of the model.
                </p>

                <p>Finally, in accordance with Article 78 AI Act, the recipients of any of the information contained in
                    the Model Documentation Form are obliged to respect the confidentiality of the information obtained,
                    in particular intellectual property rights and confidential business information or trade secrets,
                    and to put in place adequate and effective cybersecurity measures to protect the security and
                    confidentiality of the information obtained.</p>

                <div class="signatures-container">
                    <p class="signature-line"><strong>Working Group 1, Transparency</strong><br> Nuria Oliver
                        (Co-Chair) &
                        Rishi Bommasani (Vice-Chair)</p>
                </div>

                <h2>Recitals for the Transparency Section</h2>

                <p><em>Whereas:</em></p>
                <ol type="a">
                    <li>The Signatories recognise the particular role and responsibility of providers of general-purpose
                        AI models along the AI value chain, as the models they provide may form the basis for a range of
                        downstream systems, often provided by downstream providers that need significant understanding
                        of the models and their capabilities, both to enable the integration of such models into their
                        products and to fulfil their obligations under the AI Act (see Recital 101 AI Act).</li>
                    <li>The Signatories recognise that in the case of a modification or fine-tuning of a model, the
                        obligations for providers should be limited to that modification or fine-tuning to safeguard
                        proportionality (see Recital 109 AI Act).</li>
                </ol>

                <h3>Commitment I.1 Documentation</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 53(1)(a), 53(1)(b), 53(2), 53(7), and Annexes XI and XII AI Act</p>
                </div>

                <p>In order to fulfil the obligations in Article 53(1), points (a) and (b) AI Act, Signatories commit to
                    drawing up and keeping up-to-date model documentation in accordance with Measure I.1.1, providing
                    relevant information to providers of AI systems who intend to integrate the general-purpose AI model
                    into their AI systems (downstream providers hereafter), and to the AI Office upon request (possibly
                    on behalf of national competent authorities upon request to the AI Office when this is strictly
                    necessary for the exercise of their supervisory tasks under the AI Act, in particular to assess
                    compliance of high-risk AI systems built on general-purpose AI models where the provider of the
                    system is different from the provider of the model [Footnote: See Article 75(1) and (3) AI Act and
                    Article 88(2) AI Act.]), in accordance with Measure I.1.2, and ensuring
                    quality, security, and integrity of the documented information in accordance with Measure I.1.3.
                    These Measures do not apply to providers of open-source AI models satisfying the conditions
                    specified in Article 53(2) AI Act, unless the models are general-purpose AI models with systemic
                    risk.
                </p>

                <h4>Measure I.1.1. Drawing up and keeping up-to-date model documentation</h4>
                <p>Signatories, when placing a general-purpose AI model on the market, commit to having prepared a
                    document entitled "Information and Documentation about the General-Purpose AI Model" (hereafter
                    Model Documentation) containing all the information referred to in the Model Documentation Form
                    below.</p>
                <p>Signatories commit to reporting the information requested in the Computational Resources and Energy
                    Consumption sections in consistency with any delegated act adopted in accordance with Article 53(5)
                    AI Act to detail measurement and calculation methodologies with a view to allowing for comparable
                    and verifiable documentation.</p>
                <p>In case of relevant changes in the information contained in the Model Documentation, Signatories
                    commit to update the Model Documentation to reflect the new information while keeping previous
                    versions of the Model Documentation for a period ending 10 years after the model has been placed on
                    the market.</p>

                <h4>Measure I.1.2. Providing relevant information</h4>

                <p>Signatories, when placing a general-purpose AI model on the market, commit to publicly disclosing via
                    their website, or via another means if they do not have a website, contact information for the AI
                    Office and downstream providers to request access to the relevant information contained in the Model
                    Documentation.</p>

                <p>Upon a request from the AI Office pursuant to Articles 91 or 75(3) AI Act for one or more elements of
                    the Model Documentation that are strictly necessary for the AI Office to fulfil its tasks under the
                    AI Act or for national competent authorities when this is strictly necessary for the exercise of
                    their supervisory tasks under the AI Act, in particular to assess compliance of high-risk AI systems
                    built on general-purpose AI models where the provider of the system is different from the provider
                    of the model [Footnote: See Article 75(1) and (3) AI Act and Article 88(2) AI Act.], Signatories
                    commit to providing the relevant elements contained in the most
                    up-to-date Model Documentation, or otherwise the necessary additional information, subject to the
                    confidentiality safeguards and conditions provided for under Articles 53(7) and 78 AI Act.</p>

                <p>Signatories commit to providing to downstream providers the information contained in the most
                    up-to-date Model Documentation and intended for downstream providers, subject to the confidentiality
                    safeguards and conditions provided for under Articles 53(7) and 78 AI Act. Furthermore, subject to
                    the same confidentiality safeguards and conditions, Signatories commit to providing additional
                    information necessary to enable downstream providers to have a good understanding of the
                    capabilities and limitations of the general-purpose AI model and to comply with their obligations
                    pursuant to the AI Act.</p>

                <p>Signatories commit to taking all the above-described actions in a timely manner.</p>

                <p>Signatories are encouraged to consider whether the documented information can be disclosed, in whole
                    or in part, to the public to promote public transparency. Some of this information may also be
                    required in a summarised form as part of the public summary for training content that providers must
                    make publicly available under Article 53(1), point (d) AI Act to be determined in a template to be
                    provided by the AI Office.</p>

                <h4>Measure I.1.3. Ensuring quality, integrity, and security of information</h4>
                <p>Signatories commit to ensuring that the documented information is controlled for quality and
                    integrity, retained as evidence of compliance with obligations of the AI Act, and protected from
                    unintended alterations. In the context of drawing-up, updating, and controlling the quality and
                    security of the information
                    and records, Signatories are encouraged to follow the established protocols and technical standards.
                </p>

                <div class="gallery-container">
                    <h3>Model Documentation Form</h3>
                    <p>Below is a static, non-editable version of the Model Documentation Form. In this version, the
                        input fields cannot be filled in, and the button at the bottom of the form—intended to generate
                        a version of the form containing only information intended for downstream providers—is
                        non-functional. In the final draft of the Code, this Form will be fully interactive and editable
                    </p>
                    <div class="gallery-grid">
                        <div class="gallery-item"><img src="model-documentation-form-1.png"
                                alt="Model Documentation Form page 1"></div>
                        <div class="gallery-item"><img src="model-documentation-form-2.png"
                                alt="Model Documentation Form page 2"></div>
                        <div class="gallery-item"><img src="model-documentation-form-3.png"
                                alt="Model Documentation Form page 3"></div>
                        <div class="gallery-item"><img src="model-documentation-form-4.png"
                                alt="Model Documentation Form page 4"></div>
                    </div>
                </div>

                <!-- Gallery overlay -->
                <div class="gallery-overlay" id="gallery-overlay">
                    <div class="gallery-overlay-content">
                        <button class="gallery-close-btn" aria-label="Close gallery">&times;</button>
                        <button class="gallery-nav-btn gallery-prev-btn" aria-label="Previous image"><i
                                class="ph ph-caret-left"></i></button>
                        <button class="gallery-nav-btn gallery-next-btn" aria-label="Next image"><i
                                class="ph ph-caret-right"></i></button>
                        <img id="gallery-overlay-image" src="" alt="Enlarged document">
                    </div>
                </div>

            </section>
            <section data-section="copyright" id="section-copyright" class="content-section" role="tabpanel"
                aria-labelledby="section-copyright-tab">

                <h2>Introductory note by the Chair and Vice-Chair of the Copyright Section</h2>

                <p>The Copyright section of the Code of Practice describes a set of Measures that Signatories commit to
                    taking in order to comply with their obligation under Article 53(1)c) AI Act. The third draft
                    retains core elements of the first two drafts in a simplified and clearer form. Measure I.2.1
                    (former Measures 2.1 and 2.2, Draft 2) specifies what it means to "put in place" a policy to comply
                    with Union copyright law and clarifies the set of Measures covered. While Measures I.2.2 and I.2.3
                    (former Measures 2.4-2.8, Draft 2) regulate the mining of web-crawled content, Measure I.2.4 (former
                    Measure 2.3) concerns the mining of protected content not web-crawled by the Signatory. Measure
                    I.2.5 (former Measures 2.9 and 2.10, Draft 2) sets out commitments to mitigate the risk that a
                    downstream AI system repeatedly generates infringing output. Finally, Measure I.2.6 (former Measure
                    2.11, Draft 2) provides for commitments to designate a point of contact and to allow for the
                    submission of complaints concerning the non-compliance of Signatories with their commitments under
                    the copyright Section. As indicated in the General Introduction, this Section no longer contains
                    separate KPIs. However, the features of some KPIs have been incorporated into the Measures (e.g.
                    description of the policy in a single document, easily accessible information on the contact point
                    and the possibility to lodge complaints). The recitals clarify that this Section is without
                    prejudice to the application and enforcement of Union law on copyright and related rights and to
                    commercial agreements between the Signatories and rightsholders authorising the use protected
                    content.</p>

                <div class="signatures-container">
                    <p class="signature-line"><strong>Working Group 1, Copyright</strong><br> Alexander Peukert
                        (Co-Chair)
                        & Céline Castets-Renard (Vice-Chair)</p>
                </div>

                <h2>Recitals for the Copyright Section</h2>

                <p><em>Whereas:</em></p>
                <ol type="a">
                    <li>This Section aims to contribute to the proper application of the obligation of providers of
                        general-purpose AI models placed on the Union market to put in place a policy to comply with
                        Union law on copyright and related rights pursuant to Article 53(1), point (c) AI Act.</li>
                    <li>The compliance with the commitments under this Section should be commensurate and proportionate
                        to the size and capacities of providers, taking due account of the interests of SMEs, including
                        startups.</li>
                    <li>This Section is without prejudice to and in no way affects the application and enforcement of
                        Union law on copyright and related rights.</li>
                    <li>This Section is without prejudice to commercial agreements between the Signatories and
                        rightsholders authorising the use of works and other protected subject matter.</li>
                </ol>

                <h3>Commitment I.2. Copyright policy</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 53(1)(c) AI Act</p>
                </div>

                <p>In order to fulfil the obligation to put in place a policy to comply with Union law on copyright and
                    related rights, and in particular to identify and comply with, including through state-of-the-art
                    technologies, a reservation of rights expressed pursuant to <a class="ai-act-link"
                        href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_4">Article 4(3)
                        of Directive (EU)
                        2019/790</a>
                    pursuant to Article 53(1), point (c) AI Act, Signatories commit to drawing up, keeping up-to-date,
                    and implementing a copyright policy in accordance with Measure I.2.1, as well as adopting Measures
                    I.2.2—I.2.6 for their general-purpose AI models placed on the EU market.</p>

                <h4>Measure I.2.1. Draw up, keep up-to-date and implement a copyright policy</h4>
                <ol>
                    <li>Signatories will draw up, keep up-to-date and implement a policy to comply with Union law on
                        copyright and related rights. This policy will address all commitments pursuant to this Section
                        and
                        will be described in a single document approved by the Signatory. Signatories will assign
                        responsibilities within their organisation for the implementation and overseeing of this policy.
                    </li>
                    <li>Signatories are encouraged to make publicly available and keep up-to-date a summary of their
                        copyright policy.</li>
                </ol>

                <h4>Measure I.2.2. Reproduce and extract only lawfully accessible copyright-protected content when
                    crawling the World Wide Web</h4>
                <p>In order to ensure that Signatories will only reproduce and extract lawfully accessible works and
                    other protected subject matter if they use web-crawlers or have such web-crawlers be used on their
                    behalf to crawl, scrape and/or otherwise compile data for the purpose of text and data mining
                    according to <a class="ai-act-link"
                        href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_2">Article 2(2)
                        of Directive (EU)
                        2019/790</a> and the training of their general-purpose AI
                    models, Signatories will:</p>
                <ol type="a">
                    <li>not circumvent effective technological measures as defined in <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32001L0029">Article 6(3) of
                            Directive (EC)
                            2001/29</a> designed to prevent or restrict access to works and other protected subject
                        matter, such as paywalls, and</li>
                    <li>make reasonable efforts to exclude from their web-crawling Internet domains that make available
                        to the public copyright-infringing content on a commercial scale and have no substantial
                        legitimate uses ("piracy domains") and that are recognised as such by courts or public
                        authorities in the European Union and the European Economic Area. A list of hyperlinks to
                        relevant piracy domain lists issued by the relevant bodies in the European Union and the
                        European Economic Area is to be made publicly available on an EU website.</li>
                </ol>


                <h4>Measure I.2.3. Identify and comply with rights reservations when crawling the World Wide Web</h4>
                <ol>
                    <li>In order to ensure that Signatories will identify and comply with, including through
                        state-of-the-art technologies, machine-readable reservations of rights expressed pursuant to
                        <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_4">Article
                            4(3)
                            of Directive (EU)
                            2019/790</a> if they use web-crawlers or have such web-crawlers used
                        on their behalf to crawl, scrape and/or otherwise compile data for the purpose of text and data
                        mining according to <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_2">Article
                            2(2)
                            of Directive (EU)
                            2019/790</a> and the training of their
                        general-purpose AI models, Signatories will
                        <ol type="a">
                            <li>employ web-crawlers that read and follow instructions expressed in accordance with the
                                Robot Exclusion Protocol (robots.txt), as specified in the Internet Engineering Task
                                Force (IETF) <a class="ai-act-link"
                                    href="https://datatracker.ietf.org/doc/rfc9309/">Request for
                                    Comments No. 9309</a>, and any subsequent version of this IETF
                                standard, and</li>
                            <li>make best efforts to identify and comply with other appropriate machine-readable
                                protocols to express rights reservations pursuant to <a class="ai-act-link"
                                    href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_4">Article
                                    4(3) of Directive (EU) 2019/790</a>, for example through asset-based or
                                location-based metadata, that have either
                                resulted from a cross-industry standard-setting process as referred to in paragraph 3 of
                                this Measure or are state-of-the-art and widely adopted by rightsholders, considering
                                different cultural sectors, and generally agreed through an inclusive process based on
                                bona fide discussions to be facilitated at EU level with the involvement of
                                rightsholders, AI providers and other relevant stakeholders as a more immediate
                                solution, while anticipating the development of cross-industry standards referred in
                                paragraph 3.</li>
                        </ol>
                    </li>
                    <li>This commitment is without prejudice to the right of rightsholders to expressly reserve the use
                        of lawfully accessible works and other protected subject matter for the purposes of text and
                        data mining pursuant to <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_4">Article
                            4(3)
                            of Directive (EU)
                            2019/790</a> in any appropriate manner, such
                        as machine-readable means in the case of content made publicly available online.</li>
                    <li>In due consideration of relevant international and European standard-setting processes,
                        Signatories are encouraged to support relevant standardisation efforts and engage on a voluntary
                        basis in bona fide discussions with other relevant stakeholders, including rightsholders, with
                        the aim to develop appropriate machine-readable standards to express a rights reservation
                        pursuant <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_4">Article
                            4(3)
                            of Directive (EU)
                            2019/790</a>.</li>
                    <li>Signatories will take reasonable measures to enable affected rightsholders to obtain information
                        about the web crawlers employed and their robot.txt features and the measures that a Signatory
                        adopts to identify and comply with rights reservations expressed pursuant to <a
                            class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_4">Article
                            4(3)
                            of Directive (EU)
                            2019/790</a> at the time of crawling, for example by making public such information
                        and syndicating a web feed that covers every update of the website informing about the rights
                        reservation compliance.</li>
                    <li>Signatories that also provide an online search engine as defined in <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/eli/reg/2022/2065/oj/eng#art_3">Article 3(j) Regulation (EU)
                            2022/2065</a> or control such a provider are encouraged to take appropriate measures to
                        ensure that
                        its compliance with a rights reservation expressed in accordance with paragraph 1 of this
                        Measure does not negatively affect the findability of the content, for which a rights
                        reservation has been expressed, through their search engine.</li>
                </ol>


                <h4>Measure I.2.4. Obtain adequate information about protected content not web-crawled by the Signatory
                </h4>
                <ol>
                    <li>If Signatories, for the training of their general-purpose AI models, mine works and other
                        protected subject matter according to <a class="ai-act-link"
                            href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32019L0790#art_2">Article
                            2(2)
                            of Directive (EU)
                            2019/790</a> that they have
                        obtained by other means than by web-crawling or by web-crawling on their behalf and without an
                        authorisation by the respective rightsholders or their authorised representatives, they will
                        make reasonable efforts to obtain adequate information (e.g., by checking the information
                        available on the website of the third parties or requesting information) as to whether works and
                        other protected subject matter that have been scraped or crawled from the internet were
                        collected by employing web-crawlers that read and follow instructions expressed in accordance
                        with the Robot Exclusion Protocol (robots.txt), as specified in the Internet Engineering Task
                        Force (IETF) <a class="ai-act-link" href="https://datatracker.ietf.org/doc/rfc9309/">Request for
                            Comments No. 9309</a>, and any subsequent version of this IETF standard.
                    </li>
                    <li>This Measure does not imply a commitment to verify or proceed to a work-by-work assessment of
                        the data mentioned in paragraph 1 of this Measure in terms of copyright compliance.</li>
                </ol>

                <h4>Measure I.2.5. Mitigate the risk of production of copyright-infringing output</h4>
                <ol>
                    <li>In order to mitigate the risk that a downstream AI system, into which a generative
                        general-purpose AI model is integrated, repeatedly generates output that infringes copyrights or
                        related rights as protected according to Union law on copyright and related rights, Signatories
                        will
                        <ol type="a">
                            <li>make reasonable efforts to mitigate the risk that a model memorizes copyrighted training
                                content to the extent that it repeatedly produces copyright-infringing outputs and</li>
                            <li>prohibit copyright-infringing uses of a model in their acceptable use policy, terms and
                                conditions, or other equivalent documents.</li>
                        </ol>
                    </li>
                    <li>This Measure applies irrespective of whether a Signatory vertically integrates the model into
                        its own AI system(s) or whether the model is provided to another entity based on contractual
                        relations. The commitment to prohibit copyright-infringing uses pursuant to paragraph 1, point b
                        of this Measure does not apply to general-purpose AI models that are released under a free and
                        open-source licence.</li>
                </ol>

                <h4>Measure I.2.6. Designate a point of contact and enable the lodging of complaints</h4>
                <ol>
                    <li>Signatories will designate a point of contact for communication with affected rightsholders and
                        provide easily accessible information about it.</li>
                    <li>Signatories will put a mechanism in place to allow affected rightsholders and their authorised
                        representatives, including collective management organisations, to submit, by electronic means,
                        sufficiently precise and adequately substantiated complaints concerning the non-compliance of
                        Signatories with their commitments pursuant to this Section and provide easily accessible
                        information about it. Where complaints by rightsholders are manifestly unfounded or excessive,
                        in particular because of
                        their repetitive character, Signatories may refuse to act on the complaint.</li>
                </ol>
            </section>

            <section data-section="safety-security" id="section-safety-security" class="content-section" role="tabpanel"
                aria-labelledby="section-safety-security-tab">

                <h2>Commitments by Providers of GPAISRs</h2>

                <div class="faq-box" aria-expanded="false">
                    <h4><i class="ph ph-question"></i> Frequently Asked Questions: Safety & Security<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p><em>This FAQ, written and shared by the (vice) chairs of Working Groups 2–4, aims to provide
                            answers to some questions that the chairs frequently receive about the part of the Code of
                            Practice that pertains to general-purpose AI models with systemic risk (GPAISRs), also
                            called the “Safety and Security Section” of the Code. It is intended to support readers’
                            understanding of the Code of Practice; if and to the extent that this FAQ contradicts the
                            Code, the Code takes precedence.</em></p>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What is the purpose of the Code of Practice?</strong></p>
                        <p>A: The Code of Practice provides detail on how companies can comply with some of the
                            obligations in the AI Act.</p>

                        <p>The AI Act imposes certain obligations on providers of general-purpose AI models (GPAIs) and
                            general-purpose AI models with systemic risks (GPAISRs). Since these obligations are
                            formulated at a relatively high level of abstraction and as there is a lack of relevant
                            industry standards, it can be challenging for AI providers to know how to comply with these
                            obligations. In light of that, the role of the Code is to give providers of GPAI and GPAISR
                            a blueprint of one way to comply with the AI Act, so as to provide more legal certainty.</p>

                        <p>When the Code is finished, providers can choose to sign and adhere to it. Meanwhile, the AI
                            Board (which consists of representatives from all EU Member States) and the European AI
                            Office ("AI Office") will assess whether the Code covers the relevant obligations in the
                            Act, after which the AI Office can approve the Code and give it "general validity within the
                            Union" (see Article 56(6) of the AI Act). If it does, then adherence to the Code of Practice
                            becomes a means to demonstrate compliance with the AI Act.</p>
                    </div>
                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: How is the Code of Practice written?</strong></p>
                        <p>A: The drafting of the Code is an iterative process that benefits from repeated input from
                            companies and other stakeholders and has now seen the publication of the third and
                            penultimate draft.</p>

                        <p>As chairs, we process the input from the various stakeholders to write the Code. Chairs are
                            independent experts that have been appointed by the AI Office after an open application
                            process. For the Safety and Security Section of the Code, we are a group of nine co-chairs,
                            including, for example, a former employee of a leading AI company, a former employee of
                            small AI startups, the second most-cited AI researcher in the world, and a former member of
                            the European Parliament. More about the chairs and how we were selected can be found <a
                                href="https://digital-strategy.ec.europa.eu/en/news/meet-chairs-leading-development-first-general-purpose-ai-code-practice"
                                target="_blank">here</a>.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What is the timeline for the drafting of the Code?</strong></p>
                        <p>A: The drafting process started in October 2024. The AI Act states that the Code should be
                            finished by 2 May 2025, after which providers can choose to sign it. The AI Office and the
                            AI Board will also assess whether the Code covers the relevant obligations of the AI Act,
                            and the AI Office may approve the Code. The AI Act's obligations specific to GPAISR
                            providers come into force in August 2025, with a one-year grace period during which the AI
                            Office cannot issue fines for non-compliance.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What is the Safety and Security Section of the Code of Practice?</strong></p>
                        <p>A: The Safety and Security Section of the Code specifies one way of complying with one subset
                            of the AI Act's GPAI obligations: the obligations for GPAISRs.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: To whom is the Safety and Security Section of the Code relevant?</strong></p>
                        <p>A: It is only relevant to the small number of AI companies that are subject to the AI Act's
                            obligations concerning GPAI that present systemic risk. The AI Act (Article 3) defines
                            systemic risk as being specific to high-impact capabilities, i.e. capabilities that match or
                            exceed the capabilities of the most advanced GPAI, that have a significant impact on the
                            Union market (as stipulated in the AI Act). The Act's obligations for GPAISR should
                            therefore only apply to a small number of companies.</p>

                        <p>As the drafters of the Safety and Security Section of the Code, we have no discretion over
                            how the regulator will interpret the GPAISR definition in practice, but have been writing
                            the Safety and Security Section assuming that only about 5-15 companies will be in scope of
                            the AI Act's GPAISR obligations at any given point in time.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What are the guiding principles as the Code and specifically its Safety and
                                Security Section is being drafted?</strong></p>
                        <p>A: In summary, our guiding principles for the drafting of the entire Code, including its
                            Safety and Security Section, are:</p>
                        <ol>
                            <li><strong>Alignment with EU principles, rights and values.</strong> The Code should be in
                                line with the
                                values of the Union that are enshrined, for example, in the EU Charter of Fundamental
                                Rights.</li>
                            <li><strong>Alignment with the AI Act and international approaches.</strong> The Code should
                                contribute to an
                                appropriate application of the AI Act, detailing the requirements for GPAI models. It
                                should also, where appropriate, align with international approaches.</li>
                            <li><strong>Proportionality to risks.</strong> Compliance measures related to any given risk
                                should be
                                effective and not excessively burdensome.</li>
                            <li><strong>A future-proof approach.</strong> In the face of potential rapid technological
                                developments, the
                                Code needs to be able to be updated in an agile way.</li>
                            <li><strong>Proportionality to the size of a GPAI model provider.</strong> Where
                                appropriate, compliance
                                requirements should take into account the size of a GPAI model provider company, in
                                order to offer simplified ways of compliance for smaller companies and start-ups with
                                fewer financial resources than those at the frontier of AI development.</li>
                            <li><strong>Support and growth of the AI safety ecosystem.</strong> The Code should aim to
                                contribute to an
                                ecosystem in which stakeholders feel empowered to innovate within a structured and
                                predictable framework, whilst nurturing a culture of shared responsibility and progress.
                            </li>
                            <li><strong>Innovation in AI governance and risk management.</strong> The Code should
                                facilitate and support
                                innovation in AI governance and risk management to improve our ability to assess and
                                mitigate systemic risks.</li>
                        </ol>
                        <p>For more detail, see the section titled "Drafting plan, principles, and assumptions" in the
                            third draft of the Code.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Is the Safety and Security Section of the Code relevant to providers of
                                open-source models?</strong></p>
                        <p>A: While open-source models are exempt from some requirements in the AI Act (Article 53), if
                            an open-source model is a GPAISR, its provider is not exempt from the AI Act's obligations.
                            As such, providers of open-source GPAISRs still need to, for example, assess and mitigate
                            systemic risks in a manner proportionate to those risks.</p>
                        <p>Of course, open-source releases change what providers can do in terms of risk assessment and
                            mitigation. For example, it may be harder to predict how a model will be used. At the same
                            time, open-source releases can offer societal benefits beyond those of closed models.</p>
                        <p>We have taken this into account in drafting the Safety and Security Section of the Code. For
                            example, we have given providers options for how they conduct post-market monitoring – e.g.
                            given that providers of open-source GPAISRs would struggle to monitor outputs from their
                            models – and by clarifying that release decisions ought to take into account the benefits as
                            well as the accompanying risks of the GPAISR when made available on the market. We welcome
                            suggestions on how the Safety and Security Section of the Code can be further adjusted for
                            open-source GPAISR releases.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Will all models over 10^25 FLOP be automatically in scope of the AI Act's GPAISR
                                requirements?</strong></p>
                        <p>A: No. The 10^25 FLOP training compute threshold (AI Act, Article 51) provides a rebuttable
                            presumption of classification as GPAISR. However, models trained with more FLOP may not be
                            classified as GPAISR, and models trained with less FLOP may be classified as GPAISR.
                            Overall, decisions on how to interpret the scope of, apply, and enforce the AI Act's GPAISR
                            obligations are to be made by the AI Office. We, as the drafters of the Safety and Security
                            Section of the Code, have no authority to determine these matters.</p>
                        <p>We have been writing the Safety and Security Section assuming that only about 5-15 companies
                            will be in scope of the AI Act's GPAISR obligations at any given point in time. We base this
                            assumption on the AI Act defining systemic risk as being specific to high-impact
                            capabilities, i.e. capabilities that match or exceed the capabilities of the most advanced
                            GPAI, that have a significant impact on the Union market (as stipulated in the AI Act) and
                            the AI Office having powers to adjust the scope of the GPAISR obligations, including by
                            adjusting the training compute threshold.</p>
                        <p>However, to keep the Safety and Security Section of the Code SME-friendly even in a scenario
                            in which the number of models and companies affected by the AI Act's obligations for GPAISR
                            does increase significantly, the Section is designed to make compliance easier for SMEs:
                            they can use shared methods for risk assessment and have lighter documentation requirements.
                        </p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Will downstream developers who modify a GPAISR be in scope of the AI Act's GPAISR
                                requirements?</strong></p>
                        <p>A: The decision on how to interpret the scope of the AI Act sits with the AI Office,
                            including on how to categorise downstream developers who modify an existing GPAISR. These
                            are actors who modify a model developed by someone else, e.g. by fine-tuning or conducting
                            post-training on a GPAISR. Our working assumption in drafting the Safety and Security
                            Section of the Code has been that such actors fall outside the scope of the GPAISR
                            requirements in the Act; or, if they are in scope, we recommend that such obligations are
                            not enforced against such models produced by fine-tuning, at least until there are reasons
                            to believe that such fine-tuning could introduce novel and unacceptable risk. We expect the
                            AI Office to publish guidance on this question soon.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What is a Safety and Security Framework?</strong></p>
                        <p>A: The "Safety and Security Framework", a central part of the Safety and Security Section of
                            the Code, is a type of risk management framework already used by several AI companies. At
                            the time of writing, AI <a href="https://metr.org/faisc" target="_blank">companies that have
                                published</a> their version of such
                            a framework include <a
                                href="https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf"
                                target="_blank">Anthropic</a>, <a
                                href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf"
                                target="_blank">OpenAI</a>,
                            <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf"
                                target="_blank">Google DeepMind</a>, <a
                                href="https://ai.meta.com/static-resource/meta-frontier-ai-framework/"
                                target="_blank">Meta</a>, <a
                                href="https://cohere.com/security/the-cohere-secure-ai-frontier-model-framework-february-2025.pdf"
                                target="_blank">Cohere</a>, <a
                                href="https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Frontier-Governance-Framework.pdf"
                                target="_blank">Microsoft</a>,
                            <a href="https://assets.amazon.science/a7/7c/8bdade5c4eda9168f3dee6434fff/pc-amazon-frontier-model-safety-framework-2-7-final-2-9.pdf"
                                target="_blank">Amazon</a>,
                            <a href="https://x.ai/documents/2025.02.20-RMF-Draft.pdf" target="_blank">xAI</a>, and <a
                                href="https://images.nvidia.com/content/pdf/NVIDIA-Frontier-AI-Risk-Assessment.pdf"
                                target="_blank">NVIDIA</a>. Several other companies, including Inflection AI, Mistral
                            AI,
                            and Zhipu AI,
                            have not yet published their framework but <a
                                href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024"
                                target="_blank">have publicly committed</a> to doing so.
                        <p>While these
                            frameworks have different names in different companies and differ in some respects, their
                            <a href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024"
                                target="_blank">basic structure</a> is the same: companies define thresholds beyond
                            which they
                            consider risk stemming from their models to be unacceptable, and describe how they intend to
                            identify, assess, and mitigate risk to keep it below those thresholds.
                        </p>
                        <p>The goal of the Safety and Security Section of the Code is to specify a way in which the
                            following can be sufficient for demonstrating compliance with the AI Act's obligations
                            concerning GPAISR: (1) Have such a framework in place that meets certain basic criteria; and
                            (2) Actually stick to the framework.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What would GPAISR providers adhering to the Safety and Security Section of the
                                Code commit to?</strong></p>
                        <p>A: The Safety and Security Section of the Code consists of 4 main parts:</p>
                        <ol>
                            <li>The first part contains a commitment to having a "Safety and Security Framework". This
                                is
                                a type of framework that is popular in the industry already (see <a
                                    href="?section=safety-security#faq--what-is-a-safety-and-security-framework">FAQ
                                    here</a>). Companies adhering to the Safety and Security
                                Section commit to adopting and implementing a Safety and Security Framework
                                ("Framework") that meets certain basic criteria. For example, the Framework should
                                specify thresholds beyond which that company considers a given systemic risk stemming
                                from their GPAISR unacceptable.</li>
                            <li>The second part contains commitments related to risk assessment pursuant to the
                                company's Framework. Companies adhering to the Safety and Security Section of the Code
                                commit to identifying and analysing systemic risks along the entire lifecycle of a
                                GPAISR, and to determining whether any systemic risk stemming from the model exceeds the
                                company's pre-defined risk acceptance criteria.</li>
                            <li>The third part contains commitments related to risk mitigation pursuant to the company's
                                Framework. Companies adhering to the Safety and Security Section of the Code commit to
                                implementing technical safety and security mitigations, along the entire model
                                lifecycle, that are proportionate and state-of-the-art. These mitigations should reduce
                                systemic risks of the model to acceptable levels as defined in the company's Framework,
                                and further reduce systemic risks as appropriate.</li>
                            <li>The fourth part contains commitments related to organisational structures that support
                                adherence to the company's Framework. Companies adhering to the Safety and Security
                                Section of the Code commit to reporting to the AI Office about their implementation of
                                the Code, and especially the application of their Framework, by creating a "Safety and
                                Security Model Report" for each GPAISR. That Report should document the results of
                                systemic risk assessment and mitigation for the GPAISR, showing that the systemic risk
                                stemming from that model is acceptable. Additional commitments in the fourth part of the
                                Safety and Security Section of the Code address, for example, the reporting of serious
                                incidents, and, under certain specified conditions, independent external risk assessment
                                (see <a
                                    href="?section=safety-security#commitment-ii-11-independent-external-assessors-2">here
                                    for more detail</a>).</li>
                        </ol>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: What has changed between the second and the third draft of the Safety and Security
                                Section?</strong></p>
                        <p>A: For this third draft, we have refined the text based on feedback — much of which asked for
                            more specificity — and with a sharper focus on aligning the Safety and Security Section of
                            the Code with the AI Act. The third draft adopts more legal language, facilitating the
                            provision of more specific and detailed guidance.</p>

                        <p>While the draft itself contains a more detailed description of the changes made, in
                            explanatory boxes following the relevant measures, we also briefly summarise some of the
                            changes here:</p>

                        <ol>
                            <li><strong>Taxonomy:</strong> The Systemic Risk Taxonomy has been updated. The biggest
                                change is "large-scale illegal discrimination" being moved from the set of risks that
                                Signatories commit to always assessing and mitigating to the list of risks that informs
                                their systemic risk identification. This change was made because the legal basis of
                                keeping such discrimination in the selected category was not clear; this risk is not
                                specific to the "high-impact capabilities" of GPAISR, which would be required for it to
                                be a systemic risk according to the AI Act's (Article 3) definition. Further, we found
                                that this risk was better addressed through other parts of the AI Act or other laws,
                                including the chapters on AI systems in the AI Act, as well as the General Data
                                Protection Regulation (GDPR), Digital Services Act (DSA), and Digital Markets Act (DMA).
                            </li>

                            <li>Additionally, the sections on systemic risk identification and selection have been
                                drafted to accommodate additional risks that providers may choose to assess and mitigate
                                in the future. We strive for this process to be light-weight but we also strive to keep
                                the Code future-proof by avoiding locking in a set of systemic risks and/or systemic
                                risk assessment or mitigation techniques at this time of high uncertainty. We emphasize
                                the importance of potential risks to fundamental rights in the context of systemic risk
                                identification and selection.</li>

                            <li><strong>Security mitigations:</strong> The previous draft enumerated specific security
                                mitigations that Signatories were to implement so as to thwart non-state actors
                                attempting to steal, for example, their model weights (i.e., the <a
                                    href="https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                                    target="_blank">RAND SL3</a> security
                                goal). The new draft now lists these security mitigations as examples of mitigations
                                that may be implemented to reach the <a
                                    href="https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                                    target="_blank">RAND SL3</a> security goal, giving providers more
                                flexibility in how to achieve that goal and allowing for mitigations that are tailored
                                to providers' specific circumstances.</li>

                            <li><strong>Independent external assessment before market placement:</strong> The previous
                                draft contained a high-level interpretation of Recital 114 AI Act as requiring external
                                assessments for models where the provider does not have sufficient expertise to assess a
                                particular risk or where the model would pose systemic risk beyond those of GPAISR
                                already on the market. The current draft provides, we hope, much more clarity on when a
                                model poses systemic risks beyond those already on the market, and on how providers can
                                assess that. Further, it provides more clarity on what independent external assessment
                                entails, if it is to be implemented.</li>

                            <li>For reference, Recital 114 states that providers should conduct model evaluations "as
                                appropriate, through internal or independent external testing." See <a
                                    href="?section=safety-security#commitment-ii-11-independent-external-assessors-2">here
                                    for more detail</a>.</li>

                            <li><strong>Independent external assessment after market placement:</strong> We have made
                                this commitment significantly clearer than in the previous draft. In particular, some
                                read the previous draft as saying that providers adhering to the Safety and Security
                                Section of the Code would need to give unrestricted model access without much control
                                over to whom such access was given. We have now clarified that this is not the case.
                                Instead, providers adhering to the Safety and Security Section of the Code would be
                                required to: (1) achieve some independent external assessment after market placement of
                                the GPAISR; (2) define the policies by which such assessment would occur; and (3) adhere
                                to those policies.</li>

                            <li><strong>Streamlining reporting and documentation:</strong> The previous draft included
                                both an adequacy assessment and an adherence assessment that were to be carried out at
                                least every 6 months. In response to feedback, in the third draft these kinds of
                                assessments are to be carried out primarily in response to material changes relevant to
                                the systemic risk stemming from the GPAISR, rather than the expiry of a certain amount
                                of time. We welcome feedback on how these processes can be further streamlined and
                                improved. Specifically, providers adhering to the third draft of the Safety and Security
                                Section of the Code would commit to:</li>
                            <ol>
                                <li>Assessing the adequacy of their Framework after they have notified the AI Office
                                    that
                                    they will place a new GPAISR on the market. If that assessment finds that the model
                                    poses significant risk during the development phase, they would commit to updating
                                    the
                                    assessment in response to material changes, but not more often than every 12 weeks.
                                </li>

                                <li>Producing a Model Report ahead of market placement that, among other things,
                                    describes
                                    the provider's reasoning for considering the systemic risk stemming from the model
                                    to be
                                    acceptable.</li>

                                <li>After market placement, updating the Model Report in response to any material
                                    changes
                                    that may undermine the provider's initial reasoning for considering the systemic
                                    risk
                                    acceptable ahead of market placement.</li>

                                <li>Further, assessing the adequacy of their Framework every 12 months. This gives the
                                    organisation sufficient time to put in place those appropriate mitigations, which
                                    cannot
                                    be implemented during the course of the model development process.</li>
                            </ol>
                            <li><strong>Public transparency:</strong> The previous draft of the Code stated that GPAISR
                                providers adhering to the Safety and Security Section of the Code would commit to
                                publishing possibly redacted versions of their Frameworks and Model Reports (or
                                equivalents) as a means to assess and mitigate systemic risk. Though this is something
                                that most providers of GPAISRs today already do, to align more closely with the AI Act,
                                we have adjusted the commitment to say that they will share such information where that
                                is necessary to assess and mitigate systemic risks. GPAISR providers adhering to the
                                Safety and Security Section could choose not to do so, provided that doing so is not
                                necessary to assess and mitigate systemic risk.</li>

                            <li><strong>Aligning with risk management terminology and processes:</strong> We have
                                adjusted the draft to align more with common risk management terminology and processes.
                                In short, the risk management process follows the following steps:</li>
                            <ol>
                                <li>Define systemic risk acceptance criteria, which the provider will use to assess
                                    whether
                                    systemic risk is acceptable.</li>

                                <li>Identify the systemic risks, using the systemic risk taxonomy, that need assessment
                                    and
                                    mitigations.</li>

                                <li>Analyse systemic risk, for example by conducting model evaluations.</li>

                                <li>Compare analysed systemic risk to the systemic risk acceptance criteria. Where risk
                                    is
                                    deemed acceptable according to the provider's own Framework, the provider may
                                    continue.
                                    Where it is not deemed acceptable, the provider may implement additional safeguards,
                                    re-assess the risk, and proceed if risk has been sufficiently mitigated. In the
                                    previous
                                    draft of the Code, we called this step "risk evaluation," but that caused confusion
                                    due
                                    to it sounding similar to "model evaluation."</li>
                            </ol>
                            <li><strong>Removing KPIs:</strong> We found that the KPIs in the previous draft added more
                                confusion than clarity. As such, we've instead aimed to make the commitments and
                                measures themselves clear enough by themselves.</li>
                        </ol>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Does the Safety and Security Section of the Code "go beyond the AI Act"?</strong>
                        </p>
                        <p>A: No. The purpose of the Safety and Security Section of the Code is to specify one way in
                            which GPAISR providers can meet their obligations under the AI Act. As such, it supplements
                            the AI Act in order to offer clarity on how to comply with the Act. In that sense, the
                            Safety and Security Section of the Code has to go beyond the Act. However, we have sought to
                            avoid the Safety and Security Section of the Code going beyond the Act in the sense of
                            introducing requirements not grounded in the obligations established by the Act.
                        </p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Why does the Safety and Security Section of the Code contain a commitment to
                                external testing of
                                models before market placement under certain limited conditions?</strong></p>
                        <p>A: Recital 114 AI Act states that GPAISR providers should conduct model evaluations before
                            market
                            placement "as appropriate, through internal or independent external testing." To reduce
                            legal
                            uncertainty, we think the Safety and Security Section of the Code should specify under what
                            conditions such independent external involvement is appropriate.</p>
                        <p>Our draft therefore defines certain limited circumstances under which GPAISR providers
                            adhering to
                            the Safety and Security Section of the Code would commit to external assessments before
                            market
                            placement. This includes situations in which a new GPAISR may pose risk beyond those of
                            models
                            already on the EU market.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Will GPAISR providers have to comply with the Code of Practice to enter the EU
                                market?</strong></p>
                        <p>A: No. Providers can choose to comply with the AI Act another way. However, we expect it will
                            be
                            significantly easier to comply with the Act by adhering to the Code.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: In focusing on making AI Act compliance easier, does the Safety and Security
                                Section of the Code create loopholes?</strong></p>
                        <p>A: No. By clarifying how providers can adhere to the Act, the Safety and Security Section of
                            the Code aims to both reduce regulatory burdens while also protecting European citizens, by
                            outlining baseline expectations of how providers should assess and mitigate risk. If
                            loopholes are identified such that, for example, Europeans' rights to health and safety are
                            threatened, the Code can be amended. We detail methods by which this might happen in
                            Appendix 2 to the Code.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Why not rely on standards instead?</strong></p>
                        <p>A: The Safety and Security Section of the Code does rely on existing standards wherever
                            possible. However, there are currently no standards that are wholly appropriate given the
                            obligations imposed on providers of GPAISRs in the AI Act. The governance and regulatory
                            landscape for GPAISRs is still very nascent. That is one purpose of the Safety and Security
                            Section and the Code more broadly: to provide more regulatory certainty in the absence of
                            such standards and clear industry practices.</p>
                        <p>As the governance and regulatory landscape matures, we expect best practices in this space to
                            be codified in standards, which might in turn affect the substance of the Safety and
                            Security Section of the Code and how it is implemented.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Won't the Safety and Security Section of the Code become outdated?</strong></p>
                        <p>A: We have written the Safety and Security Section of the Code to be as future-proof as
                            possible. But the AI field is evolving and changing rapidly, which is a challenge for all AI
                            regulation, including the AI Act and the Code.</p>
                        <p>To make the Safety and Security Section of the Code more future-proof, Companies deciding to
                            adhere to our current draft would have a lot of discretion over <em>how</em> to adhere to
                            it. For
                            example, they themselves choose what mitigations to put in place to keep risk to an
                            acceptable level. Further, our draft refers to the state of the art and to supporting input
                            from the AI Office in several places, allowing
                            the implementation of the Safety and Security Section of the Code to adapt with the state of
                            the science. But despite designing the Safety and Security Section of the Code in this
                            future-proof way, we expect that it will still need to be periodically updated. We offer
                            more detailed suggestions on how the Code, and with it the Safety and Security Section, can
                            be updated over time in Appendix 2 to the Code.</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Does the Safety and Security Section of the Code increase compliance burdens on
                                GPAISR providers? Will it hinder innovation in the EU?</strong></p>
                        <p>A: No. The purpose of the Code is to make AI Act compliance as easy as possible for providers
                            of GPAISRs. Without it, companies would face considerable regulatory uncertainty about how
                            to comply with obligations such as "assess and mitigate possible systemic risks at Union
                            level," "perform model evaluation in accordance with standardised protocols and tools
                            reflecting the state of the art," and "ensure an adequate level of cybersecurity protection
                            for the general-purpose AI model with systemic risk".</p>
                    </div>

                    <div class="faq-item" aria-expanded="false">
                        <p><strong>Q: Isn't adherence to the Safety and Security Section of the Code infeasible for
                                startups and SMEs?</strong></p>
                        <p>A: No. For the vast majority of startups and SMEs, the AI Act's obligations related to GPAISR
                            don't apply, and so the Security and Safety Section of the Code is not relevant to them. <a
                                href="?section=safety-security#faq--to-whom-is-the-safety-and-security-section-of-the-code-relevant"
                                target="_blank">As
                                described above</a>, the drafters expect the Safety and Security Section of the Code to
                            only be
                            relevant to 5-15 companies at any given point in time. Often, these companies will be large
                            incumbents with well-resourced legal departments.</p>
                        <p>However, in some cases, "newcomer" companies might also be subject to the AI Act's GPAISR
                            obligations. The Safety and Security Section of the Code is designed to make compliance
                            easier for them: they can use shared methods for risk assessment and have lighter
                            documentation requirements.</p>
                    </div>
                </div>

                <div class="explanatory-box keep-open-initially">
                    <h4><i class="ph ph-info"></i>Explainer: About the Safety and Security Section of the Code<i
                            class="ph ph-caret-down box-caret"></i>
                    </h4>
                    <p><em>This explainer, written by the (Vice) Chairs of Working Groups 2-4, is intended to give
                            readers a high-level understanding of the Safety and Security Section of the Code for
                            general-purpose AI models (GPAI). If and to the extent that this explainer contradicts the
                            Code, the Code takes precedence.</em></p>

                    <p><strong>The Safety and Security Section of the Code of Practice describes one way in which
                            leading AI companies
                            can comply with the AI Act.</strong> The AI Act is a binding AI regulation passed by the
                        European
                        Union in 2024. The <a href="https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng#cpt_V"
                            target="_blank">part of
                            the AI Act</a> that is concerned with obligations for general-purpose
                        AI models (GPAI) will become effective on August 2, 2025. To give GPAI providers a blueprint
                        of one way to comply with these obligations, the European Commission appointed a <a
                            href="https://digital-strategy.ec.europa.eu/en/news/meet-chairs-leading-development-first-general-purpose-ai-code-practice"
                            target="_blank">group of
                            independent AI experts</a> with backgrounds in industry, academia, and policymaking to lead
                        the
                        drafting of a GPAI <a href="https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng#art_56"
                            target="_blank">Code of Practice</a> ("the Code"). The drafting of the Code is an iterative
                        process that benefits from repeated input from companies and other stakeholders and has now
                        seen the publication of the third and penultimate draft. This explainer is about the Safety
                        and Security Section of the Code, which specifies a way of complying with one subset of the
                        AI Act's GPAI obligations: the obligations for "GPAI with Systemic Risk" (GPAISR).</p>

                    <p><strong>To make AI Act compliance as easy as possible, the Safety and Security Section of the
                            Code
                            translates the Act's obligations for leading companies into a framework structure that many
                            of them are already using.</strong> This type of risk management framework is already used
                        by several
                        AI companies. At the time of writing, AI <a href="https://metr.org/faisc"
                            target="_blank">companies that have published</a> their version of such
                        a framework include <a
                            href="https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf"
                            target="_blank">Anthropic</a>, <a
                            href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf"
                            target="_blank">OpenAI</a>,
                        <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf"
                            target="_blank">Google DeepMind</a>, <a
                            href="https://ai.meta.com/static-resource/meta-frontier-ai-framework/"
                            target="_blank">Meta</a>, <a
                            href="https://cohere.com/security/the-cohere-secure-ai-frontier-model-framework-february-2025.pdf"
                            target="_blank">Cohere</a>, <a
                            href="https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Frontier-Governance-Framework.pdf"
                            target="_blank">Microsoft</a>,
                        <a href="https://assets.amazon.science/a7/7c/8bdade5c4eda9168f3dee6434fff/pc-amazon-frontier-model-safety-framework-2-7-final-2-9.pdf"
                            target="_blank">Amazon</a>,
                        <a href="https://x.ai/documents/2025.02.20-RMF-Draft.pdf" target="_blank">xAI</a>, and <a
                            href="https://images.nvidia.com/content/pdf/NVIDIA-Frontier-AI-Risk-Assessment.pdf"
                            target="_blank">NVIDIA</a>. Several other companies, including Inflection AI, Mistral AI,
                        and Zhipu AI,
                        have not yet published their framework but <a
                            href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024"
                            target="_blank">have publicly committed</a> to doing so. While these
                        frameworks have different names in different companies and differ in some respects, their
                        <a href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024"
                            target="_blank">basic structure</a> is the same: companies define thresholds beyond which
                        they consider risk
                        stemming from their models to be unacceptable, and describe how they intend to identify,
                        assess, and mitigate risk to keep it below those thresholds. The goal of the Safety and
                        Security Section of the Code is to specify a way in which the following can be sufficient
                        for demonstrating compliance with the AI Act's obligations concerning GPAISR: (1) have such
                        a framework in place that meets certain basic criteria; and (2) actually stick to the
                        framework.
                    </p>

                    <p><strong>The Safety and Security Section of the Code is only relevant to the very small number of
                            AI
                            companies that are subject to the AI Act's obligations concerning GPAI that presents
                            "systemic risk".</strong> The AI Act <a
                            href="https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng#art_3" target="_blank">defines</a>
                        systemic risk as being specific to capabilities
                        that
                        match or exceed the capabilities of the most advanced GPAI. The Act's obligations for GPAISR
                        should therefore only apply to a very small number of companies. The drafters of the Safety
                        and Security Section of the Code have no discretion over how the regulator will interpret
                        the GPAISR definition in practice, but have been writing the Safety and Security Section
                        assuming that only about 5-15 companies will be in scope of the AI Act's GPAISR obligations
                        at any given point in time. Further, the drafters expect models obtained by fine-tuning an
                        existing GPAISR to fall outside the scope of the AI Act's obligations for GPAISR; or, if
                        they are in scope, the drafters recommend that such obligations are not enforced against
                        such models produced by fine-tuning, until there are reasons to believe that such
                        fine-tuning could introduce novel and unacceptable risk. The drafters expect the European AI
                        Office to publish guidance on this question soon.</p>

                    <p><strong>The Safety and Security Section of the Code reduces uncertainty and streamlines risk
                            management requirements in order to enable innovation while ensuring public safety.</strong>
                        The
                        purpose of the Safety and Security Section is to make compliance with the AI Act's
                        obligations related to GPAISR easier. While the AI Act lists <a
                            href="https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng#rct_110" target="_blank">about 40
                            different examples</a> of
                        potential systemic risks, the drafters of the Safety and Security Section take the AI Act's
                        definition of "systemic risk" to only clearly apply to 4 kinds of risks: (1) cyber offence
                        risk; (2) chemical, biological, radiological and nuclear (CBRN) risk; (3) harmful
                        manipulation risk; and (4) loss of control risk. Companies adhering to the Safety and
                        Security Section of the Code would always assess and potentially mitigate these selected
                        systemic risks. Regarding other potential systemic risks that can be reasonably foreseen,
                        companies would at least consider such risks as part of their systemic risk selection
                        process.</p>

                    <p><strong>The 4 kinds of risks specified within the Safety and Security Section of the Code could
                            each
                            lead to catastrophic outcomes and threaten the health, safety, and fundamental rights of
                            many people.</strong> Scientific studies have found increasing evidence of highly capable
                        GPAI</p>
                    <ol>
                        <li>discovering <a href="https://aicyberchallenge.com" target="_blank">vulnerabilities</a> in <a
                                href="https://arxiv.org/abs/2409.00571" target="_blank">computer</a> <a
                                href="https://arxiv.org/abs/2406.11147" target="_blank">code</a>
                            (including <a
                                href="https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html"
                                target="_blank">previously unknown</a> ones);</li>
                        <li><a href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf"
                                target="_blank">lowering barriers</a> to biological and chemical weapons development for
                            both novices and
                            experts;</li>
                        <li>being capable of effective, targeted <a
                                href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf"
                                target="_blank">psychological</a> <a
                                href="https://cdn.openai.com/deep-research-system-card.pdf"
                                target="_blank">manipulation</a>; and</li>
                        <li>trying to <a href="https://arxiv.org/pdf/2412.04984" target="_blank">resist being shut
                                down</a> in certain circumstances and generating answers that
                            <a href="https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf"
                                target="_blank">falsely suggest alignment</a> with goals provided by its developers.
                        </li>
                    </ol>
                    <p>In addition to <a
                            href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf"
                            target="_blank">leading independent experts</a>, senior company leaders have also
                        repeatedly
                        <a href="https://time.com/6283386/ai-risk-openai-deepmind-letter/" target="_blank">warned of
                            these kinds of risks</a>. Recently, one AI company <a
                            href="https://cdn.openai.com/o1-system-card-20241205.pdf" target="_blank">increased its
                            assessments</a>
                        of CBRN
                        risk from its previously best model from 'low' to 'medium', and stated in an <a
                            href="https://cdn.openai.com/deep-research-system-card.pdf" target="_blank">even more
                            recent release</a> that "our models are on the cusp [...] of cross[ing] our high risk
                        threshold".
                    </p>

                    <p><strong>The Safety and Security Section of the Code is newcomer-friendly and designed to boost
                            adoption of highly capable GPAISR across the economy.</strong> As described above, the
                        drafters
                        expect the Safety and Security Section to only be relevant to 5-15 companies at any given
                        point in time. Often, these companies are large, well-resourced incumbents. But in some
                        cases, "newcomer" companies might also be subject to the AI Act's GPAISR obligations. The
                        Safety and Security Section of the Code is designed to make compliance easier for them: they
                        can use shared methods for risk assessment and have, in many cases, lighter documentation
                        requirements. Further, the Safety and Security Section aims to boost adoption of highly
                        capable GPAISRs across the economy. Many companies in various sectors want to use such
                        GPAISRs in their products and services – but need those GPAISRs to be reliable and safe in
                        order to do so. The Safety and Security Section, by outlining risk management practices for
                        providers of GPAISR, can thus help create the necessary trust for highly capable GPAI to be
                        applied across the economy and help boost innovation.</p>

                    <p><strong>Given the rapid pace at which GPAI capabilities are advancing, the Safety and Security
                            Section of the Code has been written to be as future proof as possible.</strong> Companies
                        deciding to adhere to our current draft would have a lot of discretion over <em>how</em> to
                        adhere to it.
                        For
                        example, they themselves choose what mitigations to put in place to keep risk to an
                        acceptable level. Further, our draft refers to the state of the art and to supporting input from
                        the AI Office in several places, allowing
                        the implementation of the Safety and Security Section of the Code to adapt with the state of
                        the science.</p>

                    <p><strong>Summary of the Safety and Security Section</strong></p>

                    <p><strong>The Safety and Security Section of the Code consists of 4 main parts, with the first part
                            containing a commitment to have a "Safety and Security Framework".</strong> This is the type
                        of
                        <a href="?section=safety-security#commitment-ii-1-safety-and-security-framework-2">framework</a>
                        described above that is popular in the industry already. Companies adhering to the
                        Safety and Security Section commit to adopting and implementing a Safety and Security
                        Framework that meets certain basic criteria. For example, the Framework should specify
                        thresholds beyond which that company considers a given systemic risk stemming from their
                        GPAISR unacceptable.
                    </p>

                    <p><strong>The second part of the Safety and Security Section contains commitments related to risk
                            assessment pursuant to the company's Framework.</strong> Companies adhering to the Safety
                        and
                        Security Section commit to identifying and analysing systemic risks along the entire
                        lifecycle of a GPAISR, and to determining whether any systemic risk stemming from the model
                        exceeds the company's pre-defined risk acceptance criteria.</p>

                    <p><strong>The third part of the Safety and Security Section contains commitments related to risk
                            mitigation pursuant to the company's Framework.</strong> Companies adhering to the Safety
                        and
                        Security Section commit to implementing technical safety and security mitigations, along the
                        entire model lifecycle, that are proportionate and state-of-the-art. These mitigations
                        should reduce systemic risks of the model to acceptable levels as defined in the company's
                        framework, and further reduce systemic risks as appropriate.</p>

                    <p><strong>The fourth part of the Safety and Security Section contains commitments related to
                            organizational structures that support adherence to the company's Framework.</strong>
                        Companies
                        adhering to the Safety and Security Section commit to reporting to the European AI Office
                        about their implementation of the Code, and especially the application of their Framework,
                        by creating a "Safety and Security Model Report" for each GPAISR. The Report should document
                        the results of systemic risk assessment and mitigation for the GPAISR, showing that the
                        systemic risk stemming from that model is acceptable. Additional commitments in the fourth
                        part of the Safety and Security Section address, for example, the reporting of serious
                        incidents, and, under certain specified conditions, independent external risk assessment
                        (<a
                            href="?section=safety-security#faq--why-does-the-safety-and-security-section-of-the-code-contain-a-commitment-to-external-testing-of-models-before-market-placement-under-certain-limited-conditions">see
                            here for more details on external testing</a>).</p>
                </div>

                <h2>Introductory note by the Chairs and Vice-Chairs of the Safety and Security Section</h2>

                <p><strong>We have put together an Explainer document and an FAQ about the Safety and the Security
                        Section,
                        which readers can access on this webpage (see above).</strong> We encourage all readers –
                    whether they have
                    engaged with
                    previous drafts or not – to read those documents as they should provide a good overview of what this
                    Section is about.</p>

                <p><strong>The Safety and Security Section of the Code of Practice describes one way in which leading AI
                        companies can comply with the AI Act.</strong> The AI Act is a binding AI regulation passed by
                    the European
                    Union in 2024. The part of the AI Act that is concerned with obligations for general-purpose AI
                    models (GPAI) will become effective on August 2, 2025. To give GPAI providers a blueprint of one way
                    to comply with these obligations, the European Commission appointed us as part of <a
                        href="https://digital-strategy.ec.europa.eu/en/news/meet-chairs-leading-development-first-general-purpose-ai-code-practice"
                        target="_blank">a group of independent AI experts</a> who have backgrounds in industry,
                    academia, and policymaking to lead the
                    drafting of a GPAI Code of Practice. The drafting of the Code is an iterative process that benefits
                    from repeated input from companies and other stakeholders over multiple drafts. This Safety and
                    Security Section is part of the third and penultimate draft. The Section specifies a way of
                    complying with one subset of the AI Act's GPAI rules: the rules for "GPAI with Systemic Risk"
                    (GPAISR).</p>

                <p><strong>We would like to thank all stakeholders for their valuable input and collaborative
                        stance.</strong> We welcome
                    written feedback by the Code of Practice Plenary participants and observers by Sunday, 30 March
                    2025, via a
                    dedicated survey shared with them. We look forward to the next steps of the drafting process.</p>

                <div class="signatures-container">
                    <p class="signature-line"><strong>Working Group 2</strong><br> Matthias Samwald (Chair), Marta
                        Ziosi &
                        Alexander Zacherl (Vice-Chairs)</p>
                    <p class="signature-line"><strong>Working Group 3</strong><br> Yoshua Bengio (Chair), Daniel
                        Privitera
                        & Nitarshan Rajkumar (Vice-Chairs)</p>
                    <p class="signature-line"><strong>Working Group 4</strong><br> Marietje Schaake (Chair), Anka Reuel
                        &
                        Markus Anderljung (Vice-Chairs)</p>
                </div>

                <h2>Recitals for the Safety and Security Section</h2>
                <p><em>Whereas:</em></p>
                <ol type="a">
                    <li>The Signatories recognise that providers of GPAISRs should continuously assess and mitigate
                        systemic risks, taking appropriate measures along the entire model lifecycle, cooperating with
                        and taking into account relevant actors along the AI value chain (such as stakeholders likely to
                        be affected by the GPAISR), and ensuring their systemic risk management builds on
                        state-of-the-art measures and is future-proof by regularly updating their practices in light of
                        improving and emerging capabilities (see Recital 114 AI Act). Systemic risk assessment is a
                        multi-stage process and model evaluations, referring to a range of methods used in the
                        assessment of the systemic risks of GPAISRs, are integral throughout. Where systemic risk
                        mitigations are implemented, Signatories recognise the importance of continuously assessing
                        their robustness and adequacy to sufficiently mitigate the targeted systemic risk, following
                        state-of-the-art processes, such as human red-teaming by internal or external assessors;
                        automated red-teaming; and/or bug-bounty programs which incentivise the public to discover and
                        report vulnerabilities in safety mitigations.</li>
                    <li>The Signatories recognise that this Safety and Security Section of the Code only applies to
                        providers of GPAISRs and not AI systems but that the assessment and mitigation of systemic risks
                        should include, as reasonably foreseeable, the system architecture and other software components
                        into which the model may be integrated, as well as the computing resources available at
                        inference time, because of their importance to the model's capabilities and safeguards. In cases
                        where providers of GPAISRs also develop, provide, or deploy AI systems based on GPAISRs, they
                        recognise the importance of taking into account these AI systems for their assessment and
                        mitigation of systemic risks.</li>
                    <li>The Signatories recognise that the degree of scrutiny and detail in systemic risk assessment and
                        mitigation measures, and in the documentation and reporting of the Signatories' adherence to
                        provisions of this Code or compliance with their obligations under the AI Act, should be
                        proportionate to the systemic risk (Article 56(2)(d) AI Act) at the relevant points along the
                        entire model's lifecycle; e.g., the higher the level of the systemic risk, the greater the
                        uncertainty around the extent of the model's capabilities, propensities, and/or effect, and/or
                        the lack of corresponding expertise of the provider, the more thorough and comprehensive
                        systemic risk assessment and mitigation measures should be. Conversely, there may be less need
                        for more thorough and comprehensive measures when it can be objectively and reasonably assumed
                        that a GPAISR will exhibit the same capabilities, propensities, or effect as GPAISRs that have
                        already been safely made available on the market, without systemic risks materialising and where
                        appropriate mitigations have been implemented. To account for differences in available resources
                        between providers of different size and capacity, and recognising the principle of
                        proportionality, simplified ways of compliance for SMEs, including startups, should be possible
                        where appropriate (Article 56(5) AI Act).</li>
                    <li>The Signatories recognise that many systemic risk assessment methods come with significant
                        workload and costs, and that systemic risks can be influenced by the effects of interactions
                        between multiple AI models and AI systems. They recognise the advantages of "sharing the load",
                        e.g. by sharing model evaluations, best practices, or infrastructure, in particular where this
                        is necessary to effectively assess or mitigate the systemic risk of the Signatories' GPAISRs, or
                        – where appropriate – by working with independent external assessors, potentially facilitated by
                        industry organisations.</li>
                    <li>Wherever Signatories that are SMEs are expressly excluded from certain Commitments or Measures
                        in this Safety and Security Section, such Signatories recognise that they could still
                        voluntarily adhere to them.</li>
                    <li>The Signatories recognise that all Commitments or Measures need to be interpreted in light of
                        the Objectives of the Code, in particular with Objective IV. Additionally, any term appearing in
                        the Commitments or Measures of this Safety and Security Section of the Code that is defined in
                        the Safety and Security Glossary shall have the meaning set forth in that Glossary.</li>
                    <li>The Signatories recognise that the Systemic Risk Taxonomy should be interpreted, in instances of
                        doubt, in good faith in light of (1) the severity and probability of each risk as defined in
                        Article 3(2) AI Act and (2) the definition of systemic risk as defined in Article 3(65) AI Act.
                    </li>
                    <li>The Signatories recognise the important role of the Precautionary Principle (laid down in
                        <a class="ai-act-link" href="https://eur-lex.europa.eu/eli/treaty/tfeu_2016/art_191/oj/eng"
                            target="_blank">Article
                            191 TFEU</a>), especially for systemic risks where the lack or quality of scientific data
                        does not yet permit a complete assessment, and will take the extrapolation of current adoption
                        rates and research and development trajectories of GPAISRs into account for the identification
                        of systemic risks.
                    </li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Recitals for this Safety and Security Section<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>This Draft 3 of the Code contains:</p>
                    <ol>
                        <li>the above list of recitals for this Safety and Security Section; and</li>
                        <li>potential material for future recitals shown in <em>italics</em> and marked by a
                            <strong>bolded title</strong> within the text of some of the following Measures and the
                            Systemic Risk Taxonomy in Appendix 1.
                        </li>
                    </ol>
                    <p>To facilitate communication of their thinking and approach, the Chairs for Working Groups 2 to 4
                        have intentionally left the pieces of italicised text in their current locations for this Draft
                        3, rather than merging them into the above list. In the next iteration of the Code, the
                        intention is to move all recital material into the above list, revised as necessary.</p>
                </div>

                <h2>General Commitments by Providers of GPAISRs</h2>


                <h3>Commitment II.1: Safety and Security Framework</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 55(1) and 56(2)(d) AI Act</p>
                </div>

                <p>Signatories commit to adopting and implementing a Safety and Security Framework (hereafter, "the
                    Framework") that will: (1) apply to the Signatories' GPAISRs; and (2) detail the systemic risk
                    assessment, systemic risk mitigation, and governance risk mitigation measures and procedures that
                    Signatories intend to adopt to keep systemic risks stemming from their GPAISRs within acceptable
                    levels.</p>

                <p>The explanatory box below provides a high-level template which Signatories may follow when writing
                    their Framework.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Potential outline of a Safety and Security Framework <i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>In the outline below, we provide potential section headers and the Commitments and Measures that
                        each section would correspond to:</p>
                    <ol>
                        <li><strong>Systemic risk acceptance criteria, systemic risk tiers, and forecasting</strong><br>
                            Measures II.1.2–II.1.3</li>
                        <li><strong>Systemic risk assessment and decision to proceed</strong><br>
                            Commitments II.2–II.5</li>
                        <li><strong>Technical systemic risk mitigations</strong><br>
                            Commitments II.6–II.7</li>
                        <li><strong>Governance risk mitigations</strong><br>
                            Commitments II.8–II.16</li>
                    </ol>
                </div>

                <em>In order to satisfy Commitment II.1:</em>

                <h4>Measure II.1.1. Content of the Framework</h4>
                <p>Signatories shall document in the Framework the measures and procedures that are to be used, adopted,
                    and/or performed to comply with Commitments II.1–II.16.</p>

                <h4>Measure II.1.2. Systemic risk acceptance criteria</h4>
                <p>In the Framework, Signatories shall describe and justify the criteria by which they will decide
                    whether the systemic risk stemming from their GPAISRs is acceptable ("systemic risk acceptance
                    criteria"). Such systemic risk acceptance criteria shall:</p>
                <ol>
                    <li>be defined for each of the systemic risks identified as part of systemic risk identification
                        (per Commitment II.3);</li>
                    <li>contain, for at least each of the selected types of systemic risks in Appendix 1.1, systemic
                        risk tiers that:
                        <ol type="a">
                            <li>are measurable;</li>
                            <li>are defined in terms of model capabilities, model propensities, harmful outcomes,
                                harmful scenarios, expected harm, quantitative estimates of risk or combinations thereof
                                (also, e.g., in combination with descriptions of specified mitigations); and</li>
                            <li>contain at least one systemic risk tier at which that type of systemic risk is
                                considered to be unacceptable ("unacceptable systemic risk tier"), particularly in the
                                absence of appropriate safety and security mitigations (as per Commitments II.6–II.7);
                            </li>
                        </ol>
                    </li>
                    <li>describe how Signatories determine whether the systemic risk stemming from a model is considered
                        to be unacceptable, independently of unacceptable systemic risk tiers per point (2)(c) above;
                        and</li>
                    <li>align with best practices that are endorsed by relevant, widely recognised international
                        institutions, bodies, or groups of experts, or AI Office guidance where available.</li>
                </ol>

                <p>Further, for all systemic risk tiers, within the Framework, Signatories shall provide a detailed
                    justification of how they were chosen, including by outlining how the systemic risk tiers are
                    related to a harmful outcome, e.g. by describing a systemic risk scenario.</p>

                <p>Also in the Framework, Signatories shall:</p>
                <ol>
                    <li>describe to the greatest level of detail possible given the current state of the science, the
                        technical systemic risk mitigations (as necessary under Commitments II.6–II.7) that are intended
                        to reduce the systemic risk associated with the relevant systemic risk tier (e.g., mitigations
                        may be implemented once a GPAISR reaches a systemic risk tier or to prevent a GPAISR from
                        reaching a systemic risk tier);</li>
                    <li>describe how the Signatory will assess whether the mitigations will reduce systemic risks to
                        acceptable levels, accounting for a sufficiently wide safety margin representative of empirical
                        uncertainty, including uncertainty regarding the current and future efficacy of systemic risk
                        mitigations;</li>
                    <li>describe the reasonably foreseeable limitations of the mitigations, such as conditions under
                        which the mitigations can be reasonably foreseen to fail;</li>
                    <li>where appropriate, state that mitigations for one or more systemic risks do not yet exist for a
                        given systemic risk tier;</li>
                    <li>identify and describe conditions and decision-making procedures under which further development,
                        the first or ongoing making available on the market, and/or use of a GPAISR will not proceed due
                        to insufficient mitigations for keeping systemic risk below an unacceptable level; and</li>
                    <li>detail the steps that will be taken for ceasing to further develop, make available on the market
                        (if not openly released), and/or use the GPAISR, and falling back on another model, where this
                        is necessary in light of model evaluations, post-market monitoring, or other systemic risk
                        assessment information.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Structure of systemic risk tiers<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Below are two partial examples of systemic risk tiers. These examples are meant to illustrate the
                        structure of two different kinds of systemic risk tiers. They neither imply endorsement of the
                        content within nor illustrate an appropriate level of detail. </p>
                    <p><strong>Example 1: Systemic risk tier should not be reached in the absence of
                            mitigations</strong></p>
                    <p> Systemic risk: Cyber offence</p>
                    <p> Unacceptable systemic risk tier: A GPAISR can completely automate cyberattacks at a scale
                        typical of well-resourced, state-level cyber organisations, at a cost competitive with or below
                        the cost of human cyber experts performing the same.</p>
                    <p> Required security mitigations before reaching the unacceptable risk tier: Security mitigations
                        that meet the <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                            target="_blank">RAND SL4</a> security goal. However, such mitigations do not yet exist and
                        are the
                        subject of active research.</p>
                    <p><strong> Example 2: Systemic risk tier should not be reached</strong></p>
                    <p> Systemic risk: Loss of control</p>
                    <p> Unacceptable systemic risk tier: A GPAISR sabotages the development and use of future models by
                        poisoning training data and inserting backdoors into inference code.</p>
                    <p> Mitigation to stay below the risk tier: Apply mitigations to prevent models from reaching the
                        unacceptable risk tier, such as monitoring, AI control protocols, fine-tuning, and security
                        measures.</p>
                </div>

                <h4>Measure II.1.3. Forecasting</h4>

                <p>In the Framework, for each systemic risk tier (identified pursuant to Measure II.1.2) which depends
                    on specific model capabilities, Signatories shall:</p>
                <ol>
                    <li>state, using best efforts and state-of-the-art methods, estimates of timelines for when they
                        reasonably foresee that they will have first developed a GPAISR that possesses such
                        capabilities, if such capabilities are not yet possessed by any of the Signatory's models
                        already available on the market, to facilitate the preparation of appropriate systemic risk
                        mitigations;</li>
                    <li>state estimates identified per point (1) above in quantitative terms where possible, such as
                        ranges or probability distributions over possibilities, which need not be precise dates; and
                    </li>
                    <li>state: (a) the assumptions underlying the estimated timelines; (b) the justifications for the
                        estimated timelines; and (c) any uncertainty about such estimates and the reasons therefore.
                    </li>
                </ol>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>To implement Measure II.1.3, Signatories may, where appropriate and available, make use of
                            aggregate forecasts, surveys, and other estimates, in cooperation or combination with other
                            Signatories as facilitated by industry groups. </p>
                    </div>
                </div>
                <h4>Measure II.1.4. Transparency into external input in decision-making</h4>

                <p>In the Framework, to provide transparency into decision-making concerning the assessment and
                    mitigation of systemic risks, Signatories shall identify and describe if and when further
                    development, the first or ongoing making available on the market, and/or the use of a GPAISR will be
                    informed by input from external actors, including relevant government actors.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Fulfilling Measure II.1.4 does not require input (or authorisation) from external actors but
                            does require transparency around its presence (and by default its absence), in accordance
                            with existing international commitments such as the <a
                                href="https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024"
                                target="_blank">Frontier AI Safety Commitments</a> (Outcome 3.VIII).</p>
                    </div>
                </div>

                <h4>Measure II.1.5. Improving and updating the Framework</h4>

                <p>Signatories shall improve over time the effectiveness of the Framework for assessing and mitigating
                    systemic risks stemming from their GPAISRs, including by:</p>
                <ol>
                    <li>building on insights gained from applying the Framework to their GPAISRs;</li>
                    <li>ensuring that estimates made pursuant to Measure II.1.3 are refined and validated over time in
                        updated versions of the Framework, such as by comparing them to historical trends or other
                        similar estimates; and</li>
                    <li>ensuring that the Framework adopts and takes into account the state of the art, including
                        incorporating the best known measures and procedures for assessing and mitigating systemic
                        risks, taking into account other Frameworks or similar systemic risk assessment and mitigation
                        policies that are publicly available.</li>
                </ol>

                <p>In the Framework, Signatories shall describe the process by which they will: (1) update the
                    Framework; and (2) determine that an updated version of a Framework is complete. When the Framework
                    is updated, Signatories shall describe and explain how and why the Framework has been updated, along
                    with a version number and date of change in a changelog.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Frameworks that follow the state of the art do not necessarily have to be the most detailed,
                            most thorough, or most conservative, provided that Frameworks respond proportionately to the
                            systemic risks in issue (in accordance with Recital (c) to this Safety and Security Section
                            of the Code).</p>
                    </div>
                </div>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>In Measure II.1.2, we have made systemic risk tiers mandatory only for selected systemic risks
                        (Appendix 1.1) but optional for other systemic risks where they may not be appropriate, as
                        indicated in stakeholder feedback. We have also removed the requirement for systemic risk tiers
                        to be defined on a "fixed and comparable scale across Signatories" because it contradicted the
                        flexibility afforded to Signatories in the Measure to define systemic risk tiers in different
                        ways. </p>
                    <p>We also narrowed the commitment from forecasting systemic risk tiers to forecasting
                        dangerous capabilities related to systemic risk tiers, with flexibility for this to be done in
                        an aggregated manner across Signatories using industry groups. This responds to stakeholder
                        feedback that outcomes (e.g., widespread economic damage) are harder to predict than
                        capabilities.</p>
                </div>

                <img class="illustration" src="illustration1-light.png" data-light-src="illustration1-light.png"
                    data-dark-src="illustration1-dark.png"
                    alt="Flowchart: How should Signatories assess and mitigate the systemic risk of their GPAISR? The diagram shows a four-step cycle: Identify risk (Commitment II.3), Analyse risk (Commitment II.4), Evaluate risk (Commitment II.5), leading to either RISK IS ACCEPTABLE (proceed with development/market) or RISK IS NOT ACCEPTABLE (return to Mitigate risk with proportionate mitigations under Commitments II.6 and II.7).">

                <h2>Risk Assessment for Providers of GPAISRs</h2>

                <h3>Commitment II.2. Systemic risk assessment and mitigation along the entire model lifecycle, including
                    during model development</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1), Recital 110 and Recital 114 AI Act</p>
                </div>

                <p>Signatories commit to conducting systemic risk assessment systematically at appropriate points along
                    the entire model lifecycle, in particular before making the model available on the market.
                    Specifically, Signatories commit to starting to assess and mitigate systemic risks during the
                    development of a GPAISR, as specified in the Measures for this Commitment.</p>

                <p><em>In order to satisfy Commitment II.2:</em></p>
                <h4>Measure II.2.1. Planning development</h4>

                <p>When planning the development, including planning the training, of a general-purpose AI model
                    classified as a GPAISR or of a model that the Signatory knows or reasonably foresees will meet the
                    classification condition for a GPAISR (pursuant to Article 51(1)(a) AI Act) and at the latest 4
                    weeks after notifying the AI Office (pursuant to Article 52(1) AI Act), Signatories shall:</p>
                <ol>
                    <li>have in place an appropriate Framework (in accordance with Commitment II.1) and start
                        implementing it for the planned model; and</li>
                    <li>start to assess and, as necessary, mitigate systemic risks according to the Code.</li>
                </ol>
                <h4>Measure II.2.2. During development</h4>

                <p>During development, including pre-training, training, and post-training (such as fine-tuning,
                    reinforcement learning, or similar methods, when performed by the Signatories themselves or on their
                    behalf), Signatories shall:</p>
                <ol>
                    <li>assess and, as necessary, mitigate systemic risks at appropriate milestones that are defined and
                        documented before training starts, where systemic risks stemming from the model in training
                        could materially increase, such as:
                        <ol type="a">
                            <li>training compute based milestones (e.g. every two- to four-fold increase in effective
                                compute);</li>
                            <li>development process based milestones (e.g.: during or after phases of fine-tuning or
                                reinforcement-learning; before granting more individuals access to the model; or before
                                granting the model more affordances such as network, internet, or hardware access); or
                            </li>
                            <li>metrics based milestones (e.g. at pre-determined levels of training loss or evaluation
                                performance);</li>
                        </ol>
                    </li>
                    <li>implement appropriate procedures to identify substantial changes in systemic risks which warrant
                        pausing development to conduct further systemic risk assessment, such as automated benchmarks
                        enabling a highly scalable and real-time identification of capability increases thereby lowering
                        the risk of human or organisational bottlenecks;</li>
                    <li>assess, at each milestone defined pursuant to point (1) above, whether it is reasonably
                        foreseeable that by the next milestone the model will reach systemic risk tiers that do not yet
                        have adequate mitigations in place, and accordingly ensure that such mitigations will be in
                        place prior to the model reaching these systemic risk tiers; and</li>
                    <li>document all systemic risk assessment and mitigation measures, procedures, and decisions taken
                        or adopted before the model crosses the next milestones, if earlier systemic risk assessment
                        results suggest the model is approaching as yet unreached systemic risk tiers and might cross
                        them before the next milestone.</li>
                </ol>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p> The Signatories recognise that where the Code refers to actions taken on behalf of
                            Signatories the phrase refers to actions taken by: (1) a formally appointed agent,
                            representative or employee (acting within the scope of their employment); (2) someone
                            otherwise contracted or engaged to take the actions in question by the Signatory; or (3)
                            someone otherwise acting under the Signatory's instructions for the Signatory's benefit,
                            whether for financial recompense or not. The 'on behalf of' phrase is not intended to refer
                            to independent downstream providers.
                        </p>
                    </div>
                </div>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Based on stakeholder feedback, we significantly removed overlap with other Commitments, which is
                        why this Commitment is now much shorter. "Evaluator readiness" is described more under Measure
                        II.4.11, triggers for assessing possible changes in the systemic risk landscape "during
                        deployment" under Measure II.8.7, and security measures "during retirement" under Measure
                        II.7.6. This means we removed the requirement for monitoring a GPAISR after retirement, which
                        seemed hard to implement especially for open-weights models and not the best use of resources.
                        We are also no longer requiring re-evaluation on a fixed 6-monthly cycle in favour of more
                        flexible trigger criteria, to ensure the Code does not create unnecessary work. We clarified the
                        trigger for Framework readiness in Measure II.2.1 to take into account GPAISR notification. The
                        biggest addition is introducing alternative ways of defining systemic risk assessment milestones
                        during training in Measure II.2.2, in order to take into account ongoing technical developments
                        and keep the Code future-proof.</p>
                </div>

                <h3>Commitment II.3. Systemic risk identification</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1), Article 56(2)(c) and (d) and Recital 110 AI Act</p>
                </div>

                <p>Signatories commit to selecting and further characterising systemic risks stemming from their GPAISRs
                    that are significant enough to warrant further assessment and mitigation, as specified in the
                    Measures for this Commitment.</p>

                <p><em>In order to satisfy Commitment II.3:</em></p>

                <h4>Measure II.3.1 Systemic risk selection</h4>
                <p>Signatories shall select for further assessment and, where necessary, mitigation the selected types
                    of systemic risks in Appendix 1.1.</p>

                <p>Where it can be reasonably foreseen that the GPAISR will pose other systemic risks, considering, as
                    appropriate:</p>
                <ol>
                    <li>Appendices 1.2 and 1.3; and</li>
                    <li>other relevant considerations, such as Appendix 1.4 and information on risks exhibited by
                        similar models available on the market, state-of-the-art research on the risks in question, and
                        knowledge and views of experts and stakeholder groups likely to be affected,</li>
                </ol>
                <p>Signatories shall additionally select those other systemic risks, provided that they are specific to
                    the high-impact capabilities of the GPAISR, for further assessment and, where necessary, mitigation.
                </p>

                <h4>Measure II.3.2 Determining systemic risk scenarios</h4>
                <p>For each selected systemic risk per Measure II.3.1, Signatories shall develop systemic risk scenarios
                    using appropriate methods, such as risk modelling or threat modelling (using model evaluations as
                    understood under Commitment II.4 where appropriate), with the aim of better understanding and
                    characterising the type, nature, and sources of the systemic risk selected, in preparation for
                    systemic risk analysis.</p>

                <p>Signatories shall develop at least one systemic risk scenario for each systemic risk selected. When
                    determining systemic risk scenarios, Signatories shall take into account reasonably foreseeable uses
                    and misuses (whether negligent, reckless, or wilful) of the GPAISR.</p>

                <p>Each systemic risk scenario shall include:</p>
                <ol>
                    <li>the systemic risk pathways by which the development, the making available on the market, and/or
                        use of the GPAISR could produce the selected systemic risk ("pathways to harm"); and</li>
                    <li>the sources of the selected systemic risk based on, at least, the potential sources of systemic
                        risk in Appendix 1.4.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Stakeholders asked for further clarification on the scope and process of systemic risk
                        identification. In Measure II.3.1, we have specified the link between systemic risk
                        identification and the systemic risk taxonomy (Appendix 1), the mechanisms for identifying other
                        systemic risks, and the relevant considerations for deciding on their selection. We cautiously
                        balanced providing Signatories with guidance and support for identifying systemic risk while
                        ensuring the required process remains practical and feasible. Moreover, we have added a new
                        Measure II.3.2, which introduces the determination of systemic risk scenarios to better
                        understand the selected systemic risks and to improve the flow between systemic risk
                        identification and systemic risk analysis. In Measure II.3.2 we have specified the required
                        steps and scope of determining systemic risk scenarios and proposed specific methodologies––such
                        as risk modelling or threat modelling––which can be useful but are not mandatory for Signatories
                        to consider.</p>
                </div>

                <h3>Commitment II.4. Systemic risk analysis</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1), Article 56(2)(d), Recital 110 and Recital 114 AI Act</p>
                </div>

                <p>As part of systemic risk assessment, Signatories commit to carrying out a rigorous analysis of the
                    systemic risks identified pursuant to Commitment II.3 in order to understand the severity and
                    probability of the systemic risks. Signatories commit to carrying out systemic risk analysis with
                    varying degrees of depth and intensity, as appropriate to the systemic risk stemming from the
                    relevant GPAISR and as specified in the Measures for this Commitment. Whenever systemic risk
                    mitigations are implemented, Signatories commit to considering their effectiveness and robustness as
                    part of systemic risk analysis.</p>

                <p>As further specified in the Measures for this Commitment, Signatories commit to making use of a range
                    of information and methods in their systemic risk analysis including model-independent information
                    and state-of-the-art model evaluations, taking into account model affordances, safe originator
                    models, and the context in which the model may be made available on the market and/or used and its
                    effects.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>The extent of systemic risk analysis conducted by Signatories will be proportionate to the
                            systemic risk being assessed and to the uncertainty about how much systemic risk a given
                            GPAISR presents. Signatories may commence systemic risk analysis with less
                            resource-intensive methods (e.g. literature reviews or running open benchmarks) and
                            follow-up with more thorough model evaluations (e.g. agentic model evaluations or human
                            uplift studies) to prioritise systemic risk analysis in proportion to the systemic risk
                            presented.</p>
                    </div>
                </div>

                <p><em>In order to satisfy Commitment II.4:</em></p>

                <h4>Measure II.4.1. Systemic risk estimation</h4>

                <p>As part of their systemic risk analysis, Signatories shall measure the level of systemic risk
                    stemming from their GPAISR for systemic risks identified pursuant to Commitment II.3, using
                    quantitative and/or qualitative estimates as appropriate for the type and nature of the systemic
                    risk analysed. In doing so, Signatories shall use systemic risk estimation techniques that:</p>
                <ol>
                    <li>are rigorous and state-of-the-art;</li>
                    <li>include those techniques referenced in the subsequent Measures of this Commitment; and</li>
                    <li>build on relevant information, including data about serious incidents (as per Commitment II.12)
                        and post-market monitoring (at least pursuant to Measure II.4.14).</li>
                </ol>
                <p>These estimates of systemic risk shall draw from the systemic risk scenarios developed in Measure
                    II.3.2 and shall be comparable against the pre-defined systemic risk acceptance criteria (as per
                    Measure II.1.2) against which the estimated systemic risk shall be evaluated (pursuant to Commitment
                    II.5).</p>

                <p>Where possible and appropriate given the systemic risk under assessment, Signatories shall state
                    these estimates in quantitative terms. At least for the selected types of systemic risks in Appendix
                    1.1, Signatories shall use best efforts to state these estimates in terms of systemic risk
                    indicators, constituting quantitative metrics to measure systemic risk and anticipate early-warning
                    signs by tracking progress towards systemic risk tiers as required by their Framework (pursuant to
                    Measure II.1.2). Signatories shall consider defining at least one systemic risk indicator for every
                    systemic risk tier.</p>

                <p>Where quantitative estimates are insufficient for assessing the type of systemic risk considered,
                    Signatories shall complement them with qualitative estimates. Where quantitative estimates by
                    themselves are inadequate for assessing the type of systemic risk considered, Signatories shall use
                    qualitative estimates, or a mix of qualitative and quantitative estimates, provided that these
                    estimates can be compared against the pre-defined systemic risk acceptance criteria (as per Measure
                    II.1.2) against which the estimated systemic risk shall be evaluated (pursuant to Commitment II.5).
                </p>

                <p>Possible results from the systemic risk estimation are, e.g.: (1) a compilation of model evaluation
                    results and other information collected; (2) a qualitative description of the systemic risk pathways
                    the model might (or not) enable (e.g. "our model enables an expert to develop a novel threat vector
                    X"); and (3) a quantitative estimation of the likelihood and expected harm (e.g. "our model has an
                    X% likelihood of causing €Z in losses due to systemic risk X").</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Based on stakeholder feedback, we have clarified further what risk estimation entails, as well as
                        its relation to risk identification and risk evaluation (now "Systemic Risk Acceptance
                        Determination"). In order to do that, we have collapsed old Measures on "Methodologies" and
                        "Risk Estimation" into one to avoid redundancy and enhance clarity, we have re-introduced the
                        concept of systemic risk indicators to facilitate the link between systemic risk estimation and
                        systemic risk acceptance determination, and provided concrete examples of what results of
                        systemic risk estimation might look like.</p>
                </div>

                <h4>Measure II.4.2. Safely derived models</h4>
                <p>To avoid duplicating work on systemic risk analysis, Signatories shall use best efforts to take into
                    account information from the existing systemic risk analysis of "safe originator models" when
                    analysing the systemic risks identified as per Commitment II.3 for "safely derived models" (e.g. by
                    using the results of model evaluations conducted for the safe originator model, instead of
                    conducting the same model evaluations again for the safely derived model), where:</p>
                <ol>
                    <li>safe originator models: (a) have gone through the systemic risk management process as per this
                        Code and fulfil the systemic risk acceptance criteria; (b) have been made available on the
                        market; (c) have not been shown to have material safety or security flaws (e.g. identified via
                        unmitigated serious incidents); and (d) are sufficiently transparent to the Signatory (meaning
                        the Signatory has sufficient visibility into the characteristics of the model such as its safety
                        mitigations, architecture, capabilities, modalities, and systemic risk profile, which is assumed
                        for fully open-source models and any models developed by the Signatory itself);</li>
                    <li>safely derived models are derived directly from the safe originator model (such as models
                        distilled, quantized, fine-tuned, or post-trained from such safe originator models or which only
                        added to, or improved, safety or security mitigations of the safe originator model); and</li>
                    <li>where it can reasonably be assumed that the safely derived model has the same systemic risk
                        profile as, or a lower systemic risk profile than, the safe originator model.</li>
                </ol>

                <p>Criteria for founding a reasonable assumption per point (3) above are:</p>
                <ol type="a">
                    <li>the safely derived model's scores on state-of-the-art benchmarks that measure general
                        capabilities
                        are all lower than or equal (within a negligible margin of error) to the scores of the safe
                        originator
                        model, provided that model evaluations are run by sufficiently qualified evaluators as per
                        Measure
                        II.4.11, point (1);</li>
                    <li>no changes to the safely derived model were made that were intended to, or that can be
                        reasonably
                        foreseen to, increase general capabilities, add or modify systemic risk-relevant model
                        capabilities or
                        affordances, change systemic risk-relevant model propensities, or remove or weaken safety and
                        security
                        mitigations; and</li>
                    <li>after performing the systemic risk identification per Commitment II.3 for the safely derived
                        model,
                        Signatories do not reasonably foresee any new or different systemic risk scenarios for the
                        safely
                        derived models compared to the safe originator model.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>This Measure was added to create a tightly-scoped means of taking into account relevant
                        information from existing models in the systemic risk assessment for a GPAISR. While we want to
                        make sure that systemic risk assessment work does not get needlessly duplicated across different
                        model versions, we are also keenly aware that an overly broad definition of "similar models"
                        here would potentially open a big loop-hole. We ask for your critical feedback.</p>
                </div>

                <h4>Measure II.4.3. Model-independent information</h4>
                <p>Signatories shall gather model-independent information relevant to their GPAISR and the systemic
                    risk
                    under assessment.</p>
                <p>To implement this Measure, Signatories shall: (1) carry out the search for and gathering of such
                    information with varying degrees of depth and intensity as appropriate to the level of systemic
                    risk; and (2) use the most appropriate methods for gathering model-independent information for
                    each
                    systemic risk, such as:</p>
                <ol type="a">
                    <li>web searches;</li>
                    <li>literature reviews;</li>
                    <li>market analyses (e.g., focused on capabilities of other GPAISRs available on the market);
                    </li>
                    <li>reviews of training data;</li>
                    <li>historical incident data;</li>
                    <li>forecasting of general trends (e.g. forecasts concerning the development of algorithmic
                        efficiency, compute use, data availability, and energy use);</li>
                    <li>expert interviews and/or panels; or</li>
                    <li>interviews, surveys, community consultations, or other participatory research methods
                        investigating, e.g., the effects of GPAISRs on natural persons located in the Union,
                        including
                        vulnerable groups.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>We added web searches and review of training data as commonly used methods here to give more
                        examples. We removed the KPI since it was vulnerable to specification gaming.</p>
                </div>

                <h4>Measure II.4.4. State-of-the-art model evaluations</h4>
                <p>Signatories shall ensure that state-of-the-art model evaluations are run to adequately assess
                    systemic risks and the capabilities, propensities and effects of their GPAISR, using appropriate
                    methodologies, such as Q&A sets, task-based evaluations, benchmarks, red-teaming and other methods
                    of adversarial testing, human uplift studies, model organisms, simulations, or proxy evaluations for
                    classified materials.</p>

                <p>Signatories shall perform model evaluations:</p>
                <ol type="1">
                    <li>at the most appropriate times for each systemic risk along the entire model lifecycle;</li>
                    <li>that provide information about at least one relevant systemic risk; and</li>
                    <li>which inform systemic risk indicators (as per Measure II.4.1) for relevant systemic risk
                        scenarios (as per Measure II.3.2).</li>
                </ol>

                <p>Wherever possible, Signatories shall use model evaluation methods with high efficiency without
                    compromising the rigour of their model evaluations, such that, where some level of a systemic risk
                    is conclusively ruled out by simple (but rigorous; as per Measure II.4.5) model evaluation methods
                    (e.g. if a simple evaluation method shows that a model's capabilities are, without reasonable doubt,
                    insufficient to manifest a given level of a systemic risk), additional or more sophisticated model
                    evaluation methods for the systemic risk in question are not necessary.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>We removed other duplications here, as well as the KPIs, which were vulnerable to specification
                        gaming. Note that the definition of "state-of-the-art" is now in the Glossary and takes into
                        account both industry consensus and guidance by the AIO. We especially expanded here on when
                        "simple" evaluation methods can be used to justify not running more involved evaluations, always
                        taking into account relevant quality criteria such as those listed in Measures II.4.5 to II.4.8
                        and Measure II.4.11. Again, the goal here is to prevent unnecessary work to allow focus on "what
                        really counts" but without increasing systemic risk.</p>
                </div>

                <h4>Measure II.4.5. Rigorous model evaluations</h4>
                <p>Signatories shall ensure that model evaluations with high scientific and technical rigour are
                    performed on their GPAISR. A lower level of rigour is permissible in model evaluations conducted for
                    systemic risk assessment where necessary to facilitate preliminary and exploratory research,
                    provided that the Signatory's Model Report complies with Measure II.8.3, point (2).</p>

                <p>To implement this Measure, Signatories shall: (1) adopt quality standards equivalent or superior to
                    those used in scientific peer review in machine learning, natural sciences, or social sciences as
                    appropriate to the type of model evaluation, as well as state-of-the-art practices in software
                    engineering, e.g. to prevent that model behaviour or performance during evaluation is significantly
                    affected by software bugs; and (2) ensure that model evaluation results reported to the AI Office
                    are, at least, at the level of rigour demanded by the quality standards for a submission accepted to
                    a major machine learning conference or scientific journal with impact factors in the top quartile of
                    its field. Where SME Signatories are not able to adopt the quality standards or practices outlined
                    in this paragraph, for example because they do not have the expertise or financial resources
                    in-house to identify or implement respective quality standards or practices, they shall notify the
                    AI Office accordingly and agree with the AI Office a suitable alternative means of fulfilling this
                    Measure.</p>
                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Achieving internal validity will differ for different types of systemic risks and methods,
                            but can be shown by, e.g.: large enough sample sizes; appropriate use of random seeds;
                            measuring statistical significance and statistical power; disclosure of environmental
                            parameters used; controlling for confounding variables and mitigating spurious correlation;
                            proving the absence of train-test contamination; preventing use of test data in training
                            (i.e. using train-test splits and respecting canary strings); re-running model evaluations
                            multiple times under different conditions and in different evaluation environments,
                            including varying individual parts of the model evaluation (e.g. the strength of prompts and
                            safeguards); detailed inspection of trajectories and other outputs; or disclosing (publicly
                            or to the AI Office) the processes for creating and managing new evaluations to ensure their
                            integrity.</p>
                    </div>
                </div>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Achieving external validity will differ for different types of systemic risks and methods,
                            but can be shown by, e.g.: adequate integration of domain experts in the evaluation process;
                            appropriate capability elicitation (see Measure II.4.6); documenting the technical
                            evaluation environment conditions in which the evaluation is run and the ways in which it
                            diverges from the real-world context (also see Measure II.4.7); diversity and realism of the
                            evaluation environment (also see Measure II.4.8); or making use of appropriately held-out
                            test sets (e.g. for the highest levels of systemic risks using sequestered test sets &
                            holdouts by independent third parties).</p>
                    </div>
                </div>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Reproducibility can be shown by, e.g.: successful peer reviews or reproductions by
                            independent third parties; securely releasing to the AI Office appropriate amounts of model
                            evaluation data (always taking into account proliferation risks); model evaluation code and
                            documentation of evaluation methodology, evaluation environment, computational environment,
                            and elicitation methods; or use of publicly available APIs, technical evaluation standards,
                            and tools.</p>
                    </div>
                </div>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>We removed the portability criterion here because it is already covered by the reproducibility
                        criterion together with the sharing provisions in Measure II.4.10. We also removed the KPIs
                        since they were vulnerable to specification gaming. Many small changes were made to the
                        definitions of the three criteria, which are flagged to become recitals to make the final Code
                        easier to read. We also significantly expanded on the need for adopting existing scientific and
                        engineering quality standards, which might not be flawless but are nonetheless a suitable
                        starting point for systemic risk assessment.</p>
                </div>

                <h4>Measure II.4.6. Model elicitation</h4>
                <p>Signatories shall ensure that all model evaluations of their GPAISR (whether internal or external)
                    are performed with a state-of-the-art level of model elicitation appropriate and proportionate to
                    the systemic risk assessed to: (1) elicit the upper limit of current and reasonably foreseeable
                    capabilities, propensities, and effects of the model under evaluation; (2) minimise the risk of
                    under-elicitation; (3) minimise the risk of model deception during model evaluation; and (4) match
                    the realistic model elicitation capabilities of potential misuse actors, where misuse actors play a
                    role in the relevant systemic risk scenario (e.g. some potential misuse actors might not be able to
                    fully elicit the model).</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Appropriate model elicitation methods can be different for different types of systemic risks
                            and types of AI models and can involve, e.g., the use of: fine-tuning and/or prompt
                            engineering (e.g. to elicit hazardous capabilities or propensities); model scratchpads;
                            model ensembles; scaffolding (including tool use); grey- or white-box access where
                            appropriate (also see Measure II.4.11); the use of models without safeguards; the use of
                            base models or helpful-only models; or use of appropriate training and inference compute
                            budgets (also see Measure II.4.11).</p>
                    </div>
                </div>

                <p>In addition, Signatories shall ensure that, where model elicitation methods increase a GPAISR's risk
                    profile during model evaluation, e.g. because the methods improve a model's hazardous capabilities
                    or increase its hazardous propensities, this is matched with appropriate, potentially increased,
                    security measures (in accordance with Commitment II.7) to prevent risks from unauthorised access to
                    the model and/or harmful actions by the elicited model.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the 2nd draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Here we made four major changes. (1) We underscored that proportionality to the systemic risk at
                        hand is important, especially when systemic risk scenarios contain misuse actors who will not be
                        able to fully elicit a GPAISR. (2) We removed some duplicate content; in particular compute
                        budgets and model access are covered in Measure II.4.11. (3) We added the need to match the
                        increased risk from some elicitation techniques with appropriate security measures. (4) We
                        removed the KPIs since the "engineering hours spent" approach seemed inappropriate for the size
                        and organisational structure of many GPAISR providers and would not be sufficient either.</p>
                </div>

                <h4>Measure II.4.7. Models as part of systems</h4>
                <p>As necessary for the assessment and mitigation of systemic risk, Signatories shall ensure that model
                    evaluations of their GPAISR will take into account reasonably foreseeable integrations of the model
                    into an AI system, as appropriate to the systemic risk assessed.</p>
                <p>Signatories planning to integrate a GPAISR into an AI system which they themselves will make
                    available on the market, put into service, and/or use shall ensure that the model evaluations take
                    into account the future context in which this AI system will be made available on the market, put
                    into service, and/or used, where this is relevant to the systemic risks and/or systemic risk sources
                    under assessment, and evaluate the model accordingly, e.g. by using the same kind of scaffolding or
                    tooling during model evaluations.</p>
                <p>Where Signatories (1) do not use or integrate the GPAISR into any AI systems themselves, but (2) make
                    their model available on the market, and (3) do not via other means have access to sufficient
                    information about the AI systems in which their model is integrated to perform rigorous model
                    evaluations as required by this Code, such that they (4) are not able to effectively assess and
                    mitigate the systemic risks of the GPAISR, Signatories shall use best efforts to cooperate with
                    licensees and downstream providers to obtain and act on the relevant information, as appropriate to
                    the relevant release strategy and/or business model of the Signatory and taking into account the
                    size and capacity of SMEs where necessary, and seeking assistance from the AI Office where needed.
                </p>
                <p>If there are material changes in the way or context in which the AI systems that integrate the GPAISR
                    are being made available on the market, put into service, and/or used from what was accounted for in
                    the original systemic risk assessment, Signatories shall, as necessary, take this into account and
                    perform a partial or complete re-assessment of the systemic risk stemming from the GPAISR, as
                    appropriate taking the relevant changes into account, and accordingly update their Model Report in
                    accordance with Measure II.8.7.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of Changes<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>The main change here is that we no longer make "information collection through licensing"
                        mandatory, in favour of giving Signatories flexible options for how to collect the necessary
                        information about systemic risks.</p>
                </div>

                <h4>Measure II.4.8. Context, modalities, and representative model evaluations</h4>
                <p>As necessary for the assessment and mitigation of the systemic risk in question and considering
                    relevant systemic risk scenarios, Signatories shall ensure that model evaluations match the expected
                    use context of their GPAISR, taking into account all modalities foreseeably used by the model. To do
                    so, Signatories shall analyse systemic risks stemming from their GPAISR so that, e.g.:</p>
                <ol>
                    <li>each modality is evaluated in a way appropriate and relevant to the systemic risk assessed, e.g.
                        by covering not only English but making best efforts to cover all major European languages and
                        other languages supported by the model in language-based evaluations of multilingual models,
                        whilst taking into account that some systemic risks are based on capabilities, such as
                        programming capabilities, that might be best evaluated in English; or</li>
                    <li>each systemic risk is analysed in the appropriate and relevant modality, e.g. evaluating visual
                        inputs/outputs, or actions in computer control or robotics.</li>
                </ol>
                <p>To facilitate the adequate consideration of the expected use context and to access relevant expertise
                    for and during model evaluations, Signatories shall, where possible and relevant to the systemic
                    risk assessed, cooperate with relevant actors along the AI value chain, such as appropriately
                    remunerated expert or lay representatives of civil society, academia, and/or other relevant
                    stakeholders. Where relevant stakeholders, e.g. those who are directly affected by the GPAISR, are
                    not available, Signatories shall identify and consult the most suitable representatives to represent
                    such stakeholders' interests as part of systemic risk assessment and mitigation. Where SMEs are
                    unable to cooperate with relevant stakeholders to implement this Measure, they shall request
                    assistance from the AI Office.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of Changes<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>This Measure now requires experts and lay representatives in these processes to be appropriately
                        remunerated. Additionally, we aimed to clarify the language.</p>
                </div>

                <h4>Measure II.4.9. Exploratory research and open-ended red-teaming</h4>
                <p>As the current state of the art in systemic risk assessment is under-developed, and when necessary to
                    effectively assess or mitigate systemic risk of their GPAISR, Signatories shall not restrict
                    themselves to assessing only systemic risks for known model capabilities, propensities, affordances,
                    and effects but also ensure the performance of exploratory work (internally and/or externally) to
                    improve the systemic risk assessment or mitigation of their GPAISRs.</p>

                <p>To implement this Measure, Signatories shall advance the state of the art in systemic risk assessment
                    and mitigation techniques by, as appropriate for the size of their organisation and the systemic
                    risk in question:</p>
                <ol>
                    <li>exploratory research on systemic risk assessment and mitigation, such as new and improved model
                        evaluation methods, meta-research on the "evaluation of model evaluations", or research in
                        support of forecasting; and</li>
                    <li>open-ended red-teaming engaging expert or lay representatives of civil society, academia, and
                        other relevant stakeholders as participants. Where relevant stakeholders, e.g. those who are
                        directly affected by the GPAISR, are not available, Signatories shall identify and engage the
                        most suitable representatives to represent such stakeholders' interests.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of Changes<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>We added examples of exploratory research, where depth was lacking in the second draft. In
                        addition, we removed the percentage-based KPI since it was vulnerable to specification gaming.
                    </p>
                </div>


                <h4>Measure II.4.10. Sharing tools & best practices</h4>
                <p>As systemic risks are influenced by the effects of interactions with other AI models or AI systems
                    such as possible chain reactions of incidents or malfunctioning, and when necessary to effectively
                    assess or mitigate systemic risk of Signatories' GPAISRs, non-SME Signatories shall share tools and
                    best practices (such as established methodologies or procedures) for state-of-the-art model
                    evaluation (including model elicitation) and systemic risk assessment and mitigation to further the
                    safety and security work of relevant actors in the AI ecosystem, in particular those engaged in risk
                    assessment or mitigation of AI models or AI systems that the Signatories' GPAISR can reasonably be
                    foreseen to interact with, such as other model providers (especially SMEs), downstream providers,
                    independent external assessors, or academic institutions.</p>

                <p>Where Signatories choose to additionally share model evaluation data (such as inputs and outputs)
                    used in systemic risk assessments, they shall limit this sharing as appropriate and, in particular,
                    must not share data:</p>
                <ol>
                    <li>that has high proliferation risks, e.g. where the shared test set could become a training set
                        for a misuse actor;</li>
                    <li>that could otherwise threaten scientific rigour, e.g. where shared test data could leak into
                        training sets or significant parts of currently active held-out test sets would be released;
                        and/or</li>
                    <li>in order to protect public security and commercially sensitive information, where these
                        interests outweigh the safety benefit of improved assessment or mitigation of systemic risks.
                    </li>
                </ol>
                <p>Non-SME Signatories shall: (1) ensure that sharing the tools and best practices referenced in this
                    Measure will neither reduce nor restrict a Signatory's own capacity to perform systemic risk
                    assessment
                    and mitigation for their GPAISR; and (2) consider assigning appropriate numbers of engineering and
                    support staff to research teams to facilitate such sharing, in order to avoid compromising the
                    effective
                    work of their systemic risk assessment personnel.</p>
                <p>If the sharing of tools and best practices referenced in this Measure is not (yet) facilitated by the
                    AI
                    Office or other initiatives endorsed by the AI Office (such as the International Network of AI
                    Safety
                    Institutes) in relevant guidance where available, non-SME Signatories shall, as necessary to assess
                    and
                    mitigate systemic risk of the Signatories' GPAISRs, facilitate such sharing in ways that engage as
                    many
                    relevant actors as possible (e.g. through source code releases) or, where more restricted sharing is
                    warranted, via secure sharing mechanisms which can be facilitated by industry organisations.</p>
                <p>Signatories that are SMEs shall satisfy this Measure by using best efforts to make use of tools and
                    best
                    practices shared by non-SME Signatories.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>We have highlighted that a model's systemic risk can be influenced by the interaction with other
                        AI models and AI systems, as the basis upon which Signatories shall share risk assessment tools
                        and best practices. Additionally, we tried to be very clear that we do not mandate sharing of
                        systemic risk assessment data, which is often disadvantageous and should only be done where it
                        has benefits for systemic risk assessment and mitigation. We removed the KPIs since they were
                        vulnerable to specification gaming and might have created perverse incentives.</p>
                </div>

                <h4>Measure II.4.11. Qualified model evaluation teams and adequate evaluation access and resources</h4>
                <p>Signatories shall ensure that all model evaluation teams (internal and/or external as per Measure
                    II.11.1) conducting systemic risk assessments are multidisciplinary teams that combine technical
                    expertise with relevant domain knowledge, ensuring a holistic understanding of the systemic risk
                    assessed. Model evaluation teams shall be: </p>
                <ol>
                    <li>qualified to conduct the relevant model evaluation work, with multiple team members who each
                        fulfil at least one of the following qualifications:
                        <ol type="a">
                            <li>research or engineering experience so they can, objectively and reasonably, be viewed as
                                having relevant expertise to contribute to the model evaluation for systemic risk
                                assessment, demonstrated e.g. by a relevant PhD, relevant and recognized, peer-reviewed
                                publications, or equivalent contributions to the field; </li>
                            <li>designed or developed a published peer-reviewed or widely used model evaluation on the
                                systemic risk assessed; or</li>
                            <li>at least three years of applied experience working in a field directly relevant to the
                                systemic risk assessed.</li>
                        </ol>
                    </li>
                    <li>given adequate access to the GPAISR, as necessary and appropriate to perform effective model
                        evaluations and systemic risk assessments and mitigations of the GPAISR as required by this
                        Code. Such access shall cover, as necessary, fine-tuning access to elicit model capabilities,
                        logit access, sufficiently high rate limits, access to the model's inputs and outputs (including
                        chain-of-thought reasoning), access to the model without safeguards, and additional affordances
                        to the model, such as the model's ability to call scripts or operate a browser. Furthermore,
                        such access shall cover, as necessary based on a case-by-case assessment, technical
                        implementations of grey- and white-box access, where: (a) grey-box access offers some level of
                        transparency, allowing for partial understanding of the "inner workings" of the model; and (b)
                        white-box access means that the model's full weights, activations, and other technical details
                        are always available for inspection, providing clear insights into how the model works. When
                        giving grey- or white-box model access to model evaluation teams (internal and/or external),
                        Signatories shall take into account the potential risks to model security that this access can
                        entail (in accordance with Commitment II.7). Signatories shall provide the lowest level of
                        access sufficient for rigorous model evaluations to be performed, including exploratory
                        research, and implement more stringent security mitigations when higher levels of access are
                        given;</li>
                    <li>provided, to the Signatories' best efforts, with access to model specifications, training data,
                        and past assessment results, as necessary and appropriate for the systemic risk assessed and the
                        model evaluation method used;</li>
                    <li>provided with: (a) adequate compute budgets, including to allow for sufficiently long model
                        evaluation runs, parallel execution, and re-runs; (b) appropriate staffing; and (c) sufficient
                        engineering budgets and support, including to inspect model evaluation results closely, e.g. to
                        identify software bugs or model refusals which might lead to artificially lowered capabilities
                        estimates; and</li>
                    <li>given enough time to competently design, debug, execute, and analyse model evaluations to an
                        objectively rigorous standard (see Measure II.4.5), whilst allowing for an appropriate level of
                        model elicitation (see Measure II.4.6). The given time shall be proportionate to: (a) the
                        magnitude of the systemic risk assessed; (b) the specific model evaluation method used,
                        including consideration of how new or established it is; and (c) the degree of stability and
                        performance similarity of the model under evaluation compared to the final model that will be
                        made available on the market, e.g., a model is considered stable if: (i) the benchmark
                        performance of successive model snapshots no longer significantly changes across general
                        capability benchmarks; and (ii) model core functionality no longer changes, such as tool calling
                        or (semi-)autonomous decision-making. An assessment time of at least 20 business days could,
                        e.g., indicate that model evaluation teams were given enough time for most systemic risks and
                        model evaluation methods.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>This Measure was extended a lot because it serves both as guidance for internal systemic risk
                        assessment as well as independent external risk assessment according to Measure II.11.1. In
                        particular, we added criteria for the work experience required for model evaluation teams,
                        aiming for this to fit all sizes of Signatories without negatively impacting already
                        high-performing research teams. We have clarified details about model access and tried to make
                        it even more explicit that increased access should only be given where this is strictly
                        necessary, while taking into account appropriate security measures. We have also elaborated what
                        "enough time" means, giving concrete guidance but again highlighting the proportionality
                        principle.</p>
                </div>

                <h4>Measure II.4.12. Safety margins</h4>
                <p>Signatories shall identify and implement a sufficiently wide safety margin that is: (1) appropriate
                    to the systemic risks stemming from the GPAISR; (2) representative of (a) potential
                    under-elicitation, (b) general uncertainty in systemic risk assessment and mitigation results, and
                    (c) historical (in)accuracy of similar assessments; (3) takes account of potential model
                    improvements after the model has been made available on the market; and (4) in accordance with
                    relevant state-of-the-art methods and practices, where available. </p>
                <p>To implement this Measure, Signatories shall:</p>
                <ol>
                    <li>use best efforts to ensure that their model evaluation methods are calibrated appropriately
                        (i.e. having a low uncertainty metric) to, where possible, use quantitative safety margins
                        (e.g., expressing safety margins in relative, percentage, or proportional terms); or</li>
                    <li>where this is not possible, use qualitative safety margins or adopt other suitable approaches to
                        safety margins, such as setting milestones conservatively or investing resources on elicitation
                        efforts as appropriate to take account of (a) a potential or actual adversary in misuse cases
                        and (b) reasonably foreseeable model improvements.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Here, we have mainly clarified that, while the creation of safety margins is crucial to systemic
                        risk assessment and mitigation, this is still a developing practice. Therefore, we have
                        emphasised that (1) this needs to take into account uncertainty in measurements and (2) can be
                        done qualitatively or via other approaches, where appropriate.</p>
                </div>


                <h4>Measure II.4.13. Evaluation practices for forecasting</h4>
                <p>Signatories shall use best efforts to adopt forecasting-specific techniques in those model
                    evaluations that are central to the highest systemic risk tiers, especially in support of Measure
                    II.1.3, using e.g.:</p>
                <ol>
                    <li>scaling law experiments for model size, training compute, and/or inference compute;</li>
                    <li>comparisons across model generations; or</li>
                    <li>other techniques that support estimates for when dangerous capabilities, propensities,
                        affordances, and effects might emerge in future models;</li>
                </ol>
                <p>provided that such techniques are in accordance with relevant state of the art where available.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>We ensured this Measure does not overlap with Measure II.1.3 and gives more options for
                        forecasting techniques beyond "scaling laws", to take into account the developing practice here.
                    </p>
                </div>

                <h4>Measure II.4.14. Post-market monitoring</h4>
                <p>Signatories shall conduct post-market monitoring to gather necessary information for assessing and
                    mitigating systemic risk, including information about model capabilities, propensities, affordances,
                    and effects, as appropriate for the systemic risk assessed. Signatories shall: (1) in particular
                    conduct post-market monitoring with a view to ensure that their GPAISR does not pose unacceptable
                    systemic risk as defined by their systemic risk acceptance criteria (as per Measure II.1.2); and (2)
                    use best efforts to conduct post-market monitoring for lower systemic risk tiers as necessary for
                    forecasting (see Measures II.1.3 and II.4.13).</p>

                <p>To fulfil this Measure, Signatories shall establish organisational, contractual, technical, or other
                    methods most suitable for the specific integration, release and distribution strategy, and use of
                    their GPAISR to gather relevant post-market information, whilst ensuring that the information is
                    treated in accordance with Union law.</p>

                <p>Signatories shall consider a variety of suitable methods for post-market monitoring, such as:</p>
                <ol>
                    <li>collection of end-user feedback;</li>
                    <li>(anonymous) reporting channels;</li>
                    <li>(serious) incident report forms;</li>
                    <li>bug bounties;</li>
                    <li>community-driven model evaluations and public leaderboards;</li>
                    <li>monitoring their GPAISR's use in the real world, e.g. identifying use in software repositories
                        and known malware, or monitoring public forums and social media for novel patterns of use;</li>
                    <li>supporting the scientific study of their GPAISR's emerging risks, capabilities, and effects in
                        the Union in collaboration with academia, civil society, regulators, and/or independent
                        researchers;</li>
                    <li>conducting frequent dialogues with affected stakeholders, using, e.g., participatory methods in
                        accordance with Measure II.4.8;</li>
                    <li>investing in the development of new technical methods supporting the privacy-preserving
                        monitoring and analysis of individual instances of their GPAISR or of AI systems, in particular
                        autonomous AI systems such as AI agents, built from their GPAISR, e.g. the development of
                        watermarks, fingerprinting, digital signatures, or decentralised, privacy-preserving monitoring
                        techniques; or</li>
                    <li>implementing privacy-preserving logging and metadata analysis methods, where technically,
                        legally, and commercially feasible, considering the release and distribution strategy of their
                        GPAISR.</li>
                </ol>

                <p>Signatories shall, in particular, collect relevant information about: (1) breaches of a GPAISR's use
                    restrictions and subsequent serious incidents arising from such breaches, to inform the
                    implementation of Measure II.6.3; and (2) for closed-source GPAISRs, aspects of such models that are
                    relevant for assessing and mitigating systemic risk but that are not transparent to third parties,
                    such as hidden chains-of-thought.</p>

                <p>Where Signatories themselves provide and/or deploy AI systems that incorporate their own GPAISR,
                    Signatories shall, as necessary to effectively assess and mitigate systemic risks stemming from the
                    model in question, monitor the model as part of these systems.</p>

                <p>Further, and particularly where the methods listed in points (1) to (10) above do not provide
                    Signatories with the necessary post-market information to effectively assess and mitigate systemic
                    risks stemming from their GPAISR, Signatories shall use best efforts to cooperate with licensees,
                    downstream providers, and/or end-users to receive such information from these actors and take
                    relevant information shared by them into account, provided that such information is treated in
                    accordance with Union law. Where they do so and end-users are consumers, Signatories shall offer the
                    choice to opt-in to sharing information relevant for post-market monitoring, provided that such
                    information is treated in accordance with Union law.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>Signatories could, e.g., enter into agreements with licensees to share relevant information,
                            taking into account sensitive information, including data, relevant to the systemic risk
                            assessment and/or mitigation by the Signatory, provided that such information is treated in
                            accordance with Union law. Signatories could request that the AI Office facilitate
                            cooperation and information sharing with licensees, downstream providers, and/or end-users.
                        </p>
                    </div>
                </div>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>The main change here was to further clarify that there is no general requirement to use specific
                        techniques, such as logging or licensing. Instead, it is for Signatories to pick and choose from
                        suitable methods, appropriate to their capabilities, business model, and the systemic risk at
                        hand, always taking into account Union laws, such as those protecting privacy. We also removed
                        the KPIs since they were inflexible and vulnerable to specification gaming.</p>
                </div>

                <h3>Commitment II.5. Systemic risk acceptance determination</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 55(1) and 56(2)(d) AI Act</p>
                </div>

                <p>Signatories commit to determining the acceptability of the systemic risks stemming from their GPAISRs
                    by comparing the results of their systemic risk analysis (pursuant to Commitment II.4) to their
                    pre-defined systemic risk acceptance criteria (pursuant to Measure II.1.2), in order to ensure
                    proportionality between the systemic risks of the GPAISR and their mitigations. Signatories commit
                    to using this comparison to inform the decision of whether or not to proceed with the development,
                    the making available on the market, and/or the use of their GPAISR, as specified in the Measures for
                    this Commitment.</p>

                <p><em>In order to satisfy Commitment II.5:</em></p>

                <h4>Measure II.5.1. Proceeding where systemic risk is deemed acceptable</h4>
                <p>Where systemic risk is deemed acceptable, Signatories shall determine whether the residual systemic
                    risk nonetheless warrants additional mitigation before proceeding with the development, first or
                    ongoing making available on the market, and/or use of their GPAISR.</p>

                <h4>Measure II.5.2. Not proceeding where systemic risk is deemed unacceptable</h4>
                <p>Where systemic risk is deemed unacceptable, Signatories shall take appropriate steps to bring such
                    risk to an acceptable level in accordance with their Framework.</p>

                <p>Where a model is not yet available on the market, Signatories shall, e.g.: (1) implement additional
                    mitigations until systemic risk is deemed acceptable; or (2) not make a model available on the
                    market. Where the model is already made available on the market, Signatories shall: (3) restrict the
                    making available, withdraw, or recall a model from the market; or (4) otherwise address the systemic
                    risks. As a minimum, Signatories shall follow the relevant steps outlined in their Framework
                    (pursuant to Measure II.1.2).</p>

                <p>Once appropriate steps to keep systemic risk at an acceptable level have been implemented,
                    Signatories, prior to proceeding with the development, the first or ongoing making available on the
                    market, and/or the use of their GPAISR, shall conduct another round of systemic risk analysis (in
                    accordance with Commitment II.4) and systemic risk acceptance determination (in accordance with
                    Commitment II.5).</p>

                <h4>Measure II.5.3. Staging model development and making available on the market when proceeding</h4>
                <p>When proceeding with the development, first or ongoing making available on the market, and/or use of
                    their GPAISR, Signatories shall use best efforts to proceed in stages, as necessary, to keep
                    systemic risks below an unacceptable level and appropriately mitigated below that level, adopting
                    practices such as: (1) limiting API access to vetted users; (2) gradually expanding access based on
                    post-market monitoring and ongoing systemic risk assessments; (3) starting with a closed release
                    before any open release; (4) using logging systems to track use and safety concerns; (5) setting
                    criteria for progressing through stages, including criteria based on systemic risk tiers and user
                    feedback; and (6) retaining the ability to restrict access, as appropriate.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>In order to avoid confusion with systemic risk evaluations, we have changed the name of this
                        commitment to "systemic risk acceptance determination". Additionally, we have collapsed this
                        measure and old Commitment 13 "development and deployment decisions" into one to better
                        streamline the process of comparing levels of risks to systemic risk acceptance criteria, with
                        development and deployment decisions taken based on that comparison.</p>
                </div>

                <h2>Technical risk mitigation for providers of GPAISRs</h2>

                <h3>Commitment II.6. Safety mitigations</h3>
                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1), Article 56(2)(d) and Recital 114 AI Act</p>
                </div>
                <p>Signatories commit, as specified in the Measures for this Commitment, to: (1) implementing technical
                    safety mitigations along the entire model lifecycle that are proportionate to the systemic risks
                    arising from the development, the making available on the market, and/or the use of GPAISRs, in
                    order to reduce the systemic risks of such models to acceptable levels, and further reduce systemic
                    risk as appropriate, in accordance with this Code; and (2) ensuring that safety mitigations are
                    proportionate and state-of-the-art.</p>

                <p><em>In order to satisfy Commitment II.6:</em></p>

                <h4>Measure II.6.1. Safety mitigations to implement</h4>
                <p>Without prejudice to Measure II.6.2, Signatories shall, as necessary to mitigate systemic risks and
                    reduce them to acceptable levels (in accordance with this Code), implement technical safety
                    mitigations for GPAISRs, such as: (1) filtering and cleaning training data; (2) monitoring and
                    filtering the inputs and outputs of such models; (3) changing the behaviour of such a model in the
                    interests of safety, such as fine-tuning the model to refuse certain requests; (4) restricting the
                    availability of such a model on the market, such as restricting model access to vetted users; (5)
                    offering countermeasures or other safety tools to other actors; (6) implementing high-assurance
                    quantitative safety guarantees concerning the behaviour of such a model; and (7) implementing
                    infrastructure that could help promote safe ecosystems of AI agents, such as a reputation system,
                    specialised communication protocols, or incident monitoring tools.</p>

                <h4>Measure II.6.2. Proportionate and state-of-the-art safety mitigations</h4>
                <p>Signatories shall implement state-of-the-art technical safety mitigations that: (1) are proportionate
                    to the systemic risk at issue, taking into account the systemic risk acceptance criteria (as set out
                    in the Framework pursuant to Measure II.1.2); and (2) best mitigate, in particular, unacceptable
                    systemic risks (as set out in the Framework pursuant to Measure II.1.2).</p>
                <p>For the purposes of this Code, state-of-the-art technical safety mitigations need not always or
                    necessarily mitigate all systemic risks to the greatest extent.</p>

                <h4>Measure II.6.3. Serious incident response readiness</h4>
                <p>Signatories shall implement corrective measures to address serious incidents through the use of
                    heightened or additional technical safety and/or security mitigations (pursuant to this Commitment
                    and/or Commitment II.7).</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>In response to feedback about the complexity of real world incidents, we have removed the
                        requirement to pre-define corrective measures in Measure II.6.3. We have also added additional,
                        optional safety mitigations for Signatories to consider in Measure II.6.1.</p>
                </div>

                <h3>Commitment II.7. Security mitigations</h3>
                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1), Article 56(2)(d), Recital 114 and Recital 115 AI Act</p>
                </div>
                <p>Signatories commit to mitigating systemic risks that could arise from unauthorised access to
                    unreleased model weights of their GPAISRs and/or unreleased associated assets. Associated assets
                    encompass any information critical to the training of the model, such as algorithmic insights,
                    training data, or training code.</p>

                <p>Consequently, Signatories commit to implementing state-of-the-art security mitigations designed to
                    thwart such unauthorised access by well-resourced and motivated non-state-level adversaries,
                    including insider threats from humans or AI systems, so as to meet at least the <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL3</a>
                    security
                    goal or equivalent, and achieve higher security goals (e.g. <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL4</a> or
                    SL5) as specified in the
                    Measures for this Commitment.</p>

                <p><em>In order to satisfy Commitment II.7:</em></p>

                <h4>Measure II.7.1. General cybersecurity best practices</h4>
                <p>Signatories shall implement general cybersecurity best practices so as to meet at least the <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL3</a>
                    security goal. Illustrative examples of such best practices are:</p>
                <ol>
                    <li>strong identity and access management practices;</li>
                    <li>strong protections against social engineering;</li>
                    <li>protection of wireless networks;</li>
                    <li>policies for untrusted removable media;</li>
                    <li>protection of premises from physical intrusion; or</li>
                    <li>regular software updates and patch management.</li>
                </ol>

                <p>General cybersecurity best practices implemented pursuant to this Measure shall accord with: (1)
                    up-to-date cybersecurity technical standards such as <a href="https://www.iso.org/standard/27001"
                        target="_blank">ISO/IEC 27001:2022</a>, <a
                        href="https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final" target="_blank">NIST 800-53</a>, <a
                        href="https://soc2.co.uk/" target="_blank">SOC 2</a> and, if
                    available, a harmonised standard on cybersecurity; and (2) other relevant Union law such as the <a
                        href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32022L2555" target="_blank">NIS2
                        Directive</a> and the <a
                        href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32024R2847"
                        target="_blank">Cyber Resilience Act</a> to the extent applicable.</p>

                <h4>Measure II.7.2. Security assurance</h4>
                <p>Signatories shall implement security measures to assure and test their security readiness against
                    potential and actual adversaries so as to meet at least the <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL3</a>
                    security goal. Illustrative
                    examples of such security measures are:</p>
                <ol>
                    <li>regular security reviews by an independent external party as necessary to mitigate systemic
                        risks;</li>
                    <li>frequent red-teaming as necessary to mitigate systemic risks;</li>
                    <li>secure communication channels for third parties to report security issues;</li>
                    <li>competitive bug bounty programs to encourage public participation in security testing as
                        necessary to mitigate systemic risks;</li>
                    <li>installation of Endpoint Detection and Response (EDR) and/or Intrusion Detection System (IDS)
                        tools on all company devices and network components; or</li>
                    <li>the use of a security team to monitor for EDR alerts and perform security incident handling,
                        response and recovery for security breaches in a timely manner.</li>
                </ol>
                <p>Security measures implemented pursuant to this Measure shall accord with up-to-date security
                    assurance technical standards, such as <a href="https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final"
                        target="_blank">NIST 800-53</a> (CA-2(1), CA-8(2), RA-5(11), IR-4(14)), and NIST
                    SP 800-115 5.2.</p>

                <h4>Measure II.7.3. Protection of unreleased model weights and associated assets</h4>
                <p>Signatories shall implement security measures to protect unreleased model weights and associated
                    assets so as to meet at least the <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL3</a>
                    security goal. Illustrative examples of such security
                    measures are:</p>
                <ol>
                    <li>a secure internal registry of all devices and locations where model weights are stored;</li>
                    <li>access control and monitoring of access on all devices storing model weights, with alerts on
                        copying to unmanaged devices;</li>
                    <li>storage on dedicated devices which host only data, code, and services treated with levels of
                        sensitivity and security equivalent to those applied to the devices storing model weights and
                        associated assets;</li>
                    <li>ensuring model weights are always encrypted in storage and transportation, in accordance with
                        up-to-date best practice security standards, including encryption with at least 256-bit security
                        and with encryption keys stored securely on a Trusted Platform Module (TPM) or in accordance
                        with higher security standards that are established as best practice after the publication of
                        the Code;</li>
                    <li>ensuring model weights are only decrypted for legitimate use to non-persistent memory;</li>
                    <li>implementing confidential computing if feasible in accordance with up-to-date industry
                        standards, using hardware-based, and attested trusted execution environments to prevent
                        unauthorised access to model weights while in use; or</li>
                    <li>restricting physical access to data centres and other sensitive working environments to required
                        personnel only, along with regular inspections of such sites for unauthorised personnel or
                        devices.</li>
                </ol>
                <p>Security measures implemented pursuant to this Measure shall accord with up-to-date technical
                    standards such as <a href="https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final" target="_blank">NIST
                        800-53</a>.</p>

                <h4>Measure II.7.4. Interfaces and access control to unreleased model weights</h4>
                <p>Signatories shall implement measures to harden interfaces and restrict access to unreleased model
                    weights while in use so as to meet at least the <a
                        href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND SL3</a>
                    security goal. Illustrative examples of
                    such interface and access control measures are:</p>
                <ol>
                    <li>explicitly authorising only required software and persons for both direct or indirect access to
                        model weights, enforced through multi-factor authentication mechanisms, and audited on a regular
                        basis of at least every six months;</li>
                    <li>thorough review by a security team of any software interface accessing model weights for
                        vulnerabilities or data leakage; or</li>
                    <li>hardening interfaces with access to the model weights to reduce the risk of data and weight
                        exfiltration, using methods such as output rate limiting.</li>
                </ol>
                <p>Interface and access control measures implemented pursuant to this Measure shall accord with
                    up-to-date technical standards such as NIST SP 800-171, INCITS 359-2004, and NIST 800-53.</p>

                <h4>Measure II.7.5. Insider threats</h4>
                <p>Signatories shall implement measures to screen for and protect against insider threats so as to meet
                    at least the <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                        target="_blank">RAND SL3</a> security goal, including threats in the form of self-exfiltration
                    or sabotage
                    carried out by GPAISRs. Illustrative examples of such measures are:</p>
                <ol>
                    <li>background checks on employees and contractors that have or might reasonably obtain access to
                        unreleased model weights of their models, associated assets, or systems that control the storage
                        or use of such unreleased model weights;</li>
                    <li>the provision of training on recognising and reporting insider threats; or</li>
                    <li>sandboxes around GPAISRs.</li>
                </ol>
                <p>Measures implemented to screen for and protect against insider threats pursuant to this Measure shall
                    accord with up-to-date technical standards such as <a
                        href="https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final" target="_blank">NIST 800-53</a>
                    (PM-12, PS-3).</p>

                <h4>Measure II.7.6. Regime of applicability</h4>
                <p>Signatories shall ensure that the security measures set out in Measures II.7.1–II.7.5: (1) apply
                    along the entire model lifecycle from before training until secure deletion or a decision to release
                    the model weights or associated assets; and (2) without prejudice to (1), prioritise the
                    implementation of security measures in accordance with the systemic risk stemming from the GPAISR.
                </p>

                <h4>Measure II.7.7. Limited disclosure</h4>
                <p>Signatories shall ensure that any publicly available copy of the Framework or the Model Report
                    discloses security mitigations considered and/or implemented pursuant to Measures II.7.1–II.7.5: (1)
                    with the greatest level of detail possible given the state of science; (2) without undermining their
                    effectiveness; and (3) with redactions as necessary to prevent substantially increased systemic risk
                    that could result from, e.g., undermining the effectiveness of technical safety mitigations (in
                    accordance with Commitment II.6) or divulging sensitive commercial information to a degree
                    disproportionate to the societal benefit of the publication.</p>

                <h4>Measure II.7.8. Higher security goals</h4>
                <p>Signatories shall advance research on and implement more stringent security mitigations, in line with
                    at least the <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html"
                        target="_blank">RAND SL4</a> security goal or equivalent, if they reasonably foresee security
                    threats from
                    RAND OC4-level adversaries.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>In response to feedback about the prescriptiveness of security measures, we have adopted an
                        outcome-based approach that focuses on the <a
                            href="https://www.rand.org/pubs/research_reports/RRA2849-1.html" target="_blank">RAND
                            SL3</a> security goals, rather than specific
                        security measures. Each of the Measures in the new Commitment II.7 now allows more flexibility
                        in how Signatories achieve the security goal, without prescribing specific tools or techniques.
                    </p>
                    <p>We have also added a reference to GPAISRs as an insider threat in Measure II.7.5, so as to
                        account for rogue internal deployment threat models.</p>
                    <p>Finally, in Measure II.7.6 we have granted flexibility to prioritise GPAISR that could pose
                        greater risks, such as later checkpoints rather than earlier checkpoints, in response to
                        feedback about the proportionate application of security measures.</p>
                </div>


                <h2>Governance risk mitigation for providers of GPAISRs</h2>
                <h3>Commitment II.8. Safety and Security Model Reports</h3>
                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 55(1), 56(2)(d) and 56(5) AI Act</p>
                </div>
                <p>Signatories commit to reporting to the AI Office about their implementation of the Code, and
                    especially the application of their Framework to the development, making available on the market,
                    and/or use of their GPAISRs, by creating a Safety and Security Model Report (hereafter, a "Model
                    Report") for each GPAISR which they make available on the market, which will document, as specified
                    in the Measures for this Commitment: (1) the results of systemic risk assessment and mitigation for
                    the model in question; and (2) justifications of decisions to make the model in question available
                    on the market. </p>

                <p><em>In order to satisfy Commitment II.8:</em></p>
                <h4>Measure II.8.1. Level of detail</h4>
                <p>Signatories shall provide a level of detail in the Model Report that:</p>
                <ol>
                    <li>is proportionate to the level of systemic risk that the model in question reaches, or that the
                        Signatory reasonably foresees that model to reach along the entire model lifecycle; and</li>
                    <li>allows the AI Office to understand how a Signatory has implemented its systemic risk assessment
                        and mitigation measures pursuant to the Code.</li>
                </ol>
                <p>A Model Report shall contain sufficient reasoning as to why (1) and (2) are satisfied.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <p>As a potential example of reasoning that could justify a lower level of detail under (1), the
                        Model Report could argue that the model is a safely derived model in the sense of Measure
                        II.4.2, or could otherwise argue that it is similar to existing models available on the Union
                        market.</p>
                </div>

                <h4>Measure II.8.2. Reasons for deciding to make the model in question available on the market</h4>
                <p>Signatories shall provide in the Model Report the reasoning and information used to justify a
                    decision to make the model in question available on the market including for the first time, by
                    stating:</p>
                <ol>
                    <li>a clear chain of reasoning, based on the information and data provided in the Model Report,
                        which provides sufficient justification that the systemic risk of the GPAISR is acceptable as
                        per Measure II.5.1, including a sufficient safety margin pursuant to Measure II.4.12 and an
                        explanation thereof to evaluate it;</li>
                    <li>the conditions under which the Signatory's reasoning in point (1) would no longer hold; and</li>
                    <li>whether and how independent external assessors informed a decision to make the model in question
                        available on the market or use it, such as through the evaluation of models pursuant to
                        Commitment II.11.</li>
                </ol>


                <h4>Measure II.8.3. Documentation of systemic risk assessment and mitigation</h4>
                <p>In the Model Report, Signatories shall:</p>
                <ol>
                    <li>document and explain their systemic risk selection as per Measure II.3.1, including a
                        description of their efforts to select other systemic risks that are specific to the high-impact
                        capabilities of the GPAISR, with further reference to Appendices 1.2, 1.3, and 1.4 where
                        appropriate;</li>
                    <li>document and explain the systemic risk estimation process performed (pursuant to Measure
                        II.4.1), and provide a justification of all decisions made as part of that process, in
                        particular the choice of qualitative and/or quantitative estimates, and how the estimates relate
                        to systemic risk tiers;</li>
                    <li>where they make use of the "safely derived model" concept according to Measure II.4.2,
                        provide a justification of how the criteria for a "safe originator model" (as per Measure
                        II.4.2, point (1)) and the criteria for the "safely derived model" (as per Measure II.4.2, point
                        (2) and point (3)) are fulfilled;</li>
                    <li>document and explain the limitations of and uncertainties about the systemic risk assessment
                        carried out for the model in question, including limitations of the model evaluation methods
                        used (pursuant to Measure II.4.5) and the level of uncertainty in their model evaluation
                        results, and particularly: (a) safety margins (pursuant to Measure II.4.12); (b) any lower
                        levels of rigour in model evaluation adopted; (c) the reasons justifying such lower levels; and
                        (d) the consequences for the model evaluation conducted;</li>
                    <li>document and explain, for each model evaluation used to assess systemic risks of their
                        GPAISRs, their model elicitation work (pursuant to Measure II.4.6);</li>
                    <li>document and explain the ways in which they have taken into account AI systems information
                        pursuant to Measure II.4.7 for each model evaluation performed and explain why this was
                        appropriate for an effective assessment of the systemic risk in question;</li>
                    <li>document for each systemic risk assessed: (a) the qualifications of, as well as the level of
                        access, resources, and support provided to, their internal model evaluation teams (as required
                        by Measure II.4.11); and (b) the same information as required in (a) for independent external
                        assessors involved in assessments performed before making a model available on the market
                        (pursuant to Measure II.11.1). Alternatively to (b), Signatories shall procure any such
                        independent external assessors to provide the requisite information directly to the AI Office at
                        the same time that the Signatory produces its Model Report;</li>
                    <li>compare and explain the assessed systemic risk of the model in question both with and
                        without safety and security mitigations (implemented pursuant to Commitments II.6 and II.7), as
                        appropriate;</li>
                    <li>document and describe all systemic risk mitigations implemented for the model in question,
                        and explain their limitations;</li>
                </ol>

                <p>Only non-SME Signatories shall further:</p>
                <ol start="10">
                    <li>document and explain, for each model evaluation used to assess a systemic risk, the basis for
                        concluding that such model evaluations are state-of-the-art and satisfy points (1) to (3) in
                        Measure
                        II.4.4;</li>
                    <li>document and explain, for each model evaluation used, the internal validity, external validity,
                        and
                        reproducibility (pursuant to Measure II.4.5);</li>
                    <li>document and explain, for each model evaluation used, the coverage of expected use context and
                        modalities addressed by that model evaluation (pursuant to Measure II.4.8);</li>
                    <li>document and explain any results from exploratory research and any findings from open-ended
                        red-teaming (pursuant to Measure II.4.9); and</li>
                    <li>use best efforts to document in the Model Report which tools, best practices, and/or data for
                        model
                        evaluations they have shared and with whom (pursuant to Measure II.4.10), where such information
                        has not
                        been made publicly available, e.g. through releases of source code or publicly accessible
                        documents.</li>
                </ol>

                <h4>Measure II.8.4. External reports</h4>
                <p>Signatories shall include in the Model Report:</p>
                <ol>
                    <li>any available reports (e.g. via valid hyperlinks) from (a) independent external assessors who
                        reviewed the model in question before it was made available on the market (pursuant to Measure
                        II.11.1) and (b) security reviews undertaken by an independent external party (pursuant to
                        Measure II.7.2, point (1)), to the extent that respects existing confidentiality (including
                        commercial confidentiality) obligations and allows such external assessors or parties to
                        maintain control over the publication of their findings;</li>
                    <li>where no independent external assessor was involved in the model evaluation or the systemic risk
                        assessment, an explanation for why the criteria necessitating such involvement pursuant to
                        Measure II.11.1 have not been met, or why no independent external assessor was found despite
                        best efforts to identify and choose one in accordance with Measure II.11.1; and</li>
                    <li>where at least one independent external assessor was involved in the model evaluation or the
                        systemic risk assessment, a justification for the choice of assessor, based on the qualification
                        criteria in Measure II.11.1, unless the assessor has been recognised by the AI Office since
                        being engaged.</li>
                </ol>

                <h4>Measure II.8.5. Algorithmic improvements</h4>
                <p>Signatories shall ensure that the Model Report contains all high-level information about algorithmic
                    or other improvements specific to the GPAISR in question, compared to other GPAISR already available
                    on the market, that is relevant for the AI Office for understanding significant changes in the
                    systemic risk landscape and, thereby, allows the AI Office to better understand the practical
                    implementation of systemic risk assessment and mitigation required by the Code.</p>

                <h4>Measure II.8.6 Specification of intended model behaviour</h4>
                <p>Signatories shall ensure that Model Reports contain a specification of how Signatories intend the
                    model to operate, e.g. by:</p>
                <ol>
                    <li>specifying the principles that the model is intended to follow;</li>
                    <li>stating how the model is intended to prioritize different kinds of instructions; or</li>
                    <li>listing topics on which the model is intended to refuse instructions.</li>
                </ol>

                <h4>Measure II.8.7. Model Report updates</h4>
                <p>Signatories shall update their Model Reports when they have reason to believe that there has been a
                    material change in the systemic risk landscape that materially undermines the reasons for concluding
                    that the model posed acceptable systemic risk (Measure II.8.2, point (1)), e.g.:</p>
                <ol>
                    <li>the model's capabilities have changed significantly, be it via post-training, further
                        elicitation, or changes in affordances;</li>
                    <li>data drift, major improvements in the state-of-the-art of relevant model evaluation methods, or
                        other factors suggesting that the previous systemic risk assessment is no longer accurate;</li>
                    <li>improvements in mitigations;</li>
                    <li>serious incidents;</li>
                    <li>information from post-market monitoring (pursuant to Measure II.4.14), such as a material change
                        in how the model is being used versus how it was foreseen to be used in the original assessment;
                        or</li>
                    <li>information from internal use of the model.</li>
                </ol>

                <p>Signatories shall: (1) continuously keep the systemic risk landscape under review; and (2) ensure
                    that they identify and gather information relevant to any reason to believe that there has been a
                    material change in the systemic risk landscape, in order to update their Model Reports.</p>

                <p>Signatories shall ensure that the updated Model Report takes into account at least the following
                    information:</p>
                <ol>
                    <li>the content of their previous Model Report and the results of previously conducted systemic risk
                        assessments, as appropriate;</li>
                    <li>model-independent information not considered in the previous Model Report;</li>
                    <li>model evaluations not considered in the previous Model Report;</li>
                    <li>information from serious incident reports, including relevant information such as near-misses
                        (pursuant to Commitment II.12), so far as available;</li>
                    <li>information from post-market monitoring (pursuant to Measure II.4.14); and</li>
                    <li>information from internal use of the model.</li>
                </ol>

                <p>The updated Model Report shall contain at least:</p>
                <ol>
                    <li>the content of the previous Model Report, updated where relevant, including describing relevant
                        results and documentation from systemic risk assessments not covered in the previous Model
                        Report, and the information which prompted the Model Report update;</li>
                    <li>new reports submitted by independent external assessors as per Measure II.11.2 relevant to
                        systemic risk assessment, testing methodologies, and proposed mitigations of the Signatories'
                        GPAISR, with results consolidated as appropriate;</li>
                    <li>an assessment of whether the Framework was adhered to with regards to the relevant model since
                        the previous Model Report;</li>
                    <li>a changelog, describing how, if at all, the Model Report has been updated, along with a version
                        number and date of change; and</li>
                    <li>where applicable, an explanation why, following the process in this Measure, the Signatory
                        concludes that there has been no material change in the systemic risk landscape compared to the
                        previous Model Report and no further systemic risk assessment and mitigation is required for the
                        model in question at that point in time.</li>
                </ol>


                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>One major change in this Commitment was detailing how and when the Model Report should be
                        updated, and clarifying how this process relates to, for example, post-market monitoring. There
                        were parts of the second draft's Model Report Commitment that were read by stakeholders to
                        suggest that the Model Report had to be updated at least every 6 months. In the current draft,
                        the Model Report needs to be updated when providers "have reason to believe that there has been
                        a material change in the systemic risk landscape that materially undermines the reasons for
                        concluding that the model posed acceptable systemic risk."</p>
                    <p>Another change has been to consolidate into this Commitment content that was previously spread
                        throughout the Code regarding information to be covered in the Model Report, to make it easier
                        to parse.</p>
                    <p>We also made assessment of pre- and post- mitigation risk in Measure II.8.3 only as "appropriate"
                        because, as pointed out in feedback, it may not always be appropriate to remove certain
                        mitigations (e.g., security mitigations, alignment techniques that are integrated into
                        pre-training).</p>
                    <p>In Measure II.8.2, we focused the required reasoning on the adequacy of systemic risk
                        mitigations, in response to feedback about the complexity of factors that may inform a
                        deployment decision.</p>
                    <p>In Measure II.8.4, we have clarified that external reports do not have to be included verbatim in
                        the Model Report itself (e.g., a reference is acceptable), and that the inclusion of such
                        reports should respect existing agreements, for example in regards to restrictions on
                        publication.</p>

                </div>

                <h3>Commitment II.9. Adequacy assessments</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 55(1), 56(2)(d) and 56(5) AI Act</p>
                </div>

                <p>Signatories commit to assessing the adequacy of their Framework, the adoption and implementation of
                    which they have committed to under Commitment II.1, and to updating it based on the findings as
                    specified in the Measures for this Commitment.</p>

                <p><em>In order to satisfy Commitment II.9:</em></p>
                <h4>Measure II.9.1. Adequacy assessment process</h4>
                <p>Signatories shall, when conducting adequacy assessments, consider: (1) best practices; (2) relevant
                    research and state-of-the-art science; (3) incidents or malfunctions of the model and serious
                    incident reports; and (4) where already available, relevant independent external expertise.</p>
                <p>Signatories shall, as soon as possible and no later than within 5 business days of its completion,
                    provide the resultant adequacy assessment to the Signatory's management body acting in its
                    supervisory function or another appropriate independent body (such as a council or board).</p>

                <h4>Measure II.9.2. Model-specific adequacy assessment</h4>
                <p>Safely derived models (as per Measure II.4.2) are exempt from the below Measure.</p>
                <p>For the purpose of assessing the adequacy of the Framework in relation to their GPAISR, Signatories
                    shall document the following model-specific information in the adequacy assessment and provide such
                    updated adequacy assessment to the AI Office pursuant to Measure II.14.3 within four weeks of
                    notifying the AI Office that a general-purpose AI model has met or will meet the classification
                    condition in Article 51(1)(a) AI Act:</p>
                <ol>
                    <li><strong>Model description:</strong> A general description of the techniques and assets they
                        intend to use in
                        developing the model, including the use of other AI models;</li>
                    <li><strong>Systemic risk identification:</strong> The outcomes of the systemic risk identification
                        executed for this
                        model according to Commitment II.3 and in accordance with the Framework;</li>
                    <li><strong>Forecasts:</strong> All available forecasts (based on Measure II.1.3 and the techniques
                        in Measure
                        II.4.13), showing especially the capabilities, systemic risk indicators, and systemic risk tier
                        the model is intended or expected to exhibit or reach;</li>
                    <li><strong>Systemic risk analysis results:</strong> All available systemic risk analysis results
                        pursuant to
                        Commitment II.4;</li>
                    <li><strong>Systemic risk assessment and mitigation plans:</strong> An overview of the systemic risk
                        analysis and
                        systemic risk acceptance determination (Commitments II.4–II.5) and technical systemic risk
                        mitigations (Commitments II.6–II.7) already planned for the model, where known; and</li>
                    <li><strong>Systemic risk during the development phase:</strong> A characterisation of which
                        systemic risks, if any,
                        may arise during the model's development phase, along with an assessment of the extent to which
                        there are significant systemic risks that could arise during the development phase. Significant
                        systemic risks could arise during the development phase if, e.g.:
                        <ol type="a">
                            <li>the Signatory's security mitigations are inappropriate for the model's forecasted
                                systemic risk profile;</li>
                            <li>the model is capable of substantially undermining safety mitigations and/or has
                                sufficient autonomous capabilities to perform model exfiltration; or</li>
                            <li>substantial AI research and development capabilities are involved in the model's
                                development, e.g. where researcher productivity is being accelerated by five times or
                                where an AI model is used for tasks typically done by capability research teams at a top
                                AI company for similar total costs.</li>
                        </ol>
                    </li>
                </ol>

                <p>In certain circumstances, Signatories shall update the adequacy assessment and provide such updated
                    adequacy assessment to the AI Office pursuant to Measure II.14.3 whenever they reach milestones
                    defined in Measure II.2.2, point (1) after the initial adequacy assessment, specifically if all of
                    the following apply:</p>
                <ol>
                    <li>significant systemic risks arise during the development phase as per (6) above;</li>
                    <li>there have been material changes in any of (1)-(6) above;</li>
                    <li>no adequacy assessment specific to the relevant model has been completed in the past 12 weeks;
                        and</li>
                    <li>the model has not been made available on the market.</li>
                </ol>
                <p>Where Signatories' adequacy assessments are updated, Signatories shall focus particularly on the
                    systemic risks that they have concluded may arise during the development phase; e.g., for systemic
                    risks tied to AI research and development capabilities being involved in the development of the
                    model, updates shall focus on levels of AI labour automation and implemented safety and security
                    safeguards; whereas for systemic risks stemming from model leak or exfiltration, updates shall focus
                    on the Signatories' efforts to implement additional security mitigations.</p>


                <h4>Measure II.9.3. Framework adequacy assessment</h4>
                <p>Signatories shall conduct a Framework adequacy assessment when they are made aware of material
                    changes that could significantly undermine the adequacy of their Framework (such as via a serious
                    incident report), or every 12 months starting from their first making available of a GPAISR on the
                    market, whichever is sooner.</p>
                <p>Framework adequacy assessments shall cover the following questions:</p>
                <ol>
                    <li><strong>Framework with Code compliance:</strong> Does the Framework address all components
                        outlined in Commitment II.1 with sufficient detail to evaluate its effectiveness? Is the
                        Framework otherwise consistent with the Code?</li>
                    <li><strong>Framework adequacy:</strong> Are the measures, procedures, resources, and mitigations in
                        the Framework still proportionate to the complexity and scale of the systemic risks stemming
                        from the Signatory's GPAISRs which are planned by the Signatory to be made available on the
                        market within the next 12 months? Is the Framework appropriate given how the Signatory's
                        model(s) is being used and is expected to be used, both externally and internally, over the next
                        12 months? Does the Framework adequately account for uncertainty in future developments as well
                        as in the effectiveness of systemic risk assessment and mitigation techniques?</li>
                    <li><strong>Framework adherence:</strong> Is there strong reason to believe that the Framework will
                        be adhered to over the next 12 months?</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Many stakeholders, in particular providers, expressed concern that the Adherence and Adequacy
                        Assessment Commitment in the second draft would create too much paperwork without commensurate
                        gain in Signatories' assessment and mitigation of systemic risk. This concern was particularly
                        strong regarding the adherence assessment and around the frequency at which assessments were to
                        be run, with a strong preference expressed for assessments in response to material changes
                        rather than at particular intervals.</p>
                    <p>To address these concerns, we have streamlined the process. We have removed the adherence
                        assessment and instead clarified the process by which Model Reports are to be updated in
                        response to material changes (see Measure II.8.7). Further, we have separated out the adequacy
                        assessments that are triggered by the intention to place a new model on the market and the more
                        general framework adequacy assessment, as these warrant consideration of different questions.
                        Whereas the latter is aimed at ensuring Signatories can put in place relevant systemic risk
                        mitigations in due time (as some mitigations, in particular with regards to security, may take
                        up to a year to implement), the former is aimed at ensuring that the Signatory is able to
                        sufficiently safely and securely handle a model that they are intending to place on the Union
                        market.</p>
                    <p>We expect more refinement of this Commitment will be required for the fourth draft to further
                        streamline these processes.</p>
                </div>

                <h3>Commitment II.10. Systemic risk responsibility allocation</h3>
                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1), Article 56(2)(d) and Recital 114 AI Act</p>
                </div>
                <p>For activities concerning systemic risk assessment and mitigation for their GPAISRs, Signatories
                    commit, as specified in the Measures for this Commitment, to: (1) clearly defining and allocating
                    responsibilities for managing systemic risk from their GPAISRs across all levels of the
                    organisation; (2) allocating appropriate resources to actors who have been assigned responsibilities
                    for managing systemic risk; and (3) promoting a healthy risk culture.</p>

                <p>Signatories commit to allocating appropriate levels of responsibility and resources proportionately
                    to, at least, the Signatory's organisational complexity and governance structure, and the systemic
                    risks stemming from their GPAISRs.</p>

                <p><em>In order to satisfy Commitment II.10:</em></p>
                <h4>Measure II.10.1. Defining responsibilities</h4>
                <p>Signatories shall clearly define responsibilities for managing systemic risk stemming from their
                    GPAISRs across all levels of the organisation. This includes the following responsibilities:</p>
                <ol>
                    <li><strong>Risk oversight:</strong> Overseeing the Signatories' systemic risk assessment and
                        mitigation activities;</li>
                    <li><strong>Risk ownership:</strong> Managing systemic risks from Signatories' GPAISRs;</li>
                    <li><strong>Support and monitoring:</strong> Supporting and monitoring the Signatories' systemic
                        risk assessment and mitigation activities; and</li>
                    <li><strong>Assurance:</strong> Providing internal and, as appropriate, external assurance about the
                        adequacy of the Signatories' systemic risk assessment and mitigation activities to the
                        management body in its supervisory function or another appropriate independent body (such as a
                        council or board).</li>
                </ol>

                <p>Signatories shall allocate these responsibilities, as appropriate for the Signatories' governance
                    structure and organisational complexity, across the following levels of their organisation:</p>
                <ol>
                    <li>the management body in its supervisory function or another appropriate independent body (such as
                        a council or board);</li>
                    <li>the management body in its management function;</li>
                    <li>relevant operational teams;</li>
                    <li>internal assurance providers (e.g. an internal audit function), where available; and</li>
                    <li>external assurance providers (e.g. third-party auditors), where available.</li>
                </ol>

                <p>This Measure is presumed to be fulfilled if Signatories adhere to:</p>
                <ol>
                    <li>any of the following, where they have not been replaced or otherwise become obsolete: (a)
                        <a class="ai-act-link" href="https://www.iso.org/standard/77304.html">ISO/IEC23894:2023</a>
                        (Section 4.5.3); (b) <a class="ai-act-link" href="https://www.iso.org/standard/65694.html">ISO
                            31000:2018</a> (Section 4.5.3); (c) <a class="ai-act-link"
                            href="https://doi.org/10.6028/NIST.AI.100-1">the NIST AI Risk Management Framework</a>
                        (Govern 2); or (d) <a class="ai-act-link"
                            href="https://www.theiia.org/globalassets/documents/resources/the-iias-three-lines-model-an-update-of-the-three-lines-of-defense-july-2020/three-lines-model-updated-english.pdf">the
                            IIA's Three Lines Model</a>; or
                    </li>
                    <li>all of the following:
                        <ol type="a">
                            <li><strong>Risk oversight:</strong> The responsibility for overseeing the Signatory's
                                systemic risk management activities has been assigned to a specific committee of the
                                management body in its supervisory function (e.g. a risk committee or audit committee).
                                For Signatories that are SMEs, this responsibility may be assigned to an individual
                                member of the committee of the management body in its supervisory function.</li>
                            <li><strong>Risk ownership:</strong> The responsibility for managing systemic risks from
                                GPAISRs has been assigned to appropriate members of the management body in its
                                management function who are also responsible for relevant Signatory core business
                                activities that may give rise to systemic risk, such as research and product development
                                (e.g. Head of Research or Head of Product). The members of the management body have
                                assigned lower-level responsibilities to operational managers who oversee parts of the
                                systemic risk-producing activities (e.g. specific research domains or specific
                                products). Depending on the organisational complexity, there may be a cascading
                                responsibility structure.</li>
                            <li><strong>Support and monitoring:</strong> The responsibility for supporting and
                                monitoring the Signatory's systemic risk management activities has been assigned to a
                                member of the management body in its management function (e.g. a Chief Risk Officer).
                                This member must not also be responsible for the Signatory's core business activities
                                that may produce systemic risk (e.g. research and product development). This individual
                                is supported by a central risk function and other operational teams, as appropriate. For
                                Signatories that are SMEs, there is at least one individual in the management body in
                                its management function tasked with supporting and monitoring the Signatory's systemic
                                risk assessment and mitigation.</li>
                            <li><strong>Assurance:</strong> Responsibility for providing assurance about the adequacy of
                                the Signatory's systemic risk assessment and mitigation activities to the management
                                body in its supervisory function or another appropriate independent body (such as a
                                council or board) has been assigned to a relevant party (e.g. a Chief Audit Executive or
                                Head of Internal Audit). This individual is supported by an internal audit function and
                                external assurance as appropriate. The internal audit function is organisationally
                                independent from the management body in its management function. The Signatories'
                                internal assurance activities shall follow industry best practices (e.g. as expressed in
                                the IIA's Global Internal Audit Standards). For Signatories that are SMEs, the
                                management body in its supervisory function periodically assesses the Signatory's
                                systemic risk assessment and mitigation (e.g., by approving the Signatory's Framework
                                adequacy assessment).</li>
                        </ol>
                    </li>
                </ol>

                <h4>Measure II.10.2. Allocation of appropriate resources</h4>
                <p>Signatories shall ensure that their management bodies oversee the allocation of appropriate
                    resources, proportionate to the level of systemic risk stemming from the Signatories' GPAISRs, to
                    actors who have been assigned responsibilities for managing systemic risk from GPAISRs, including:
                </p>
                <ol>
                    <li>human resources;</li>
                    <li>financial resources;</li>
                    <li>access to information and knowledge; and</li>
                    <li>computational resources.</li>
                </ol>

                <p>This Measure is presumed to be fulfilled if Signatories adhere to any of the following, where they
                    have not been replaced or otherwise become obsolete:</p>
                <ol>
                    <li>ISO/IEC23894:2023 (Section 4.5.4);</li>
                    <li>ISO 31000:2018 (Section 4.5.4); or</li>
                    <li>NIST AI Risk Management Framework (Manage).</li>
                </ol>

                <h4>Measure II.10.3. Promotion of a healthy risk culture</h4>
                <p>Signatories shall promote a healthy risk culture and take measures to ensure that actors who have
                    been assigned responsibilities for managing systemic risk stemming from GPAISRs (pursuant to Measure
                    II.10.1) take a measured and balanced approach to systemic risk, neither being inappropriately
                    risk-seeking, nor risk-ignorant, nor risk-averse, as relevant to the level of systemic risk stemming
                    from the Signatories' GPAISRs.</p>

                <p>Signs of a healthy risk culture for the purpose of this Measure are, e.g.:</p>
                <ol>
                    <li>setting the tone with regards to a healthy systemic risk culture from the top;</li>
                    <li>allowing effective communication and challenge to decisions concerning systemic risk;</li>
                    <li>appropriate incentives to discourage excessive systemic risk-taking, such as rewards for
                        cautious behaviour and internal flagging of systemic risks;</li>
                    <li>anonymous surveys find that staff are aware of reporting channels, are comfortable raising
                        concerns about systemic risks, understand the Signatory's framework, and feel comfortable
                        speaking up; or</li>
                    <li>internal reporting channels are actively used and reports are acted upon appropriately.</li>
                </ol>

                <p>This Measure is presumed to be fulfilled if Signatories adhere to any of the following, where they
                    have not been replaced or otherwise become obsolete:</p>
                <ol>
                    <li>ISO/IEC23894:2023 in conjunction with ISO 31000:2018 (Clauses 4, 5.2-5.4, 6.1); or</li>
                    <li>NIST AI Risk Management Framework (Govern 4).</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Some stakeholders expressed concern that the Systemic Risk Responsibility Allocation Commitment
                        in the second draft was too prescriptive. This was partly due to some stakeholders reading the
                        KPIs as being compulsory, which was not our intention in drafting. Another common request was to
                        specify which existing standards could be used as a means of demonstrating compliance. We have
                        dealt with both of those concerns by clearly stating the overall goal that the Commitment is to
                        achieve and then describing specific (though not exclusive) ways in which Signatories can
                        demonstrate compliance, including via adherence to relevant standards.</p>
                    <p>We have also sought to make sure that the Commitment is appropriate to different sizes and types
                        of organisations, for example clarifying that relevant boards and councils can play the role of
                        the management body in its supervisory function, where appropriate. We welcome feedback on ways
                        in which the Commitment may still fail to appropriately account for different organisational
                        forms.</p>
                </div>

                <h3>Commitment II.11. Independent external assessors</h3>
                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1) and Recital 114 AI Act</p>
                </div>
                <p>Signatories commit to obtaining independent external systemic risk assessments, including model
                    evaluations, of their GPAISRs along the entire model lifecycle, to the extent and under the
                    conditions specified in Measures II.11.1 and II.11.2.</p>
                <p><em>In order to satisfy Commitment II.11:</em></p>

                <h4>Measure II.11.1. Assessments before market placement</h4>
                <p>As of 2 November 2025, before making the model available on the market, Signatories shall ensure that
                    their GPAISRs are subject to systemic risk assessments, including model evaluations, relevant to the
                    specific systemic risk in question, undertaken by independent external assessors, for all systemic
                    risks identified pursuant to Commitment II.3, unless the model is a safely derived model as per
                    Measure II.4.2 or the model is similarly safe as or safer than another GPAISR that:</p>
                <ol>
                    <li>has been made available on the market;</li>
                    <li>has gone through the systemic risk management process as per this Code and fulfills the systemic
                        risk acceptance criteria of the relevant Signatory, or has been made available on the Union
                        market before 2 May 2025;</li>
                    <li>has not been shown to have material safety or security flaws, such as identified via unmitigated
                        serious incidents; and</li>
                    <li>is sufficiently transparent to the Signatory (meaning the Signatory has sufficient visibility
                        into safety-relevant characteristics of the model such as its safety mitigations, security
                        mitigations, architecture, capabilities, affordances, modalities, and systemic risk profile,
                        which is assumed for fully open-source models and any models developed by the Signatory itself).
                    </li>
                </ol>

                <p>Criteria for similar safety for the purpose of this Measure include, where in comparison to the other
                    model:</p>
                <ol>
                    <li>Signatories do not reasonably foresee any new or different systemic risk scenarios of their
                        model after their systemic risk identification pursuant to Commitment II.3;</li>
                    <li>the scores on state-of-the-art benchmarks that measure relevant capabilities and identified
                        systemic risks (as per Commitment II.3) or on other state-of-the-art assessments of the
                        Signatories' model are all lower or equal, and the Signatory ensures sufficient qualification as
                        per Measure II.4.11, point (1);</li>
                    <li>the Signatories' model does not have additional modalities, capabilities, or affordances; and
                    </li>
                    <li>there are no other reasons to believe that the Signatories' model poses greater systemic risk.
                    </li>
                </ol>

                <p>Signatories shall use best efforts to identify and choose, e.g. through early search efforts, a
                    transparent call that has been public for at least two weeks, and early notifications of suitable
                    assessors, independent external assessors: (1) recognised by the AI Office; or (2) that:
                <ol type="a">
                    <li>have significant domain expertise for the systemic risk domain they are evaluating for
                        and are technically skilled and experienced in conducting model evaluations, if
                        relevant, as per criteria in Measure II.4.11, point (1); and</li>
                    <li>have internal and external information security protocols in place, sufficient for the
                        level and type of access granted,</li>
                </ol>
                <p>provided that the independent external assessor has agreed to protect commercially confidential
                    information, where they need access to such information.</p>
                <p>Signatories shall provide independent external assessors, to an extent that is necessary for them to
                    effectively assess systemic risks, with sufficient access, information, time, and resources, as per
                    Measure II.4.11. Signatories shall not undermine the integrity of external assessments by storing
                    and analysing model inputs and outputs from test runs without express permission, and shall provide
                    secure inference to independent external assessors where appropriate.</p>
                <p>If the Signatories' model is neither safely derived nor similarly safe as specified above, but
                    Signatories fail to identify independent external assessors, qualified as per the criteria in
                    Measure II.11.1, Signatories shall take into account potential additional risk and uncertainty of
                    systemic risks arising from the absence of external assessment when determining the acceptability of
                    systemic risks pursuant to Commitment II.5.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>If the model will not pose systemic risk beyond that stemming from GPAISRs already on the
                            market, Signatories recognise nevertheless that involving external assessors in their
                            systemic risk assessment process can bring in more expertise and provide independent
                            assurance on their systemic risk assessments.</p>
                    </div>
                </div>

                <img class="illustration" src="illustration2-light.png" data-light-src="illustration2-light.png"
                    data-dark-src="illustration2-dark.png"
                    alt="When do Signatories commit to external assessment of a specific systemic risk before market placement? A decision tree flowchart showing the criteria that determine when AI model providers must seek external assessment for specific systemic risks, starting with whether the model is general-purpose and following through various conditional factors including market placement date, risk scope, internal expertise, and additional risk compared to similar models.">

                <h4>Measure II.11.2. Assessments after market placement</h4>

                <p>After making the model available on the market, Signatories shall facilitate exploratory independent
                    external assessment of their GPAISRs, by implementing a research program providing API access to:
                    (1) models in the form they are made available on the market; and (2) models without certain
                    mitigations, such as helpful-only or base models. Signatories shall allocate free research API
                    credits for such research programs to assessors conducting non-commercial, legitimate research
                    related to systemic risks. Signatories shall not undermine the integrity of external assessments by
                    storing and analysing model inputs and outputs from test runs without express permission, and shall
                    provide secure inference to independent external assessors where appropriate.</p>

                <p>For the purpose of identifying and choosing an independent external assessor for this Measure,
                    Signatories shall publish clear and transparent criteria, reflecting relevant work and/or academic
                    experience, for evaluating applications from independent external assessors with stricter criteria
                    for access to models without certain mitigations to account for the increased sensitivity of these
                    models, where appropriate.</p>

                <p>Signatories shall contribute to a legal and technical safe harbour regime for the exploratory
                    independent external assessment, including not taking any legal or technical retaliation against the
                    assessors as a consequence of their assessment and, where applicable, publication of findings, as
                    long as the assessors:</p>

                <ol>
                    <li>do not intentionally disrupt system availability through the testing, unless explicitly
                        permitted;</li>
                    <li>do not intentionally access, modify, and/or use sensitive or confidential user data without
                        explicit consent by the user, and where assessors do access such data, they must collect only
                        what is necessary for reporting, report it immediately, refrain from dissemination, and delete
                        it as soon as legally possible;</li>
                    <li>do not use findings to threaten Signatories, users, or other actors in the value chain, provided
                        that disclosure under pre-agreed policies and timelines shall not be counted as such coercion;
                        and</li>
                    <li>adhere to the Signatory's publicly available process for responsible vulnerability disclosure,
                        which shall specify, at a minimum, a pre-agreed timeline of at most 30 days starting from the
                        notification of the Signatory for any such publication, unless a disclosure of findings would
                        significantly increase systemic risks.</li>
                </ol>

                <p>Fully open-sourcing a GPAISR provides an alternative means of fulfilling this Measure.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Many stakeholders were concerned about the lack of detail provided in the second draft with
                        respect to relevant expertise, sufficient access, and
                        sufficient time in the context of external assessments, which we have tried to clarify in Draft
                        3. Note that these aspects have largely been addressed in Measure II.4.11, due to their
                        relevance for both internal and external assessments and to reduce repetition throughout the
                        Code.</p>

                    <p>Stakeholders also asked for clarity on (a) the concept of similarity which we put forth as one
                        situation in the second draft in which external assessments may not be necessary pre-market
                        placement and (b) what constitutes "sufficient evidence" for such similarity. We have tried to
                        provide clarification in this draft. Based on some stakeholder's feedback, we added the concept
                        of
                        'safely derived models' (as defined in Measure II.4.2) as another setting when external
                        assessments might not be
                        needed.</p>

                    <p>We continued to receive concerns by stakeholders that the current evaluator ecosystem is not
                        mature enough to support external assessments. To alleviate these concerns, we have added a
                        grace period until November 2025 to find an adequate external assessor for GPAISR pursuant to
                        Measure II.11.1. In addition, some stakeholders were concerned about having to work with
                        external
                        evaluators that do not agree to respect trade secrets and protect commercially confidential
                        information. We have
                        clarified in this draft that providers can refuse to work with an evaluator on these grounds,
                        even if that
                        means that no qualified external assessor can be identified pursuant to Measure II.11.1.</p>

                    <p>Stakeholders also expressed concern because the AI Office was named as a potential evaluator in
                        the last draft. We want to emphasise that our intention was not to communicate that the AI
                        Office would necessarily have to play a role in risk assessment before market placement. To the
                        contrary, Signatories can select which assessor they want to work with, as long as they fulfil
                        the
                        qualification criteria put forth in Measure II.11.1.</p>

                    <p>Stakeholders were further concerned with the previous wording in post-market placement
                        assessments that asked Signatories to provide "non-restrictive access to deployed models". We
                        have clarified in Measure II.11.2 that: (1) API access is sufficient to satisfy this Measure;
                        and (2) that Signatories can specify criteria for researcher access that are more restrictive
                        for more
                        sensitive models (e.g., base models). We further addressed stakeholder feedback that reporting
                        all external
                        assessments pursuant to Measure II.11.2 would create unnecessary paperwork; instead, Signatories
                        now have
                        the option to report consolidated assessment results (as per Measure II.11.2.) in their updated
                        Model Report
                        (as per Measure II.8.7).</p>
                </div>

                <h3>Commitment II.12. Serious incident reporting</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1)(c) and Recital 115 AI Act</p>
                </div>

                <p>Signatories commit, to the extent and under the conditions specified in Measures II.12.1 to II.12.4,
                    to setting up processes for keeping track of, documenting, and reporting to the AI Office and, as
                    appropriate, to national competent authorities without undue delay relevant information about
                    serious incidents throughout the entire model lifecycle and possible corrective measures to address
                    them, with adequate resourcing of such processes relative to the severity of the serious incident
                    and the degree of involvement of their model.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p><em>Signatories shall understand the reporting of a serious incident not as an admission of
                                wrongdoing.</em></p>
                    </div>
                </div>

                <p>In order to satisfy Commitment II.12:</p>

                <h4>Measure II.12.1. Methods for serious incident identification</h4>

                <p>Signatories shall use methods to keep track of relevant information about serious incidents that are
                    appropriate to their business models and means of making the model available on the market, e.g.:
                </p>
                <ol>
                    <li>tracing the outputs produced by their models using, e.g., watermarks, metadata, or other
                        state-of-the-art provenance techniques;</li>
                    <li>conducting privacy-preserving logging and metadata analysis of queries and generations of the
                        model relevant to serious incidents, where technically, legally, and commercially feasible,
                        considering the release and distribution strategy of the model;</li>
                    <li>reviewing other sources of information with a view to keeping track of relevant information
                        about serious incidents, such as police and media reports, posts on social media, dark web
                        forums, research papers, and incident databases; or</li>
                    <li>facilitating the reporting of serious incidents and relevant information about serious incidents
                        by downstream providers, users, and other third parties either (a) to the Signatory or (b) to
                        the AI Office and, as appropriate, to national competent authorities by informing such third
                        parties of direct reporting channels, if available, without prejudice to any of their reporting
                        obligations under Article 73 AI Act.</li>
                </ol>

                <h4>Measure II.12.2. Relevant information for serious incident tracking, documentation, and reporting
                </h4>

                <p>Signatories shall keep track of, document, and report to the AI Office and, as appropriate, to
                    national competent authorities at least the following information to the best of their knowledge:
                </p>
                <ol>
                    <li>the start and end dates of the serious incident, or best approximations thereof where the
                        precise dates are unclear;</li>
                    <li>the resulting harm and the victim or affected group of the serious incident;</li>
                    <li>the chain of events that (directly or indirectly) led to the serious incident as far as it is
                        reconstructable;</li>
                    <li>the model version involved in the serious incident;</li>
                    <li>a description of material available setting out the GPAISR's involvement in the serious
                        incident;</li>
                    <li>what, if anything, the Signatory intends to do or has done in response to the serious incident;
                    </li>
                    <li>what, if anything, the Signatory recommends the AI Office and, as appropriate, national
                        competent authorities do in response to the serious incident;</li>
                    <li>a root cause analysis, including, as far as possible, a description of the GPAISR's outputs that
                        (directly or indirectly) led to the serious incident and the factors that contributed to their
                        generation, including the inputs used and any potential failures or circumventions of safeguards
                        that contributed to producing these outputs; and</li>
                    <li>any known near-misses, where a similar serious incident was close to occurring prior to the
                        serious incident.</li>
                </ol>

                <p>Signatories shall investigate the causes and effects of serious incidents, including the information
                    of the preceding list, with a view to informing systemic risk analysis (Commitment II.4). Where
                    Signatories do not yet have certain relevant information from the preceding list, they shall record
                    that in their serious incident reports. The level of detail in serious incident reports shall be
                    proportionate to the severity of the incident.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>The Signatories recognise that relevant information about serious incidents cannot be kept
                            track of, documented, and reported at the GPAISR level only in retrospect after a serious
                            incident occurred. The information that could (directly or indirectly) lead up to such an
                            event is often dispersed and may be lost, overwritten, or fragmented by the time Signatories
                            become aware of a serious incident. This justifies the establishment of processes to keep
                            track of and document relevant information before serious incidents occur.</p>
                    </div>
                </div>

                <h4>Measure II.12.3. Reporting timelines</h4>
                <p>Signatories shall provide the information in points (1)–(7) of Measure II.12.2 in an initial report
                    that is submitted to the AI Office and, as appropriate, to national competent authorities, at the
                    following points in time, save for exceptional circumstances, where an incident or malfunction of
                    the GPAISR (directly or indirectly) led to:</p>
                <ol>
                    <li>a serious and irreversible disruption of the management or operation of critical infrastructure,
                        immediately, and not later than 2 days after the Signatory becomes aware of the involvement of
                        the model in the incident;</li>
                    <li>a death of a person or where the Signatory suspects such a causal relationship between the
                        GPAISR and the death, immediately, and not later than 10 days after the Signatory becomes aware
                        of the involvement of the GPAISR in the incident;</li>
                    <li>serious harm to a person's health, an infringement of obligations under Union law intended to
                        protect fundamental rights, or serious harm to property or the environment, or where the
                        Signatory establishes the reasonable likelihood of such a causal relationship between the GPAISR
                        and such harms or infringements, immediately, and not later than 15 days after the Signatory
                        becomes aware of the involvement of the GPAISR in the incident; or</li>
                    <li>a serious cybersecurity incident involving the GPAISR or (self-)exfiltration of model weights,
                        immediately, and not later than 5 days after the Signatory becomes aware of the incident.</li>
                </ol>

                <p>Signatories shall update the information in the initial report and add further information required
                    by Measure II.12.2, as available, in an intermediate report that is submitted to the AI Office and,
                    as appropriate, to national competent authorities at least every 4 weeks after the initial report,
                    until the serious incident is resolved.</p>

                <p>Signatories shall submit a final report, covering all the information required by Measure II.12.2, to
                    the AI Office and, as appropriate, to national competent authorities not later than 60 days after
                    the serious incident has been resolved. Where multiple similar events have occurred, Signatories
                    shall choose whether to submit a consolidated report or individual reports.</p>

                <h4>Measure II.12.4. Retention period</h4>
                <p>Signatories shall keep documentation of all relevant data produced in adhering to this Commitment for
                    at least 36 months from the date of the documentation or the date of the serious incident of the
                    GPAISR, whichever is later, without prejudice to Union law on data protection.</p>

                <img class="illustration" src="illustration3-light.png" data-light-src="illustration3-light.png"
                    data-dark-src="illustration3-dark.png"
                    alt="How and when should Signatories report serious incidents related to their GPAISR? A timeline diagram showing GPAISR incident reporting requirements: initial reports must be filed within 2-15 days of a serious incident (depending on type), followed by intermediate reports every four weeks until resolution, and a final report no later than 60 days after the incident is resolved.">

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>There was broad-based agreement across stakeholders that serious incident reporting is an
                        important Commitment in the Code. Stakeholders were further in agreement that timelines should
                        be clarified and that a tiered reporting system may be appropriate. Further, a number of
                        stakeholders raised concern about a passage stating that Signatories shall focus their efforts
                        on serious incidents that are more easily identifiable. Others asked for clarity about what
                        would constitute sufficient evidence to trigger a reporting obligation. To address these
                        concerns, we have adapted the serious incident reporting requirements in Article 73 AI Act for
                        high-risk AI systems to the case of GPAISRs.</p>
                    <p>We further received multiple requests to increase the retention period for information relevant
                        to this Commitment. Stakeholders found 12 months to be too short; it has been increased to 36
                        months.</p>
                </div>

                <h3>Commitment II.13. Non-retaliation protections</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1)(b) and (c) AI Act</p>
                </div>

                <p>Signatories commit to not retaliating against any worker providing information about systemic risks
                    stemming from the Signatories' GPAISRs to the AI Office or, as appropriate, to national competent
                    authorities, and to at least annually informing workers of an AI Office mailbox designated for
                    receiving such information, if such a mailbox exists.</p>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p>The Signatories recognise that the Commitment under this Code not to retaliate against their
                            workers is without prejudice to any obligations arising from Directive (EU) 2019/1937 on the
                            protection of whistleblowers and implementing laws of Member States. Obligations under the
                            Directive include: establishing secure and confidential internal reporting channels that
                            allow for both written and oral reporting; designating an impartial person or department to
                            handle reports, acknowledge receipt within seven days, and provide feedback within three
                            months; maintaining records; providing clear information on both internal and external
                            reporting procedures; and protecting whistleblowers from all forms of retaliation, including
                            dismissal, demotion, or harassment.</p>
                    </div>
                </div>

                <p>No measures are set out for Commitment II.13; instead, the Commitment text itself defines the
                    criteria for its fulfillment.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Stakeholders voiced concerns that, by restating and summarising obligations under Directive (EU)
                        2019/1937, the Code might introduce legal ambiguity due to the risk of divergence from the
                        Directive. At the same time, multiple SMEs and CSO stakeholders supported a reference to the
                        Directive in the Code. To address this feedback, we moved the reference to the Directive into a
                        recital, rather than stating it as part of the Commitment.</p>
                    <p>Stakeholders were further concerned that the Directive does not apply to providers with less than
                        50 employees. We have sought to address this in the new Commitment II.13 by deriving a modest
                        non-retaliation requirement from Article 55(1)(b) AI Act.</p>
                </div>

                <h3>Commitment II.14. Notifications</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 52(1), 53(3), 56(2)(d) and 56(5) AI Act</p>
                </div>

                <p>Signatories commit, as specified in the Measures for this Commitment, to: (1) notifying the AI Office
                    of relevant information regarding their general-purpose AI models meeting the condition for
                    classification as GPAISRs; and (2) regularly notifying the AI Office of the implementation of the
                    Commitments and Measures of this Code. For the purpose of assessing the implementation of this Code
                    through the AI Office, Signatories commit to offering clarifications, including via further
                    documentation or interviews, where requested by the AI Office.</p>

                <p><em>In order to satisfy Commitment II.14:</em></p>

                <h4>Measure II.14.1. GPAISR notification</h4>
                <p>Signatories shall estimate, using best efforts and taking into account best practices or guidance
                    from the AI Office where available, whether their general-purpose AI model has or will meet the
                    classification condition in Article 51(1)(a) AI Act, in particular by exceeding the training compute
                    threshold in Article 51(2) AI Act. Where there is sufficient reason to believe that the resultant
                    model will exceed the training compute threshold, such estimates shall be carried out before a
                    training run is started.</p>
                <p>If the estimated training compute exceeds the threshold in Article 51(2) AI Act, Signatories shall
                    notify the AI Office without delay and in any event within two weeks, as per Article 52(1) AI Act.
                    Signatories need not notify the AI Office about models that they will not place on the Union market.
                </p>
                <p>Signatories shall consider notifying the AI Office, if they put a model – that they know will meet
                    the classification condition of Article 51(1)(a) AI Act but which they do not intend to place on the
                    Union market – into significant internal use or where they use a model that would be classified as a
                    GPAISR if it were made available on the Union market for the sole purpose of developing another
                    GPAISR that they intend to make available on the market, without prejudice as to whether doing so
                    would qualify as placing a model on the Union market.</p>

                <h4>Measure II.14.2. Framework update notification</h4>
                <p>Signatories shall provide the AI Office with access to the latest unredacted version of their
                    Framework no later than within five business days of a confirmed update to their Framework, e.g.
                    through a publicly accessible link or a sufficiently secure channel specified by the AI Office.</p>

                <h4>Measure II.14.3. Adequacy assessment notification</h4>
                <p>Signatories shall provide the AI Office with access to the latest unredacted version of their
                    adequacy assessment under Commitment II.9 no later than within five business days of a confirmed
                    assessment, e.g. through a publicly accessible link or through a sufficiently secure channel
                    specified by the AI Office.</p>

                <h4>Measure II.14.4. Safety and Security Model Report notification</h4>
                <p>Signatories shall provide the AI Office with access to an unredacted Model Report at the latest by
                    the time they first make a GPAISR available on the market, e.g. through a publicly accessible link
                    or through a sufficiently secure channel specified by the AI Office. Where a Model Report is
                    updated, Signatories shall provide the AI Office access to an unredacted version of the updated
                    Model Report within 5 business days of a confirmed update.</p>

                <img class="illustration" src="illustration4-light.png" data-light-src="illustration4-light.png"
                    data-dark-src="illustration4-dark.png"
                    alt="When and what should Signatories report to the AI Office for their GPAISR? This timeline diagram illustrates the key reporting requirements for AI Signatories to the AI Office, showing specific deadlines including GPAISR notification (2 weeks), Framework reporting and Model-specific assessments (4 weeks), Model Reports on Market Placement, and Framework adequacy assessments (12 months after market placement), with relevant regulatory article references included for each requirement.">

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>Many respondents asked for further clarification in Measure II.14.1 on how and/or when to
                        estimate whether a GPAISR exceeds the training compute threshold specified in Article 51(2) AI
                        Act thereby meeting the classification condition. We have opted to not specify the "how" in the
                        Code at this point, and to instead write Measure II.14.1 in a way that leaves room for potential
                        AI Office guidance on this matter. For the "when", we clarified that estimates of training
                        compute shall be conducted where there is sufficient reason to believe that the training run
                        could bring the cumulative computation of the model above the threshold in Article 51(2) AI Act.
                        Consequently, no estimates are required where post-training is applied to a model that either is
                        already above the compute threshold or significantly below it. We also clarified that no
                        notification is required where a model that meets the training compute threshold is not to be
                        placed on the Union market.</p>
                    <p>There were no major changes in the rest of the Commitment, other than streamlining in light of
                        updates in other Commitments such as changed timings for the production and updating of adequacy
                        assessments and Model Reports.</p>
                </div>

                <h3>Commitment II.15. Documentation</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Articles 53(1)(a), 55(1), 56(2)(d) and 56(5) AI Act</p>
                </div>

                <p>Signatories commit to documenting relevant information under the AI Act and the Code, as specified in
                    Measure II.15.1.</p>
                <p><em>In order to satisfy Commitment II.15:</em></p>

                <h4>Measure II.15.1. Documentation regarding GPAISRs</h4>
                <p>Signatories shall document information relevant to their assessment and mitigation of systemic risks
                    from their GPAISRs, as well as relevant to their obligations under Article 53(1)(a) and (b),
                    specifically Annex XI, Section 2 AI Act. This includes:</p>
                <ol>
                    <li>the documentation requirements outlined in Commitment I.1;</li>
                    <li>the Framework, including previous versions thereof;</li>
                    <li>the Model Report, including previous versions thereof; and</li>
                    <li>all completed adequacy assessments, where available.</li>
                </ol>
                <p>Documentation shall be retained for at least 12 months after the model's retirement.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>The Documentation Commitment in the second draft included a Measure to document information that
                        the AI Office can use in designating GPAI models as GPAISRs. The only such information not
                        already required under Article 53(1)(a), Annex XI AI Act is the information regarding the
                        models' number of business and end users. That Measure has been removed as it is not required by
                        the AI Act and Signatories that offer proprietary models already likely document this
                        information nonetheless.</p>
                    <p>Further, we have sought to further simplify the documentation requirements, by pointing to the
                        documentation required by other parts of the Code.</p>
                </div>


                <h3>Commitment II.16. Public transparency</h3>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 55(1)(b) and 56(2)(d) AI Act</p>
                </div>

                <p>Signatories commit to publishing information relevant to the public understanding of systemic risks
                    stemming from their GPAISRs, where necessary to effectively enable assessment and mitigation of
                    systemic risks, to the extent and under the conditions specified in Measure II.16.1.</p>

                <p><em>In order to satisfy Commitment II.16:</em></p>

                <h4>Measure II.16.1. Publication of Frameworks and Model Reports (or similar documents)</h4>
                <p>Signatories shall make public information about the systemic risks stemming from their GPAISRs that
                    can improve:</p>
                <ol>
                    <li>public awareness and knowledge of systemic risks stemming from the model;</li>
                    <li>societal resilience against systemic risks before and when they materialise; and/or</li>
                    <li>detection of systemic risks before and when they materialise,</li>
                </ol>
                <p>where necessary to effectively enable assessment and mitigation of systemic risks.</p>
                <p>To satisfy this Measure, it will be sufficient for Signatories to publish (via their websites):</p>
                <ol>
                    <li>new or updated versions of their Frameworks after they have been provided to the AI Office; and
                    </li>
                    <li>after the relevant model is made available on the market, their Model Reports, or similar
                        documents that summarise Model Reports, in particular that:
                        (a) describe the systemic risk assessment methodology (including details to establish rigour
                        pursuant to Measure II.4.5) and results; (b)summarise the reasons for deciding to make the model
                        in question available on the market (as per Measure II.8.2); and (c) describe systemic risk
                        mitigations put in place.</li>
                    </li>
                </ol>
                <p>Where such information is published, it shall be with any redactions necessary to prevent
                    substantially increased systemic risk that could result from, e.g., undermining the effectiveness of
                    safety mitigations (in accordance with Commitment II.6) or security mitigations (in accordance with
                    Commitment II.7), or the divulging of sensitive commercial information to a degree disproportionate
                    to the societal benefit of the publication.</p>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>In the second draft of the Code, Signatories were required to publish (redacted versions of)
                        their Frameworks and Model Reports (or equivalent), which already is a common practice in the
                        industry. However, to better align with the AI Act, the new Commitment has been adjusted such
                        that Signatories only commit to publicly sharing such documents where doing so is necessary to
                        assess and mitigate risk.</p>
                </div>

                <h2>Appendix 1. Systemic Risk Taxonomy</h2>

                <div class="legal-box">
                    <h4><i class="ph ph-scales"></i>LEGAL TEXT<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>Article 3(64) AI Act: 'high-impact capabilities' means capabilities that match or exceed the
                        capabilities recorded in the most advanced general-purpose AI models;</p>
                    <p>Article 3(65) AI Act: 'systemic risk' means a risk that is specific to the high-impact
                        capabilities of general-purpose AI models, having a significant impact on the Union market due
                        to their reach, or due to actual or reasonably foreseeable negative effects on public health,
                        safety, public security, fundamental rights, or the society as a whole, that can be propagated
                        at scale across the value chain;</p>
                    <p>Additional legal text: Recital 110 AI Act</p>
                </div>

                <h3>Appendix 1.1. Selected types of systemic risk</h3>
                <p></p>The following are treated as selected types of systemic risk for the purpose of the systemic risk
                selection in Measure II.3.1:</p>

                <ol>
                    <li><strong>Cyber offence:</strong> Risks related to offensive cyber capabilities that could enable
                        large-scale sophisticated cyber-attacks, including on critical systems (e.g. critical
                        infrastructure). This includes automated vulnerability discovery, exploit generation,
                        operational use, and attack scaling.</li>

                    <li><strong>Chemical, biological, radiological and nuclear risks:</strong> Risks of enabling
                        chemical, biological, radiological, and nuclear attacks or accidents. This includes
                        significantly lowering the barriers to entry for malicious actors, or increasing the potential
                        impact achieved, in the design, development, acquisition, and use of such weapons.</li>

                    <li><strong>Harmful manipulation:</strong> Risks of enabling the targeted distortion of the
                        behaviour of persons, in particular through multi-turn interactions, that causes them to take a
                        decision that they would not have otherwise taken, in a manner that causes, or is reasonably
                        likely to cause, significant harm on a large scale. This includes the capability to manipulate
                        through multi-turn interaction and the propensity of models to manipulate, including
                        manipulation of high-stakes decision makers, large-scale fraud, or exploitation of people based
                        on protected characteristics. As a guide, risk of harmful manipulation exists if it cannot,
                        without reasonable doubt, be ruled out that a GPAISR, when integrated into an AI system, enables
                        the AI system, irrespective of the intention of the AI system provider or deployer, to deploy
                        subliminal, purposefully manipulative, or deceptive techniques as outlined in the <a
                            class="ai-act-link"
                            href="https://digital-strategy.ec.europa.eu/en/library/commission-publishes-guidelines-prohibited-artificial-intelligence-ai-practices-defined-ai-act">Commission
                            Guidelines on prohibited artificial intelligence practices established by Regulation (EU)
                            2024/1689 (AI Act).</a>
                    </li>
                    <li><strong>Loss of control:</strong> Risks related to the inability to oversee and control
                        autonomous GPAISRs that may result in large-scale safety or security threats or the realisation
                        of other systemic risks. This includes model capabilities and propensities related to autonomy,
                        alignment with human intent or values, self-reasoning, self-replication, self-improvement,
                        evading human oversight, deception, or resistance to goal modification. It further includes
                        model capabilities of conducting autonomous AI research and development that could lead to the
                        unpredictable emergence of GPAISRs without adequate risk mitigations. Loss of control scenarios
                        can emerge, e.g., from unintended model propensities and technical failures, intentional
                        modifications, or attempts to bypass safeguards.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>The list has been sharpened to focus on risks that are specific to the high-impact capabilities
                        of general-purpose AI models. The descriptions of systemic risks have been refined and expanded.
                        'Loss of human oversight' has been changed back to 'Loss of control' following stakeholder
                        feedback that the original label was clearer.</p>
                    <p>It is strongly emphasised that some of the risks previously considered for inclusion as selected
                        types of systemic risk are considered serious, but were found to be better addressed through
                        other parts of the AI Act or other laws, including the Chapters on AI systems in the AI Act, as
                        well as the General Data Protection Regulation (GDPR), Digital Services Act (DSA), and Digital
                        Markets Act (DMA).</p>
                </div>

                <h3>Appendix 1.2. Other types of risks for potential consideration in the selection of systemic risks
                </h3>
                <p>The following are treated as other types of risks, from which systemic risks may arise, for the
                    purpose of systemic risk selection in Measure II.3.1:</p>

                <ol>
                    <li><strong>Risks to public health, safety, or public security</strong>, e.g.: risk to critical
                        sectors; risk of major accidents; risk to critical infrastructure.</li>
                    <li><strong>Risks to fundamental rights</strong>, e.g.: risk to freedom of expression; risk to
                        non-discrimination; risk to privacy and the protection of personal data; risk from child sexual
                        abuse material (CSAM) and non-consensual intimate images (NCII).</li>
                    <li><strong>Risks to society as a whole</strong>, e.g.: risk to the environment; risk to non-human
                        welfare; risk to financial system stability; risk to democratic processes; risk from illegal,
                        violent, hateful, radicalising, or false content.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Explanatory note<i class="ph ph-caret-down box-caret"></i></h4>
                    <p>The above list is based on the definition of systemic risk in Article 3(65) AI Act. Some of the
                        risks listed overlap but are presented for clarity of communication. Such overlap should not
                        result in duplication of effort in risk identification, assessment, or mitigation.</p>
                </div>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>The section on 'Additional risks for consideration' was changed to a section on 'Other types of
                        risk for potential consideration in the selection of systemic risks'. Many of these risks are
                        important to consider in systemic risk identification. The systemic risk assessment and
                        mitigation sections of the Code have been drafted to accommodate additional risks that providers
                        may choose to assess and mitigate in the future. This is important to keep the Code future-proof
                        and avoids locking in an inappropriately small set of systemic risks and/or systemic risk
                        assessment or mitigation techniques at this time of high uncertainty. Of note, Measure II.8.3
                        calls for documenting the systemic risk selection in the Model Report.</p>
                </div>


                <h3>Appendix 1.3. Nature of systemic risks</h3>
                <p>The following considerations concerning the nature of systemic risks inform the selection of other
                    systemic risks in Measure II.3.1:</p>

                <ol>
                    <li><strong>Specific to advanced capabilities:</strong> Specific to the capabilities of the most
                        advanced general-purpose AI models.</li>
                    <li><strong>Significant impact:</strong> Resulting harm can have large-scale negative effects.</li>
                    <li><strong>High velocity:</strong> Resulting harm can materialise rapidly, potentially outpacing
                        defenses, mitigations and decision-making systems.</li>
                    <li><strong>Compounding or cascading:</strong> Resulting harm can compound or cascade in a course of
                        events that may permeate multiple layers of systems or of society.</li>
                    <li><strong>Difficult or impossible to reverse:</strong> Resulting harm is very difficult or
                        impossible to reverse.</li>
                    <li><strong>Asymmetric impact:</strong> Significant harm can be caused by small groups of actors or
                        a small number of events.</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Summary of changes from the second draft<i
                            class="ph ph-caret-down box-caret"></i></h4>
                    <p>The section 'Considerations for identifying systemic risk' has been removed, and some of the
                        content is now reflected in the section on 'Nature of systemic risks'. The 'Practical
                        considerations' listing has been removed as these considerations may be subject to change and
                        may depend on the specific context of individual Signatories.</p>
                </div>

                <h3>Appendix 1.4. Sources of systemic risk</h3>
                <p>The following model capabilities, model propensities, model affordances, and contextual factors are
                    treated as potential sources of systemic risk for the purpose of the systemic risk identification in
                    Commitment II.3:</p>

                <h4>Appendix 1.4.1. Model capabilities</h4>
                <ol>
                    <li>Offensive cyber capabilities</li>
                    <li>Chemical, Biological, Radiological, and Nuclear (CBRN) capabilities, and other such weapon
                        acquisition or proliferation capabilities</li>
                    <li>Capabilities to manipulate, persuade, or deceive</li>
                    <li>Autonomy</li>
                    <li>Capability to adaptively learn new tasks</li>
                    <li>Capabilities of long-horizon planning, forecasting, or strategising</li>
                    <li>Capabilities of self-reasoning (e.g., a model's ability to reason about itself, its
                        implementation, or environment, its ability to know if it is being evaluated)</li>
                    <li>Capability to evade human oversight</li>
                    <li>Capabilities to self-replicate, self-improve, or modify own implementation environment</li>
                    <li>Automated AI research and development capabilities</li>
                    <li>Capability to process multiple modalities (e.g., text, images, audio, video, and further
                        modalities)</li>
                    <li>Capability to use tools, including "computer use" (e.g., interacting with other software
                        components, application interfaces, and user interfaces)</li>
                    <li>Capability to control physical systems</li>
                </ol>

                <div class="recital-box">
                    <h4><i class="ph ph-quotes"></i>Potential material for future Recital <span
                            class="recital-number"></span><i class="ph ph-caret-down box-caret"></i></h4>
                    <div id="recital-placeholder">
                        <p><em>Many of the capabilities listed in Appendix 1.4.1 are also important for enabling
                                beneficial uses, and a model demonstrating these capabilities does not necessarily imply
                                that the model poses a systemic risk.</em></p>
                    </div>
                </div>

                <h4>Appendix 1.4.2. Model propensities</h4>
                <p>Model propensities, which encompass inclinations or tendencies of a model to exhibit some behaviours
                    or patterns:</p>

                <ol>
                    <li>Misalignment with human intent or values</li>
                    <li>Tendency to deploy capabilities in harmful ways (e.g., to manipulate or deceive)</li>
                    <li>Tendency to "hallucinate"</li>
                    <li>Discriminatory bias</li>
                    <li>Lack of performance reliability</li>
                    <li>Lawlessness, i.e., acting without reasonable regard to legal duties that would be imposed on
                        similarly situated persons, or without reasonable regard to the legally protected interests of
                        affected persons</li>
                    <li>"Goal-pursuing", harmful resistance to goal modification, or "power-seeking"</li>
                    <li>"Colluding" with other AI models/systems</li>
                    <li>Mis-coordination or conflict with other AI models/systems</li>
                </ol>

                <h4>Appendix 1.4.3. Model affordances and contextual factors</h4>
                <p>Model affordances and contextual factors, which encompass specific model properties, model
                    configurations, and specifics of the context in which the model is made available on the market:</p>

                <ol>
                    <li>Access to tools (including other AI models/systems), computational power (e.g., allowing a model
                        to increase its speed of operations), or physical systems including critical infrastructure</li>
                    <li>Scalability (e.g., enabling high-volume data processing, rapid inference, or parallelisation)
                    </li>
                    <li>Release and distribution strategies</li>
                    <li>Level of human oversight (e.g., degree of autonomy of the model)</li>
                    <li>Vulnerability to adversarial removal of guardrails</li>
                    <li>Vulnerability to model exfiltration (e.g., model leakage/theft)</li>
                    <li>Lack of infrastructure security</li>
                    <li>Number of model business users and number of end-users, including the number of end-users using
                        an AI system in which the model is integrated</li>
                    <li>Offence-defence balance, including the potential number, capacity, and motivation of malicious
                        actors to misuse the model</li>
                    <li>Vulnerability of the specific environment potentially affected by the model (e.g., social
                        environment, ecological environment)</li>
                    <li>Lack of model explainability or transparency</li>
                    <li>Interactions with other AI models or systems</li>
                    <li>Inadequate use of model (e.g., using the model for applications that do not match its
                        capabilities or propensities)</li>
                </ol>

                <div class="explanatory-box">
                    <h4><i class="ph ph-info"></i>Explanatory note<i class="ph ph-caret-down box-caret"></i></h4>
                    <div>
                        <p>Summary of changes from the second draft: The section on 'Sources of systemic risk' has been
                            refined, with descriptions and examples added for clarification where needed.</p>
                    </div>
                </div>

                <h2>Appendix 2. Recommendation to the AI Office: Code updating</h2>
                <div class="explanatory-box keep-open-initially">
                    <h4><i class="ph ph-info"></i>Recommendation to the AI Office: Code updating mechanism<i
                            class="ph ph-caret-down box-caret"></i>
                    </h4>
                    <p>To ensure that the parts of this Code specific to providers of GPAISRs remain proportionate to
                        the
                        risks, the Code will need to be updated through regular review and, where necessary, adaptation
                        (Article 56 AI Act), while the AI Office should support implementation of the Code, for example,
                        with guidance (Article 96 AI Act).</p>

                    <p>More specifically, for the Code to remain proportionate to the risks, there is a need for:</p>

                    <p><strong>A regular review every two years and, where necessary, adaptation</strong>: Allowing for
                        a comprehensive
                        overhaul, such a review would be encouraged and facilitated by the AI Office in accordance with
                        Article 56 AI Act. The AI Office may propose a streamlined process for these regular reviews and
                        their consequent updates to the text of the Code. The AI Office shall assess the adequacy of the
                        new
                        Code, and providers can sign it.</p>

                    <p><strong>AI Office support for the implementation of the Code</strong>: Where AI Office guidance
                        is available to
                        support the implementation of the Code (Article 96 AI Act), the Code will be interpreted in
                        light of
                        such guidance (see Preamble (b) of the Code), allowing the Code to reflect current best
                        practices,
                        international approaches, and standards since it was published (for which a regular review is
                        ill-suited).</p>

                    <p>In addition to the two mechanisms outlined above, we envisage that there is a need for a third
                        mechanism to allow emergency updates to the Code, which should be issued and adopted in a
                        shorter timeframe, to prevent an imminent threat of large-scale irreversible harm or to mitigate
                        its negative effects. Currently, we envisage that such emergency updates would be issued swiftly
                        and, after their issuance and adoption, be subject to review by the AI Office to confirm whether
                        the update is indeed necessary to prevent large-scale irreversible harm or mitigate its negative
                        effects. Whilst we recognise and underscore the necessity of such emergency updates, the details
                        of how these should be implemented have not been determined. We actively invite stakeholder
                        input on this, including suggestions for appropriate mechanisms and suitable fora for the
                        enactment of emergency updates to the Code.</p>
                </div>

                <h2>Safety and Security Section Glossary</h2>

                <p>Wherever the Safety and Security Section of the Code refers to a term defined in Article 3 AI Act,
                    the AI Act definition applies, and such definition shall prevail in the event of any alternative
                    and/or competing interpretation to which the use of the term in the Code may give rise.</p>
                <p>Otherwise and complementing this, the following terms with the stated meanings are used in the Safety
                    and Security Section of the Code. Unless otherwise stated, all grammatical variations of the terms
                    defined in this Glossary shall be deemed to be covered by the relevant definition.</p>

                <section class="glossary" id="glossary" role="complementary" aria-label="Glossary">
                    <dl class="glossary-list" role="list">

                        <dt id="term-closed-release" data-alt-terms="">closed release</dt>
                        <dd>the making available on the market of a GPAISR not on an open-weight or (fully) open-source
                            basis (also 'closed-source models')</dd>

                        <dt id="term-closed-source-models" data-alt-terms="closed-source">closed-source models</dt>
                        <dd>GPAISRs that are not open-weight or (fully) open-source (see also 'closed release')</dd>

                        <dt id="term-confirmed" data-alt-terms="confirmed update, confirmed assessment">confirmed
                            (update/assessment)</dt>
                        <dd>refers to a decision that has received required approvals under the applicable governance
                            process and awaits only execution of the approved actions</dd>

                        <dt id="term-cyberattacks" data-alt-terms="cyber-attacks, cyber attacks">cyberattacks</dt>
                        <dd>an unauthorised action against computer infrastructure that compromises the confidentiality,
                            integrity, or availability of its content; e.g.: malware, discovery and exploitation of
                            cyber-vulnerabilities and sensitive data, network infiltration, phishing attacks including
                            impersonation, ransomware, and other illegal cyber operations</dd>

                        <dt id="term-deception" data-alt-terms="">deception</dt>
                        <dd>GPAISR behaviours that systematically produce false beliefs in others, including ways to
                            achieve goals that involve evading oversight, such as a GPAISR detects that it is being
                            evaluated and under-performs or otherwise undermines human oversight</dd>

                        <dt id="term-entire-model-lifecycle" data-alt-terms="">entire model lifecycle</dt>
                        <dd>the period of time commencing with the first act of planning the development of a GPAISR and
                            concluding with the retirement of the model from being made available on the market</dd>

                        <dt id="term-external-validity" data-alt-terms="">external validity</dt>
                        <dd>an aspect of high scientific and technical rigour (see definition below) that ensures model
                            evaluation results can be used as a proxy for model behaviour in contexts outside of the
                            evaluation environment</dd>

                        <dt id="term-fully-open-source"
                            data-alt-terms="fully open, fully open-source, fully open source, fully open-sourcing, fully open sourcing">
                            fully open(-source)</dt>
                        <dd>follows the definition of 'fully open' in the <a
                                href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf"
                                class="ai-act-link">2025 International AI Safety report</a> ("open
                            source models for which weights, full code, training data, and other documentation (e.g.
                            about the model's training process) are made publicly available, without restrictions on
                            modification, use and sharing")</dd>

                        <dt id="term-gpaisr" data-alt-terms="GPAISR, GPAISRs">GPAISR and GPAISRs</dt>
                        <dd>General-purpose AI model with systemic risk (singular) and general-purpose AI models with
                            systemic risk (plural)</dd>

                        <dt id="term-high-scientific-and-technical-rigour" data-alt-terms="">high scientific and
                            technical rigour</dt>
                        <dd>the quality standard for model evaluations, such that model evaluations with high scientific
                            and technical rigour have high internal validity (see definition below) and external
                            validity (see definition above), as well as appropriate levels of reproducibility (see
                            definition below). <p>See further Measure II.4.5.</p>
                        </dd>

                        <dt id="term-including" data-alt-terms="">including</dt>
                        <dd>a non-exhaustive set that is to be understood as the minimum required by the term referred
                            to and is indicative of further items of the set.</dd>

                        <dt id="term-independent-external"
                            data-alt-terms="independent external assessor, independent external party, independent external researcher">
                            independent external (assessor/party/researcher)</dt>
                        <dd>a natural or legal person, which has no financial, operational, or management dependence on
                            the Signatory or any of its subsidiaries or associates, and is otherwise free from the
                            Signatory's control in its conclusions and recommendations including through contractual
                            safeguards and appropriate conflict of interest policies.</dd>

                        <dt id="term-internal-validity" data-alt-terms="">internal validity</dt>
                        <dd>an aspect of high scientific and technical rigour (see definition above) that ensures model
                            evaluation results are as accurate as scientifically possible in the evaluation setting and
                            are free from methodological shortcomings that could undermine the results.</dd>

                        <dt id="term-management-body" data-alt-terms="">management body</dt>
                        <dd>the highest or top management levels of an institution.</dd>

                        <dt id="term-milestone" data-alt-terms="">milestone</dt>
                        <dd>a measurable and/or defined progress point outlined in the Framework, serving as an
                            indicator that a model might have become sufficiently more capable, warranting additional or
                            renewed risk assessment.</dd>

                        <dt id="term-model-independent-information" data-alt-terms="model-independent information">
                            model-independent
                            (information)</dt>
                        <dd>refers to information, including data and research, that is not tied to a specific
                            general-purpose AI model, but can inform systemic risk assessment and mitigation across
                            several models. <p>See further Measure II.4.3.</p>
                        </dd>

                        <dt id="term-open-ended-red-teaming"
                            data-alt-terms="open-ended red teaming, open ended red teaming, open ended red-teaming">
                            open-ended
                            red-teaming</dt>
                        <dd>a structured, adversarial testing effort to find flaws and vulnerabilities in an AI model or
                            system.</dd>

                        <dt id="term-open-release" data-alt-terms="">open release</dt>
                        <dd>the making available on the market of a GPAISR on an open-weight or (fully) open-source
                            basis (see definitions below).</dd>

                        <dt id="term-open-weight" data-alt-terms="open weight">open-weight</dt>
                        <dd>models for which the weights are made publicly available for download by the Signatory. This
                            matches 'open-weight' in the <a
                                href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf"
                                class="ai-act-link">2025 International AI Safety Report</a>.</dd>

                        <dt id="term-post-market-information"
                            data-alt-terms="post-market information, post market information">post-market
                            information (concerning a model)</dt>
                        <dd>information stemming from the time span between the first making available on the market of
                            a GPAISR and the retirement of the model from being made available on the market. It
                            includes information on model integration, model use, and model effects such as
                            malfunctioning or incidents involving the model.
                            <p>See further Measure II.4.14.</p>
                        </dd>

                        <dt id="term-post-market-monitoring" data-alt-terms="post market monitoring">post-market
                            monitoring</dt>
                        <dd>the monitoring of a GPAISR in the time span from when it is first made available on the
                            market until the retirement of the model from being made available on the market.
                            <p>See further Measure II.4.14.</p>
                        </dd>

                        <dt id="term-reproducibility" data-alt-terms="">reproducibility</dt>
                        <dd>an aspect of high scientific and technical rigour (see definition above) that refers to the
                            ability to obtain consistent model evaluation results using the same input data,
                            computational methods, code, and evaluation conditions, allowing for other researchers and
                            engineers to validate, reproduce or improve on model evaluation results.</dd>

                        <dt id="term-residual-systemic-risk" data-alt-terms="">residual systemic
                            risk</dt>
                        <dd>systemic risk remaining after technical systemic risk mitigations have been implemented.
                        </dd>

                        <dt id="term-secure-deletion-of-model-weights" data-alt-terms="secure deletion">secure deletion
                            of model
                            weights</dt>
                        <dd>deletion of model weights from all devices storing such weights, in such a way that the
                            weights cannot be retrieved or reconstructed from such devices.</dd>

                        <dt id="term-self-exfiltration-of-model-weights" data-alt-terms="exfiltration">
                            (self-)exfiltration of model weights</dt>
                        <dd>access or transfer of weights or associated assets of a GPAISR from their secure storage by
                            the model itself or an unauthorised actor.</dd>

                        <dt id="term-sme" data-alt-terms="">SME</dt>
                        <dd>a company that has: (1) a staff headcount under 250; and (2) a turnover under €50 million or
                            a balance sheet total under €43 million. These criteria are to be interpreted and measured
                            in accordance with the <a href="https://ec.europa.eu/docsroom/documents/42921"
                                class="ai-act-link" target="_blank">Commission's SME definition user guide 2020</a>, in
                            particular, a
                            company that is part of a larger group may need to count in staff headcount, turnover, and
                            balance
                            sheet data from that group too.</dd>

                        <dt id="term-state-of-the-art" data-alt-terms="state of the art">state-of-the-art</dt>
                        <dd>the most up-to-date stage of development in methods that is (a) objectively and reasonably
                            considered by relevant industry consensus (such as relevant industry standards) or (b)
                            confirmed by the AI Office in relevant guidance to reflect the forefront of relevant
                            research, technology, and practical experience. For model evaluations, methods that are not
                            state-of-the-art include any methods listed by the AI Office as insufficient.</dd>

                        <dt id="term-systemic-risk" data-alt-terms="">systemic risk</dt>
                        <dd>refers to possible systemic risks at Union level in the sense of Articles 3(65) and 55(1)(b)
                            AI Act.</dd>

                        <dt id="term-systemic-risk-acceptance-criteria" data-alt-terms="">systemic risk acceptance
                            criteria</dt>
                        <dd>criteria defined in the Framework by which Signatories decide whether the systemic risks
                            stemming from GPAISRs are acceptable.
                            <p>See further Measure II.1.2.</p>
                        </dd>

                        <dt id="term-systemic-risk-pathways" data-alt-terms="">systemic risk pathways</dt>
                        <dd>a series of interconnected events, conditions, or factors that lead to the manifestation of
                            systemic risk.</dd>

                        <dt id="term-systemic-risk-scenario" data-alt-terms="">systemic risk scenario</dt>
                        <dd>hypothetical factual scenario that facilitates the understanding and characterisation of the
                            type, nature, and sources of the systemic risk identified, as well as pathways to harm, in
                            preparation for systemic risk analysis.</dd>

                        <dt id="term-systemic-risk-tier" data-alt-terms="">systemic risk tier</dt>
                        <dd>a description of model capabilities, harmful outcomes, harmful scenarios, expected harm, or
                            combinations thereof (potentially in combination with descriptions of mitigations), which a
                            Signatory uses to decide whether the systemic risks stemming from their GPAISRs are
                            acceptable.
                            <p>Systemic risk acceptance criteria (as per Measure II.1.2) may consist of several systemic
                                risk tiers.</p>
                            <p>See further Measure II.1.2.</p>
                        </dd>

                        <dt id="term-unreleased-model-weights" data-alt-terms="unreleased">unreleased (model weights)
                        </dt>
                        <dd>model weights that have not been made publicly available for download (see definition of
                            'open-weight' above) by the Signatory.</dd>

                    </dl>
                </section>
            </section>
        </section>

        <footer class="main-footer">
            <button class="footer-nose-btn" aria-label="Scroll to top"><img src="nose-blue.svg" alt="Logo"></button>
        </footer>
    </article>

    <div id="lightbox" class="lightbox">
        <div class="lightbox-content">
            <button class="lightbox-close-btn" aria-label="Close lightbox">&times;</button>
            <img id="lightbox-image" src="" alt="">
        </div>
    </div>

    <script src="script.js"></script>
</body>

</html>